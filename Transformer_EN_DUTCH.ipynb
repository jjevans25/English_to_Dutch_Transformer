{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_EN_DUTCH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPUOldhJuRy0P4qbnhYzo4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjevans25/English_to_Dutch_Transformer/blob/master/Transformer_EN_DUTCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lXTs0nDl3b4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ua1NAklnaLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEvwZ_bjnb6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading Files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryMCaiO6nfzE",
        "colab_type": "code",
        "outputId": "42b86a7a-bebd-4a5b-9185-89dfe2fd6971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nyQktsZniFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/europarl-v7.nl-en.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/europarl-v7.nl-en.nl\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      europarl_nl = f.read()\n",
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/nonbreaking_prefix2.nl\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      non_breaking_prefix_nl = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXZVS2Mankws",
        "colab_type": "code",
        "outputId": "cf7b95d8-29b4-442a-d3f7-effc091614d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "non_breaking_prefix_nl[:500]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\nk\\nl\\nm\\nn\\no\\np\\nq\\nr\\ns\\nt\\nu\\nv\\nw\\nx\\ny\\nz\\nbacc\\nbc\\nbgen\\nc.i\\ndhr\\ndr\\ndr.h.c\\ndrs\\ndrs\\nds\\neint\\nfa\\nFa\\nfam\\ngen\\ngenm\\ning\\nir\\njhr\\njkvr\\njr\\nkand\\nkol\\nlgen\\nlkol\\nLt\\nmaj\\nMej\\nmevr\\nMme\\nmr\\nmr\\nMw\\no.b.s\\nplv\\nprof\\nritm\\ntint\\nVz\\nZ.D\\nZ.D.H\\nZ.E\\nZ.Em\\nZ.H\\nZ.K.H\\nZ.K.M\\nZ.M\\nz.v\\na.g.v\\nbijv\\nbijz\\nbv\\nd.w.z\\ne.c\\ne.g\\ne.k\\nev\\ni.p.v\\ni.s.m\\ni.t.t\\ni.v.m\\nm.a.w\\nm.b.t\\nm.b.v\\nm.h.o\\nm.i\\nm.i.v\\nv.w.t\\nNr \\nNrs\\nnrs\\nnr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzqYm3HWnncl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARu_L7B_nptS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_nl = non_breaking_prefix_nl.split(\"\\n\")\n",
        "non_breaking_prefix_nl = [' ' + pref + '.' for pref in non_breaking_prefix_nl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgTsgsOhnr3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_nl = europarl_nl\n",
        "for prefix in non_breaking_prefix_nl:\n",
        "    corpus_nl = corpus_nl.replace(prefix, prefix + '$$$')\n",
        "corpus_nl = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_nl)\n",
        "corpus_nl = re.sub(r\".\\$\\$\\$\", '', corpus_nl)\n",
        "corpus_nl = re.sub(r\"  +\", \" \", corpus_nl)\n",
        "corpus_nl = corpus_nl.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9UQsZuXnuhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c8QaYROnwiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13\n",
        ")\n",
        "tokenizer_nl = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_nl, target_vocab_size=2**13\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YVgwwQvnyjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_NL = tokenizer_nl.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVHRXHkkn09T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "            for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_NL-2] + tokenizer_nl.encode(sentence) + [VOCAB_SIZE_NL-1]\n",
        "            for sentence in corpus_nl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tvPfBJ6n2zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Max Length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5gcd4hin4o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4fxJJaXn7Av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding=\"post\",\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding=\"post\",\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltbzZH6hn9yP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euGgVhqOn_9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Building "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPmx30IqoEJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Positional Encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhbLPvmvoJtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "  def get_angles(self, pos, i, d_model): # pos : (seq_length, 1) i: (1, d_model)\n",
        "    angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "    return pos * angles # (seq_length, d_model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "    angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis, :],\n",
        "                             d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UdHBKIoNQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsdFEoKhoPCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  product = tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XUrUFDqoU50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multihead attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxsRrJoJoZJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "    self.query_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units=self.d_model)\n",
        "    self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "  def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    splitted_inputs = tf.reshape(inputs, shape=shape) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    return tf.transpose(splitted_inputs, perm=[0, 2, 1, 3]) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    queries = self.query_lin(queries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)                          \n",
        "    values = self.split_proj(values, batch_size)             \n",
        "\n",
        "    attention = scaled_dot_product_attention(queries, keys, values, mask)    \n",
        "\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])       \n",
        "\n",
        "    concat_attention = tf.reshape(attention, \n",
        "                                  shape=(batch_size, -1, self.d_model))  \n",
        "    \n",
        "    outputs = self.final_lin(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJFlKN-ochL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlcnmJ5Qoew5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    attention = self.multi_head_attention(inputs,\n",
        "                                          inputs,\n",
        "                                          inputs,\n",
        "                                          mask)\n",
        "    attention = self.dropout_1(attention, training=training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    outputs = self.dense_1(attention)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_2(outputs, training=training)\n",
        "    outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhxU8vBioilf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name=\"encoder\"):\n",
        "    super(Encoder, self).__init__(name=name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "    self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                     nb_proj,\n",
        "                                     dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.enc_layers[i](outputs, mask, training)\n",
        "    \n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVHZYMCon1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSci5iwgoqMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    # self multi head attention\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # Multi head attention combined with encoder output\n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # Feed forward\n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    attention = self.multi_head_attention_1(inputs,\n",
        "                                            inputs,\n",
        "                                            inputs,\n",
        "                                            mask_1)\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                              enc_outputs,\n",
        "                                              enc_outputs,\n",
        "                                              mask_2)\n",
        "    \n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLNd-PQaot8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name=\"decoder\"):\n",
        "    super(Decoder, self).__init__(name=name)\n",
        "    self.d_model = d_model\n",
        "    self.nb_layers = nb_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                    nb_proj,\n",
        "                                    dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs= self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,\n",
        "                                   enc_outputs,\n",
        "                                   mask_1,\n",
        "                                   mask_2,\n",
        "                                   training)\n",
        "    return outputs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9VZqaN_oyaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnoJ1NdQo1nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92_yTG_vo5lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVUJgipoo8C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_NL,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_K-_GGuo-t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = loss_object(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DwdU-u4pBqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D9CMcuRpEjW",
        "colab_type": "code",
        "outputId": "dfc0a61a-7241-47d4-8ce8-e616a468790b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer\n",
        "                           )\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest checkpoint restored !!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored !!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfDcu-68pLJC",
        "colab_type": "code",
        "outputId": "cdc0f0e6-9c62-4e63-dcb5-63b529014515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.2617 Accuracy 0.0033\n",
            "Epoch 1 Batch 50 Loss 6.1803 Accuracy 0.0167\n",
            "Epoch 1 Batch 100 Loss 6.0741 Accuracy 0.0343\n",
            "Epoch 1 Batch 150 Loss 6.0003 Accuracy 0.0404\n",
            "Epoch 1 Batch 200 Loss 5.9101 Accuracy 0.0435\n",
            "Epoch 1 Batch 250 Loss 5.8204 Accuracy 0.0453\n",
            "Epoch 1 Batch 300 Loss 5.7001 Accuracy 0.0497\n",
            "Epoch 1 Batch 350 Loss 5.5875 Accuracy 0.0560\n",
            "Epoch 1 Batch 400 Loss 5.4742 Accuracy 0.0610\n",
            "Epoch 1 Batch 450 Loss 5.3629 Accuracy 0.0651\n",
            "Epoch 1 Batch 500 Loss 5.2622 Accuracy 0.0701\n",
            "Epoch 1 Batch 550 Loss 5.1699 Accuracy 0.0748\n",
            "Epoch 1 Batch 600 Loss 5.0807 Accuracy 0.0791\n",
            "Epoch 1 Batch 650 Loss 4.9942 Accuracy 0.0831\n",
            "Epoch 1 Batch 700 Loss 4.9180 Accuracy 0.0870\n",
            "Epoch 1 Batch 750 Loss 4.8423 Accuracy 0.0907\n",
            "Epoch 1 Batch 800 Loss 4.7753 Accuracy 0.0945\n",
            "Epoch 1 Batch 850 Loss 4.7110 Accuracy 0.0983\n",
            "Epoch 1 Batch 900 Loss 4.6508 Accuracy 0.1019\n",
            "Epoch 1 Batch 950 Loss 4.5943 Accuracy 0.1054\n",
            "Epoch 1 Batch 1000 Loss 4.5375 Accuracy 0.1088\n",
            "Epoch 1 Batch 1050 Loss 4.4866 Accuracy 0.1121\n",
            "Epoch 1 Batch 1100 Loss 4.4364 Accuracy 0.1151\n",
            "Epoch 1 Batch 1150 Loss 4.3905 Accuracy 0.1180\n",
            "Epoch 1 Batch 1200 Loss 4.3468 Accuracy 0.1208\n",
            "Epoch 1 Batch 1250 Loss 4.3048 Accuracy 0.1234\n",
            "Epoch 1 Batch 1300 Loss 4.2677 Accuracy 0.1260\n",
            "Epoch 1 Batch 1350 Loss 4.2273 Accuracy 0.1284\n",
            "Epoch 1 Batch 1400 Loss 4.1915 Accuracy 0.1307\n",
            "Epoch 1 Batch 1450 Loss 4.1561 Accuracy 0.1329\n",
            "Epoch 1 Batch 1500 Loss 4.1224 Accuracy 0.1350\n",
            "Epoch 1 Batch 1550 Loss 4.0909 Accuracy 0.1370\n",
            "Epoch 1 Batch 1600 Loss 4.0614 Accuracy 0.1390\n",
            "Epoch 1 Batch 1650 Loss 4.0331 Accuracy 0.1409\n",
            "Epoch 1 Batch 1700 Loss 4.0052 Accuracy 0.1428\n",
            "Epoch 1 Batch 1750 Loss 3.9780 Accuracy 0.1447\n",
            "Epoch 1 Batch 1800 Loss 3.9532 Accuracy 0.1465\n",
            "Epoch 1 Batch 1850 Loss 3.9284 Accuracy 0.1482\n",
            "Epoch 1 Batch 1900 Loss 3.9055 Accuracy 0.1498\n",
            "Epoch 1 Batch 1950 Loss 3.8825 Accuracy 0.1515\n",
            "Epoch 1 Batch 2000 Loss 3.8601 Accuracy 0.1531\n",
            "Epoch 1 Batch 2050 Loss 3.8374 Accuracy 0.1546\n",
            "Epoch 1 Batch 2100 Loss 3.8154 Accuracy 0.1562\n",
            "Epoch 1 Batch 2150 Loss 3.7938 Accuracy 0.1577\n",
            "Epoch 1 Batch 2200 Loss 3.7737 Accuracy 0.1591\n",
            "Epoch 1 Batch 2250 Loss 3.7534 Accuracy 0.1605\n",
            "Epoch 1 Batch 2300 Loss 3.7337 Accuracy 0.1619\n",
            "Epoch 1 Batch 2350 Loss 3.7137 Accuracy 0.1633\n",
            "Epoch 1 Batch 2400 Loss 3.6943 Accuracy 0.1647\n",
            "Epoch 1 Batch 2450 Loss 3.6738 Accuracy 0.1661\n",
            "Epoch 1 Batch 2500 Loss 3.6542 Accuracy 0.1675\n",
            "Epoch 1 Batch 2550 Loss 3.6339 Accuracy 0.1689\n",
            "Epoch 1 Batch 2600 Loss 3.6137 Accuracy 0.1704\n",
            "Epoch 1 Batch 2650 Loss 3.5952 Accuracy 0.1719\n",
            "Epoch 1 Batch 2700 Loss 3.5766 Accuracy 0.1733\n",
            "Epoch 1 Batch 2750 Loss 3.5588 Accuracy 0.1748\n",
            "Epoch 1 Batch 2800 Loss 3.5406 Accuracy 0.1762\n",
            "Epoch 1 Batch 2850 Loss 3.5233 Accuracy 0.1777\n",
            "Epoch 1 Batch 2900 Loss 3.5062 Accuracy 0.1792\n",
            "Epoch 1 Batch 2950 Loss 3.4891 Accuracy 0.1807\n",
            "Epoch 1 Batch 3000 Loss 3.4727 Accuracy 0.1823\n",
            "Epoch 1 Batch 3050 Loss 3.4560 Accuracy 0.1838\n",
            "Epoch 1 Batch 3100 Loss 3.4396 Accuracy 0.1854\n",
            "Epoch 1 Batch 3150 Loss 3.4234 Accuracy 0.1870\n",
            "Epoch 1 Batch 3200 Loss 3.4078 Accuracy 0.1885\n",
            "Epoch 1 Batch 3250 Loss 3.3922 Accuracy 0.1900\n",
            "Epoch 1 Batch 3300 Loss 3.3765 Accuracy 0.1916\n",
            "Epoch 1 Batch 3350 Loss 3.3611 Accuracy 0.1930\n",
            "Epoch 1 Batch 3400 Loss 3.3462 Accuracy 0.1945\n",
            "Epoch 1 Batch 3450 Loss 3.3315 Accuracy 0.1959\n",
            "Epoch 1 Batch 3500 Loss 3.3173 Accuracy 0.1973\n",
            "Epoch 1 Batch 3550 Loss 3.3033 Accuracy 0.1987\n",
            "Epoch 1 Batch 3600 Loss 3.2892 Accuracy 0.2000\n",
            "Epoch 1 Batch 3650 Loss 3.2754 Accuracy 0.2013\n",
            "Epoch 1 Batch 3700 Loss 3.2605 Accuracy 0.2027\n",
            "Epoch 1 Batch 3750 Loss 3.2465 Accuracy 0.2040\n",
            "Epoch 1 Batch 3800 Loss 3.2331 Accuracy 0.2054\n",
            "Epoch 1 Batch 3850 Loss 3.2194 Accuracy 0.2067\n",
            "Epoch 1 Batch 3900 Loss 3.2063 Accuracy 0.2080\n",
            "Epoch 1 Batch 3950 Loss 3.1931 Accuracy 0.2093\n",
            "Epoch 1 Batch 4000 Loss 3.1807 Accuracy 0.2106\n",
            "Epoch 1 Batch 4050 Loss 3.1680 Accuracy 0.2118\n",
            "Epoch 1 Batch 4100 Loss 3.1555 Accuracy 0.2131\n",
            "Epoch 1 Batch 4150 Loss 3.1428 Accuracy 0.2144\n",
            "Epoch 1 Batch 4200 Loss 3.1310 Accuracy 0.2156\n",
            "Epoch 1 Batch 4250 Loss 3.1185 Accuracy 0.2169\n",
            "Epoch 1 Batch 4300 Loss 3.1059 Accuracy 0.2182\n",
            "Epoch 1 Batch 4350 Loss 3.0949 Accuracy 0.2195\n",
            "Epoch 1 Batch 4400 Loss 3.0834 Accuracy 0.2208\n",
            "Epoch 1 Batch 4450 Loss 3.0720 Accuracy 0.2220\n",
            "Epoch 1 Batch 4500 Loss 3.0609 Accuracy 0.2232\n",
            "Epoch 1 Batch 4550 Loss 3.0500 Accuracy 0.2244\n",
            "Epoch 1 Batch 4600 Loss 3.0391 Accuracy 0.2256\n",
            "Epoch 1 Batch 4650 Loss 3.0281 Accuracy 0.2267\n",
            "Epoch 1 Batch 4700 Loss 3.0171 Accuracy 0.2279\n",
            "Epoch 1 Batch 4750 Loss 3.0070 Accuracy 0.2290\n",
            "Epoch 1 Batch 4800 Loss 2.9972 Accuracy 0.2300\n",
            "Epoch 1 Batch 4850 Loss 2.9884 Accuracy 0.2309\n",
            "Epoch 1 Batch 4900 Loss 2.9790 Accuracy 0.2318\n",
            "Epoch 1 Batch 4950 Loss 2.9703 Accuracy 0.2327\n",
            "Epoch 1 Batch 5000 Loss 2.9616 Accuracy 0.2336\n",
            "Epoch 1 Batch 5050 Loss 2.9533 Accuracy 0.2345\n",
            "Epoch 1 Batch 5100 Loss 2.9445 Accuracy 0.2353\n",
            "Epoch 1 Batch 5150 Loss 2.9362 Accuracy 0.2362\n",
            "Epoch 1 Batch 5200 Loss 2.9280 Accuracy 0.2370\n",
            "Epoch 1 Batch 5250 Loss 2.9204 Accuracy 0.2379\n",
            "Epoch 1 Batch 5300 Loss 2.9126 Accuracy 0.2387\n",
            "Epoch 1 Batch 5350 Loss 2.9047 Accuracy 0.2395\n",
            "Epoch 1 Batch 5400 Loss 2.8971 Accuracy 0.2403\n",
            "Epoch 1 Batch 5450 Loss 2.8891 Accuracy 0.2411\n",
            "Epoch 1 Batch 5500 Loss 2.8815 Accuracy 0.2419\n",
            "Epoch 1 Batch 5550 Loss 2.8739 Accuracy 0.2427\n",
            "Epoch 1 Batch 5600 Loss 2.8664 Accuracy 0.2435\n",
            "Epoch 1 Batch 5650 Loss 2.8593 Accuracy 0.2442\n",
            "Epoch 1 Batch 5700 Loss 2.8517 Accuracy 0.2450\n",
            "Epoch 1 Batch 5750 Loss 2.8441 Accuracy 0.2458\n",
            "Epoch 1 Batch 5800 Loss 2.8369 Accuracy 0.2465\n",
            "Epoch 1 Batch 5850 Loss 2.8302 Accuracy 0.2473\n",
            "Epoch 1 Batch 5900 Loss 2.8233 Accuracy 0.2480\n",
            "Epoch 1 Batch 5950 Loss 2.8164 Accuracy 0.2487\n",
            "Epoch 1 Batch 6000 Loss 2.8094 Accuracy 0.2495\n",
            "Epoch 1 Batch 6050 Loss 2.8029 Accuracy 0.2501\n",
            "Epoch 1 Batch 6100 Loss 2.7962 Accuracy 0.2508\n",
            "Epoch 1 Batch 6150 Loss 2.7894 Accuracy 0.2514\n",
            "Epoch 1 Batch 6200 Loss 2.7829 Accuracy 0.2521\n",
            "Epoch 1 Batch 6250 Loss 2.7764 Accuracy 0.2527\n",
            "Epoch 1 Batch 6300 Loss 2.7702 Accuracy 0.2533\n",
            "Epoch 1 Batch 6350 Loss 2.7637 Accuracy 0.2540\n",
            "Epoch 1 Batch 6400 Loss 2.7571 Accuracy 0.2546\n",
            "Epoch 1 Batch 6450 Loss 2.7506 Accuracy 0.2553\n",
            "Epoch 1 Batch 6500 Loss 2.7442 Accuracy 0.2559\n",
            "Epoch 1 Batch 6550 Loss 2.7380 Accuracy 0.2565\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 1975.4691054821014 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 2.2469 Accuracy 0.3363\n",
            "Epoch 2 Batch 50 Loss 2.0401 Accuracy 0.3335\n",
            "Epoch 2 Batch 100 Loss 2.0226 Accuracy 0.3366\n",
            "Epoch 2 Batch 150 Loss 2.0077 Accuracy 0.3395\n",
            "Epoch 2 Batch 200 Loss 1.9932 Accuracy 0.3405\n",
            "Epoch 2 Batch 250 Loss 1.9769 Accuracy 0.3420\n",
            "Epoch 2 Batch 300 Loss 1.9636 Accuracy 0.3435\n",
            "Epoch 2 Batch 350 Loss 1.9566 Accuracy 0.3436\n",
            "Epoch 2 Batch 400 Loss 1.9547 Accuracy 0.3435\n",
            "Epoch 2 Batch 450 Loss 1.9526 Accuracy 0.3437\n",
            "Epoch 2 Batch 500 Loss 1.9437 Accuracy 0.3445\n",
            "Epoch 2 Batch 550 Loss 1.9408 Accuracy 0.3440\n",
            "Epoch 2 Batch 600 Loss 1.9375 Accuracy 0.3445\n",
            "Epoch 2 Batch 650 Loss 1.9331 Accuracy 0.3451\n",
            "Epoch 2 Batch 700 Loss 1.9308 Accuracy 0.3455\n",
            "Epoch 2 Batch 750 Loss 1.9280 Accuracy 0.3456\n",
            "Epoch 2 Batch 800 Loss 1.9249 Accuracy 0.3461\n",
            "Epoch 2 Batch 850 Loss 1.9236 Accuracy 0.3463\n",
            "Epoch 2 Batch 900 Loss 1.9212 Accuracy 0.3464\n",
            "Epoch 2 Batch 950 Loss 1.9194 Accuracy 0.3466\n",
            "Epoch 2 Batch 1000 Loss 1.9175 Accuracy 0.3467\n",
            "Epoch 2 Batch 1050 Loss 1.9148 Accuracy 0.3467\n",
            "Epoch 2 Batch 1100 Loss 1.9129 Accuracy 0.3467\n",
            "Epoch 2 Batch 1150 Loss 1.9094 Accuracy 0.3466\n",
            "Epoch 2 Batch 1200 Loss 1.9069 Accuracy 0.3465\n",
            "Epoch 2 Batch 1250 Loss 1.9047 Accuracy 0.3466\n",
            "Epoch 2 Batch 1300 Loss 1.9028 Accuracy 0.3467\n",
            "Epoch 2 Batch 1350 Loss 1.9010 Accuracy 0.3466\n",
            "Epoch 2 Batch 1400 Loss 1.8994 Accuracy 0.3466\n",
            "Epoch 2 Batch 1450 Loss 1.8971 Accuracy 0.3467\n",
            "Epoch 2 Batch 1500 Loss 1.8953 Accuracy 0.3466\n",
            "Epoch 2 Batch 1550 Loss 1.8930 Accuracy 0.3466\n",
            "Epoch 2 Batch 1600 Loss 1.8910 Accuracy 0.3470\n",
            "Epoch 2 Batch 1650 Loss 1.8885 Accuracy 0.3473\n",
            "Epoch 2 Batch 1700 Loss 1.8857 Accuracy 0.3475\n",
            "Epoch 2 Batch 1750 Loss 1.8826 Accuracy 0.3478\n",
            "Epoch 2 Batch 1800 Loss 1.8798 Accuracy 0.3482\n",
            "Epoch 2 Batch 1850 Loss 1.8773 Accuracy 0.3486\n",
            "Epoch 2 Batch 1900 Loss 1.8745 Accuracy 0.3490\n",
            "Epoch 2 Batch 1950 Loss 1.8716 Accuracy 0.3494\n",
            "Epoch 2 Batch 2000 Loss 1.8681 Accuracy 0.3496\n",
            "Epoch 2 Batch 2050 Loss 1.8658 Accuracy 0.3499\n",
            "Epoch 2 Batch 2100 Loss 1.8625 Accuracy 0.3503\n",
            "Epoch 2 Batch 2150 Loss 1.8589 Accuracy 0.3506\n",
            "Epoch 2 Batch 2200 Loss 1.8560 Accuracy 0.3509\n",
            "Epoch 2 Batch 2250 Loss 1.8532 Accuracy 0.3512\n",
            "Epoch 2 Batch 2300 Loss 1.8502 Accuracy 0.3515\n",
            "Epoch 2 Batch 2350 Loss 1.8466 Accuracy 0.3519\n",
            "Epoch 2 Batch 2400 Loss 1.8423 Accuracy 0.3524\n",
            "Epoch 2 Batch 2450 Loss 1.8384 Accuracy 0.3528\n",
            "Epoch 2 Batch 2500 Loss 1.8338 Accuracy 0.3532\n",
            "Epoch 2 Batch 2550 Loss 1.8293 Accuracy 0.3535\n",
            "Epoch 2 Batch 2600 Loss 1.8249 Accuracy 0.3539\n",
            "Epoch 2 Batch 2650 Loss 1.8202 Accuracy 0.3543\n",
            "Epoch 2 Batch 2700 Loss 1.8162 Accuracy 0.3548\n",
            "Epoch 2 Batch 2750 Loss 1.8114 Accuracy 0.3553\n",
            "Epoch 2 Batch 2800 Loss 1.8072 Accuracy 0.3558\n",
            "Epoch 2 Batch 2850 Loss 1.8026 Accuracy 0.3564\n",
            "Epoch 2 Batch 2900 Loss 1.7977 Accuracy 0.3569\n",
            "Epoch 2 Batch 2950 Loss 1.7929 Accuracy 0.3575\n",
            "Epoch 2 Batch 3000 Loss 1.7876 Accuracy 0.3583\n",
            "Epoch 2 Batch 3050 Loss 1.7822 Accuracy 0.3589\n",
            "Epoch 2 Batch 3100 Loss 1.7773 Accuracy 0.3596\n",
            "Epoch 2 Batch 3150 Loss 1.7728 Accuracy 0.3603\n",
            "Epoch 2 Batch 3200 Loss 1.7684 Accuracy 0.3610\n",
            "Epoch 2 Batch 3250 Loss 1.7639 Accuracy 0.3617\n",
            "Epoch 2 Batch 3300 Loss 1.7589 Accuracy 0.3623\n",
            "Epoch 2 Batch 3350 Loss 1.7548 Accuracy 0.3628\n",
            "Epoch 2 Batch 3400 Loss 1.7507 Accuracy 0.3634\n",
            "Epoch 2 Batch 3450 Loss 1.7471 Accuracy 0.3640\n",
            "Epoch 2 Batch 3500 Loss 1.7434 Accuracy 0.3644\n",
            "Epoch 2 Batch 3550 Loss 1.7396 Accuracy 0.3649\n",
            "Epoch 2 Batch 3600 Loss 1.7359 Accuracy 0.3653\n",
            "Epoch 2 Batch 3650 Loss 1.7319 Accuracy 0.3659\n",
            "Epoch 2 Batch 3700 Loss 1.7282 Accuracy 0.3662\n",
            "Epoch 2 Batch 3750 Loss 1.7245 Accuracy 0.3667\n",
            "Epoch 2 Batch 3800 Loss 1.7209 Accuracy 0.3671\n",
            "Epoch 2 Batch 3850 Loss 1.7175 Accuracy 0.3675\n",
            "Epoch 2 Batch 3900 Loss 1.7137 Accuracy 0.3680\n",
            "Epoch 2 Batch 3950 Loss 1.7104 Accuracy 0.3685\n",
            "Epoch 2 Batch 4000 Loss 1.7073 Accuracy 0.3689\n",
            "Epoch 2 Batch 4050 Loss 1.7037 Accuracy 0.3693\n",
            "Epoch 2 Batch 4100 Loss 1.7002 Accuracy 0.3699\n",
            "Epoch 2 Batch 4150 Loss 1.6967 Accuracy 0.3704\n",
            "Epoch 2 Batch 4200 Loss 1.6938 Accuracy 0.3708\n",
            "Epoch 2 Batch 4250 Loss 1.6908 Accuracy 0.3712\n",
            "Epoch 2 Batch 4300 Loss 1.6877 Accuracy 0.3717\n",
            "Epoch 2 Batch 4350 Loss 1.6847 Accuracy 0.3721\n",
            "Epoch 2 Batch 4400 Loss 1.6820 Accuracy 0.3726\n",
            "Epoch 2 Batch 4450 Loss 1.6791 Accuracy 0.3729\n",
            "Epoch 2 Batch 4500 Loss 1.6763 Accuracy 0.3733\n",
            "Epoch 2 Batch 4550 Loss 1.6736 Accuracy 0.3737\n",
            "Epoch 2 Batch 4600 Loss 1.6711 Accuracy 0.3742\n",
            "Epoch 2 Batch 4650 Loss 1.6685 Accuracy 0.3746\n",
            "Epoch 2 Batch 4700 Loss 1.6661 Accuracy 0.3749\n",
            "Epoch 2 Batch 4750 Loss 1.6643 Accuracy 0.3752\n",
            "Epoch 2 Batch 4800 Loss 1.6631 Accuracy 0.3754\n",
            "Epoch 2 Batch 4850 Loss 1.6618 Accuracy 0.3756\n",
            "Epoch 2 Batch 4900 Loss 1.6610 Accuracy 0.3757\n",
            "Epoch 2 Batch 4950 Loss 1.6606 Accuracy 0.3758\n",
            "Epoch 2 Batch 5000 Loss 1.6599 Accuracy 0.3759\n",
            "Epoch 2 Batch 5050 Loss 1.6596 Accuracy 0.3760\n",
            "Epoch 2 Batch 5100 Loss 1.6591 Accuracy 0.3760\n",
            "Epoch 2 Batch 5150 Loss 1.6586 Accuracy 0.3761\n",
            "Epoch 2 Batch 5200 Loss 1.6581 Accuracy 0.3761\n",
            "Epoch 2 Batch 5250 Loss 1.6575 Accuracy 0.3762\n",
            "Epoch 2 Batch 5300 Loss 1.6568 Accuracy 0.3763\n",
            "Epoch 2 Batch 5350 Loss 1.6564 Accuracy 0.3764\n",
            "Epoch 2 Batch 5400 Loss 1.6557 Accuracy 0.3764\n",
            "Epoch 2 Batch 5450 Loss 1.6559 Accuracy 0.3765\n",
            "Epoch 2 Batch 5500 Loss 1.6557 Accuracy 0.3766\n",
            "Epoch 2 Batch 5550 Loss 1.6548 Accuracy 0.3767\n",
            "Epoch 2 Batch 5600 Loss 1.6541 Accuracy 0.3768\n",
            "Epoch 2 Batch 5650 Loss 1.6539 Accuracy 0.3768\n",
            "Epoch 2 Batch 5700 Loss 1.6534 Accuracy 0.3769\n",
            "Epoch 2 Batch 5750 Loss 1.6532 Accuracy 0.3769\n",
            "Epoch 2 Batch 5800 Loss 1.6532 Accuracy 0.3769\n",
            "Epoch 2 Batch 5850 Loss 1.6528 Accuracy 0.3770\n",
            "Epoch 2 Batch 5900 Loss 1.6526 Accuracy 0.3770\n",
            "Epoch 2 Batch 5950 Loss 1.6524 Accuracy 0.3770\n",
            "Epoch 2 Batch 6000 Loss 1.6522 Accuracy 0.3771\n",
            "Epoch 2 Batch 6050 Loss 1.6522 Accuracy 0.3771\n",
            "Epoch 2 Batch 6100 Loss 1.6520 Accuracy 0.3771\n",
            "Epoch 2 Batch 6150 Loss 1.6518 Accuracy 0.3770\n",
            "Epoch 2 Batch 6200 Loss 1.6516 Accuracy 0.3770\n",
            "Epoch 2 Batch 6250 Loss 1.6513 Accuracy 0.3770\n",
            "Epoch 2 Batch 6300 Loss 1.6510 Accuracy 0.3770\n",
            "Epoch 2 Batch 6350 Loss 1.6508 Accuracy 0.3770\n",
            "Epoch 2 Batch 6400 Loss 1.6506 Accuracy 0.3770\n",
            "Epoch 2 Batch 6450 Loss 1.6503 Accuracy 0.3770\n",
            "Epoch 2 Batch 6500 Loss 1.6502 Accuracy 0.3770\n",
            "Epoch 2 Batch 6550 Loss 1.6498 Accuracy 0.3770\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 2062.072189092636 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.8164 Accuracy 0.3635\n",
            "Epoch 3 Batch 50 Loss 1.6784 Accuracy 0.3742\n",
            "Epoch 3 Batch 100 Loss 1.6630 Accuracy 0.3775\n",
            "Epoch 3 Batch 150 Loss 1.6578 Accuracy 0.3799\n",
            "Epoch 3 Batch 200 Loss 1.6569 Accuracy 0.3807\n",
            "Epoch 3 Batch 250 Loss 1.6526 Accuracy 0.3815\n",
            "Epoch 3 Batch 300 Loss 1.6451 Accuracy 0.3816\n",
            "Epoch 3 Batch 350 Loss 1.6398 Accuracy 0.3820\n",
            "Epoch 3 Batch 400 Loss 1.6351 Accuracy 0.3828\n",
            "Epoch 3 Batch 450 Loss 1.6324 Accuracy 0.3834\n",
            "Epoch 3 Batch 500 Loss 1.6298 Accuracy 0.3833\n",
            "Epoch 3 Batch 550 Loss 1.6243 Accuracy 0.3836\n",
            "Epoch 3 Batch 600 Loss 1.6245 Accuracy 0.3841\n",
            "Epoch 3 Batch 650 Loss 1.6235 Accuracy 0.3841\n",
            "Epoch 3 Batch 700 Loss 1.6243 Accuracy 0.3838\n",
            "Epoch 3 Batch 750 Loss 1.6238 Accuracy 0.3837\n",
            "Epoch 3 Batch 800 Loss 1.6224 Accuracy 0.3839\n",
            "Epoch 3 Batch 850 Loss 1.6224 Accuracy 0.3841\n",
            "Epoch 3 Batch 900 Loss 1.6203 Accuracy 0.3842\n",
            "Epoch 3 Batch 950 Loss 1.6188 Accuracy 0.3842\n",
            "Epoch 3 Batch 1000 Loss 1.6187 Accuracy 0.3840\n",
            "Epoch 3 Batch 1050 Loss 1.6188 Accuracy 0.3836\n",
            "Epoch 3 Batch 1100 Loss 1.6174 Accuracy 0.3834\n",
            "Epoch 3 Batch 1150 Loss 1.6168 Accuracy 0.3833\n",
            "Epoch 3 Batch 1200 Loss 1.6146 Accuracy 0.3829\n",
            "Epoch 3 Batch 1250 Loss 1.6146 Accuracy 0.3827\n",
            "Epoch 3 Batch 1300 Loss 1.6151 Accuracy 0.3828\n",
            "Epoch 3 Batch 1350 Loss 1.6154 Accuracy 0.3828\n",
            "Epoch 3 Batch 1400 Loss 1.6147 Accuracy 0.3826\n",
            "Epoch 3 Batch 1450 Loss 1.6128 Accuracy 0.3825\n",
            "Epoch 3 Batch 1500 Loss 1.6115 Accuracy 0.3824\n",
            "Epoch 3 Batch 1550 Loss 1.6103 Accuracy 0.3824\n",
            "Epoch 3 Batch 1600 Loss 1.6094 Accuracy 0.3824\n",
            "Epoch 3 Batch 1650 Loss 1.6084 Accuracy 0.3825\n",
            "Epoch 3 Batch 1700 Loss 1.6067 Accuracy 0.3827\n",
            "Epoch 3 Batch 1750 Loss 1.6052 Accuracy 0.3829\n",
            "Epoch 3 Batch 1800 Loss 1.6032 Accuracy 0.3832\n",
            "Epoch 3 Batch 1850 Loss 1.6016 Accuracy 0.3835\n",
            "Epoch 3 Batch 1900 Loss 1.6000 Accuracy 0.3838\n",
            "Epoch 3 Batch 1950 Loss 1.5985 Accuracy 0.3841\n",
            "Epoch 3 Batch 2000 Loss 1.5963 Accuracy 0.3843\n",
            "Epoch 3 Batch 2050 Loss 1.5950 Accuracy 0.3844\n",
            "Epoch 3 Batch 2100 Loss 1.5928 Accuracy 0.3845\n",
            "Epoch 3 Batch 2150 Loss 1.5908 Accuracy 0.3848\n",
            "Epoch 3 Batch 2200 Loss 1.5884 Accuracy 0.3851\n",
            "Epoch 3 Batch 2250 Loss 1.5864 Accuracy 0.3852\n",
            "Epoch 3 Batch 2300 Loss 1.5841 Accuracy 0.3855\n",
            "Epoch 3 Batch 2350 Loss 1.5809 Accuracy 0.3858\n",
            "Epoch 3 Batch 2400 Loss 1.5783 Accuracy 0.3861\n",
            "Epoch 3 Batch 2450 Loss 1.5756 Accuracy 0.3864\n",
            "Epoch 3 Batch 2500 Loss 1.5725 Accuracy 0.3868\n",
            "Epoch 3 Batch 2550 Loss 1.5691 Accuracy 0.3870\n",
            "Epoch 3 Batch 2600 Loss 1.5653 Accuracy 0.3874\n",
            "Epoch 3 Batch 2650 Loss 1.5616 Accuracy 0.3876\n",
            "Epoch 3 Batch 2700 Loss 1.5579 Accuracy 0.3881\n",
            "Epoch 3 Batch 2750 Loss 1.5541 Accuracy 0.3884\n",
            "Epoch 3 Batch 2800 Loss 1.5502 Accuracy 0.3888\n",
            "Epoch 3 Batch 2850 Loss 1.5462 Accuracy 0.3893\n",
            "Epoch 3 Batch 2900 Loss 1.5423 Accuracy 0.3897\n",
            "Epoch 3 Batch 2950 Loss 1.5386 Accuracy 0.3903\n",
            "Epoch 3 Batch 3000 Loss 1.5342 Accuracy 0.3909\n",
            "Epoch 3 Batch 3050 Loss 1.5302 Accuracy 0.3915\n",
            "Epoch 3 Batch 3100 Loss 1.5262 Accuracy 0.3920\n",
            "Epoch 3 Batch 3150 Loss 1.5219 Accuracy 0.3927\n",
            "Epoch 3 Batch 3200 Loss 1.5182 Accuracy 0.3932\n",
            "Epoch 3 Batch 3250 Loss 1.5147 Accuracy 0.3937\n",
            "Epoch 3 Batch 3300 Loss 1.5110 Accuracy 0.3942\n",
            "Epoch 3 Batch 3350 Loss 1.5079 Accuracy 0.3946\n",
            "Epoch 3 Batch 3400 Loss 1.5048 Accuracy 0.3951\n",
            "Epoch 3 Batch 3450 Loss 1.5022 Accuracy 0.3955\n",
            "Epoch 3 Batch 3500 Loss 1.4994 Accuracy 0.3960\n",
            "Epoch 3 Batch 3550 Loss 1.4968 Accuracy 0.3963\n",
            "Epoch 3 Batch 3600 Loss 1.4944 Accuracy 0.3966\n",
            "Epoch 3 Batch 3650 Loss 1.4915 Accuracy 0.3970\n",
            "Epoch 3 Batch 3700 Loss 1.4888 Accuracy 0.3973\n",
            "Epoch 3 Batch 3750 Loss 1.4858 Accuracy 0.3977\n",
            "Epoch 3 Batch 3800 Loss 1.4830 Accuracy 0.3981\n",
            "Epoch 3 Batch 3850 Loss 1.4804 Accuracy 0.3984\n",
            "Epoch 3 Batch 3900 Loss 1.4782 Accuracy 0.3988\n",
            "Epoch 3 Batch 3950 Loss 1.4757 Accuracy 0.3991\n",
            "Epoch 3 Batch 4000 Loss 1.4727 Accuracy 0.3995\n",
            "Epoch 3 Batch 4050 Loss 1.4707 Accuracy 0.3998\n",
            "Epoch 3 Batch 4100 Loss 1.4687 Accuracy 0.4001\n",
            "Epoch 3 Batch 4150 Loss 1.4661 Accuracy 0.4005\n",
            "Epoch 3 Batch 4200 Loss 1.4636 Accuracy 0.4009\n",
            "Epoch 3 Batch 4250 Loss 1.4613 Accuracy 0.4013\n",
            "Epoch 3 Batch 4300 Loss 1.4593 Accuracy 0.4016\n",
            "Epoch 3 Batch 4350 Loss 1.4576 Accuracy 0.4019\n",
            "Epoch 3 Batch 4400 Loss 1.4557 Accuracy 0.4023\n",
            "Epoch 3 Batch 4450 Loss 1.4537 Accuracy 0.4026\n",
            "Epoch 3 Batch 4500 Loss 1.4520 Accuracy 0.4030\n",
            "Epoch 3 Batch 4550 Loss 1.4500 Accuracy 0.4033\n",
            "Epoch 3 Batch 4600 Loss 1.4483 Accuracy 0.4036\n",
            "Epoch 3 Batch 4650 Loss 1.4463 Accuracy 0.4039\n",
            "Epoch 3 Batch 4700 Loss 1.4448 Accuracy 0.4041\n",
            "Epoch 3 Batch 4750 Loss 1.4440 Accuracy 0.4043\n",
            "Epoch 3 Batch 4800 Loss 1.4430 Accuracy 0.4044\n",
            "Epoch 3 Batch 4850 Loss 1.4429 Accuracy 0.4045\n",
            "Epoch 3 Batch 4900 Loss 1.4427 Accuracy 0.4045\n",
            "Epoch 3 Batch 4950 Loss 1.4427 Accuracy 0.4045\n",
            "Epoch 3 Batch 5000 Loss 1.4426 Accuracy 0.4046\n",
            "Epoch 3 Batch 5050 Loss 1.4429 Accuracy 0.4046\n",
            "Epoch 3 Batch 5100 Loss 1.4433 Accuracy 0.4045\n",
            "Epoch 3 Batch 5150 Loss 1.4435 Accuracy 0.4045\n",
            "Epoch 3 Batch 5200 Loss 1.4438 Accuracy 0.4044\n",
            "Epoch 3 Batch 5250 Loss 1.4441 Accuracy 0.4044\n",
            "Epoch 3 Batch 5300 Loss 1.4443 Accuracy 0.4044\n",
            "Epoch 3 Batch 5350 Loss 1.4443 Accuracy 0.4044\n",
            "Epoch 3 Batch 5400 Loss 1.4444 Accuracy 0.4044\n",
            "Epoch 3 Batch 5450 Loss 1.4446 Accuracy 0.4044\n",
            "Epoch 3 Batch 5500 Loss 1.4447 Accuracy 0.4045\n",
            "Epoch 3 Batch 5550 Loss 1.4446 Accuracy 0.4045\n",
            "Epoch 3 Batch 5600 Loss 1.4448 Accuracy 0.4044\n",
            "Epoch 3 Batch 5650 Loss 1.4450 Accuracy 0.4044\n",
            "Epoch 3 Batch 5700 Loss 1.4455 Accuracy 0.4044\n",
            "Epoch 3 Batch 5750 Loss 1.4460 Accuracy 0.4043\n",
            "Epoch 3 Batch 5800 Loss 1.4464 Accuracy 0.4043\n",
            "Epoch 3 Batch 5850 Loss 1.4466 Accuracy 0.4042\n",
            "Epoch 3 Batch 5900 Loss 1.4470 Accuracy 0.4042\n",
            "Epoch 3 Batch 5950 Loss 1.4478 Accuracy 0.4041\n",
            "Epoch 3 Batch 6000 Loss 1.4482 Accuracy 0.4040\n",
            "Epoch 3 Batch 6050 Loss 1.4487 Accuracy 0.4039\n",
            "Epoch 3 Batch 6100 Loss 1.4493 Accuracy 0.4038\n",
            "Epoch 3 Batch 6150 Loss 1.4500 Accuracy 0.4037\n",
            "Epoch 3 Batch 6200 Loss 1.4507 Accuracy 0.4036\n",
            "Epoch 3 Batch 6250 Loss 1.4513 Accuracy 0.4034\n",
            "Epoch 3 Batch 6300 Loss 1.4517 Accuracy 0.4033\n",
            "Epoch 3 Batch 6350 Loss 1.4521 Accuracy 0.4031\n",
            "Epoch 3 Batch 6400 Loss 1.4525 Accuracy 0.4030\n",
            "Epoch 3 Batch 6450 Loss 1.4526 Accuracy 0.4030\n",
            "Epoch 3 Batch 6500 Loss 1.4529 Accuracy 0.4029\n",
            "Epoch 3 Batch 6550 Loss 1.4534 Accuracy 0.4028\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 2081.983320951462 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.6608 Accuracy 0.3947\n",
            "Epoch 4 Batch 50 Loss 1.5580 Accuracy 0.3939\n",
            "Epoch 4 Batch 100 Loss 1.5516 Accuracy 0.3952\n",
            "Epoch 4 Batch 150 Loss 1.5381 Accuracy 0.3957\n",
            "Epoch 4 Batch 200 Loss 1.5376 Accuracy 0.3966\n",
            "Epoch 4 Batch 250 Loss 1.5338 Accuracy 0.3966\n",
            "Epoch 4 Batch 300 Loss 1.5271 Accuracy 0.3970\n",
            "Epoch 4 Batch 350 Loss 1.5205 Accuracy 0.3980\n",
            "Epoch 4 Batch 400 Loss 1.5191 Accuracy 0.3977\n",
            "Epoch 4 Batch 450 Loss 1.5133 Accuracy 0.3986\n",
            "Epoch 4 Batch 500 Loss 1.5116 Accuracy 0.3996\n",
            "Epoch 4 Batch 550 Loss 1.5103 Accuracy 0.3998\n",
            "Epoch 4 Batch 600 Loss 1.5087 Accuracy 0.3997\n",
            "Epoch 4 Batch 650 Loss 1.5065 Accuracy 0.3997\n",
            "Epoch 4 Batch 700 Loss 1.5053 Accuracy 0.3997\n",
            "Epoch 4 Batch 750 Loss 1.5066 Accuracy 0.3997\n",
            "Epoch 4 Batch 800 Loss 1.5078 Accuracy 0.3997\n",
            "Epoch 4 Batch 850 Loss 1.5082 Accuracy 0.3996\n",
            "Epoch 4 Batch 900 Loss 1.5078 Accuracy 0.3998\n",
            "Epoch 4 Batch 950 Loss 1.5063 Accuracy 0.3996\n",
            "Epoch 4 Batch 1000 Loss 1.5056 Accuracy 0.3993\n",
            "Epoch 4 Batch 1050 Loss 1.5054 Accuracy 0.3993\n",
            "Epoch 4 Batch 1100 Loss 1.5059 Accuracy 0.3990\n",
            "Epoch 4 Batch 1150 Loss 1.5054 Accuracy 0.3989\n",
            "Epoch 4 Batch 1200 Loss 1.5033 Accuracy 0.3985\n",
            "Epoch 4 Batch 1250 Loss 1.5022 Accuracy 0.3983\n",
            "Epoch 4 Batch 1300 Loss 1.5022 Accuracy 0.3982\n",
            "Epoch 4 Batch 1350 Loss 1.5024 Accuracy 0.3980\n",
            "Epoch 4 Batch 1400 Loss 1.5031 Accuracy 0.3978\n",
            "Epoch 4 Batch 1450 Loss 1.5031 Accuracy 0.3974\n",
            "Epoch 4 Batch 1500 Loss 1.5019 Accuracy 0.3973\n",
            "Epoch 4 Batch 1550 Loss 1.5017 Accuracy 0.3972\n",
            "Epoch 4 Batch 1600 Loss 1.4998 Accuracy 0.3973\n",
            "Epoch 4 Batch 1650 Loss 1.4986 Accuracy 0.3975\n",
            "Epoch 4 Batch 1700 Loss 1.4975 Accuracy 0.3977\n",
            "Epoch 4 Batch 1750 Loss 1.4957 Accuracy 0.3979\n",
            "Epoch 4 Batch 1800 Loss 1.4950 Accuracy 0.3980\n",
            "Epoch 4 Batch 1850 Loss 1.4933 Accuracy 0.3982\n",
            "Epoch 4 Batch 1900 Loss 1.4916 Accuracy 0.3984\n",
            "Epoch 4 Batch 1950 Loss 1.4908 Accuracy 0.3986\n",
            "Epoch 4 Batch 2000 Loss 1.4895 Accuracy 0.3988\n",
            "Epoch 4 Batch 2050 Loss 1.4876 Accuracy 0.3991\n",
            "Epoch 4 Batch 2100 Loss 1.4857 Accuracy 0.3994\n",
            "Epoch 4 Batch 2150 Loss 1.4838 Accuracy 0.3995\n",
            "Epoch 4 Batch 2200 Loss 1.4821 Accuracy 0.3997\n",
            "Epoch 4 Batch 2250 Loss 1.4807 Accuracy 0.3999\n",
            "Epoch 4 Batch 2300 Loss 1.4788 Accuracy 0.4001\n",
            "Epoch 4 Batch 2350 Loss 1.4760 Accuracy 0.4004\n",
            "Epoch 4 Batch 2400 Loss 1.4729 Accuracy 0.4007\n",
            "Epoch 4 Batch 2450 Loss 1.4699 Accuracy 0.4010\n",
            "Epoch 4 Batch 2500 Loss 1.4660 Accuracy 0.4012\n",
            "Epoch 4 Batch 2550 Loss 1.4627 Accuracy 0.4014\n",
            "Epoch 4 Batch 2600 Loss 1.4595 Accuracy 0.4018\n",
            "Epoch 4 Batch 2650 Loss 1.4564 Accuracy 0.4020\n",
            "Epoch 4 Batch 2700 Loss 1.4530 Accuracy 0.4023\n",
            "Epoch 4 Batch 2750 Loss 1.4494 Accuracy 0.4027\n",
            "Epoch 4 Batch 2800 Loss 1.4457 Accuracy 0.4031\n",
            "Epoch 4 Batch 2850 Loss 1.4426 Accuracy 0.4036\n",
            "Epoch 4 Batch 2900 Loss 1.4393 Accuracy 0.4041\n",
            "Epoch 4 Batch 2950 Loss 1.4354 Accuracy 0.4045\n",
            "Epoch 4 Batch 3000 Loss 1.4316 Accuracy 0.4051\n",
            "Epoch 4 Batch 3050 Loss 1.4282 Accuracy 0.4056\n",
            "Epoch 4 Batch 3100 Loss 1.4245 Accuracy 0.4062\n",
            "Epoch 4 Batch 3150 Loss 1.4211 Accuracy 0.4068\n",
            "Epoch 4 Batch 3200 Loss 1.4177 Accuracy 0.4074\n",
            "Epoch 4 Batch 3250 Loss 1.4139 Accuracy 0.4080\n",
            "Epoch 4 Batch 3300 Loss 1.4107 Accuracy 0.4084\n",
            "Epoch 4 Batch 3350 Loss 1.4079 Accuracy 0.4089\n",
            "Epoch 4 Batch 3400 Loss 1.4048 Accuracy 0.4093\n",
            "Epoch 4 Batch 3450 Loss 1.4022 Accuracy 0.4097\n",
            "Epoch 4 Batch 3500 Loss 1.4000 Accuracy 0.4101\n",
            "Epoch 4 Batch 3550 Loss 1.3977 Accuracy 0.4105\n",
            "Epoch 4 Batch 3600 Loss 1.3950 Accuracy 0.4108\n",
            "Epoch 4 Batch 3650 Loss 1.3931 Accuracy 0.4111\n",
            "Epoch 4 Batch 3700 Loss 1.3906 Accuracy 0.4114\n",
            "Epoch 4 Batch 3750 Loss 1.3879 Accuracy 0.4117\n",
            "Epoch 4 Batch 3800 Loss 1.3853 Accuracy 0.4120\n",
            "Epoch 4 Batch 3850 Loss 1.3826 Accuracy 0.4123\n",
            "Epoch 4 Batch 3900 Loss 1.3800 Accuracy 0.4125\n",
            "Epoch 4 Batch 3950 Loss 1.3776 Accuracy 0.4129\n",
            "Epoch 4 Batch 4000 Loss 1.3754 Accuracy 0.4131\n",
            "Epoch 4 Batch 4050 Loss 1.3733 Accuracy 0.4134\n",
            "Epoch 4 Batch 4100 Loss 1.3710 Accuracy 0.4137\n",
            "Epoch 4 Batch 4150 Loss 1.3687 Accuracy 0.4140\n",
            "Epoch 4 Batch 4200 Loss 1.3669 Accuracy 0.4144\n",
            "Epoch 4 Batch 4250 Loss 1.3652 Accuracy 0.4147\n",
            "Epoch 4 Batch 4300 Loss 1.3634 Accuracy 0.4150\n",
            "Epoch 4 Batch 4350 Loss 1.3618 Accuracy 0.4153\n",
            "Epoch 4 Batch 4400 Loss 1.3602 Accuracy 0.4157\n",
            "Epoch 4 Batch 4450 Loss 1.3587 Accuracy 0.4160\n",
            "Epoch 4 Batch 4500 Loss 1.3569 Accuracy 0.4163\n",
            "Epoch 4 Batch 4550 Loss 1.3551 Accuracy 0.4166\n",
            "Epoch 4 Batch 4600 Loss 1.3535 Accuracy 0.4170\n",
            "Epoch 4 Batch 4650 Loss 1.3518 Accuracy 0.4173\n",
            "Epoch 4 Batch 4700 Loss 1.3503 Accuracy 0.4175\n",
            "Epoch 4 Batch 4750 Loss 1.3499 Accuracy 0.4176\n",
            "Epoch 4 Batch 4800 Loss 1.3495 Accuracy 0.4176\n",
            "Epoch 4 Batch 4850 Loss 1.3491 Accuracy 0.4177\n",
            "Epoch 4 Batch 4900 Loss 1.3491 Accuracy 0.4177\n",
            "Epoch 4 Batch 4950 Loss 1.3493 Accuracy 0.4177\n",
            "Epoch 4 Batch 5000 Loss 1.3494 Accuracy 0.4177\n",
            "Epoch 4 Batch 5050 Loss 1.3497 Accuracy 0.4177\n",
            "Epoch 4 Batch 5100 Loss 1.3501 Accuracy 0.4176\n",
            "Epoch 4 Batch 5150 Loss 1.3502 Accuracy 0.4176\n",
            "Epoch 4 Batch 5200 Loss 1.3503 Accuracy 0.4175\n",
            "Epoch 4 Batch 5250 Loss 1.3508 Accuracy 0.4175\n",
            "Epoch 4 Batch 5300 Loss 1.3510 Accuracy 0.4174\n",
            "Epoch 4 Batch 5350 Loss 1.3514 Accuracy 0.4174\n",
            "Epoch 4 Batch 5400 Loss 1.3520 Accuracy 0.4174\n",
            "Epoch 4 Batch 5450 Loss 1.3527 Accuracy 0.4173\n",
            "Epoch 4 Batch 5500 Loss 1.3533 Accuracy 0.4173\n",
            "Epoch 4 Batch 5550 Loss 1.3537 Accuracy 0.4173\n",
            "Epoch 4 Batch 5600 Loss 1.3541 Accuracy 0.4173\n",
            "Epoch 4 Batch 5650 Loss 1.3546 Accuracy 0.4172\n",
            "Epoch 4 Batch 5700 Loss 1.3552 Accuracy 0.4171\n",
            "Epoch 4 Batch 5750 Loss 1.3557 Accuracy 0.4171\n",
            "Epoch 4 Batch 5800 Loss 1.3563 Accuracy 0.4170\n",
            "Epoch 4 Batch 5850 Loss 1.3571 Accuracy 0.4169\n",
            "Epoch 4 Batch 5900 Loss 1.3577 Accuracy 0.4168\n",
            "Epoch 4 Batch 5950 Loss 1.3584 Accuracy 0.4167\n",
            "Epoch 4 Batch 6000 Loss 1.3590 Accuracy 0.4166\n",
            "Epoch 4 Batch 6050 Loss 1.3597 Accuracy 0.4164\n",
            "Epoch 4 Batch 6100 Loss 1.3603 Accuracy 0.4163\n",
            "Epoch 4 Batch 6150 Loss 1.3611 Accuracy 0.4162\n",
            "Epoch 4 Batch 6200 Loss 1.3619 Accuracy 0.4161\n",
            "Epoch 4 Batch 6250 Loss 1.3626 Accuracy 0.4159\n",
            "Epoch 4 Batch 6300 Loss 1.3632 Accuracy 0.4158\n",
            "Epoch 4 Batch 6350 Loss 1.3637 Accuracy 0.4157\n",
            "Epoch 4 Batch 6400 Loss 1.3644 Accuracy 0.4156\n",
            "Epoch 4 Batch 6450 Loss 1.3649 Accuracy 0.4155\n",
            "Epoch 4 Batch 6500 Loss 1.3653 Accuracy 0.4153\n",
            "Epoch 4 Batch 6550 Loss 1.3658 Accuracy 0.4152\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 2153.7682600021362 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.4777 Accuracy 0.3618\n",
            "Epoch 5 Batch 50 Loss 1.4911 Accuracy 0.4031\n",
            "Epoch 5 Batch 100 Loss 1.4858 Accuracy 0.4028\n",
            "Epoch 5 Batch 150 Loss 1.4771 Accuracy 0.4033\n",
            "Epoch 5 Batch 200 Loss 1.4798 Accuracy 0.4043\n",
            "Epoch 5 Batch 250 Loss 1.4705 Accuracy 0.4058\n",
            "Epoch 5 Batch 300 Loss 1.4673 Accuracy 0.4077\n",
            "Epoch 5 Batch 350 Loss 1.4630 Accuracy 0.4078\n",
            "Epoch 5 Batch 400 Loss 1.4566 Accuracy 0.4081\n",
            "Epoch 5 Batch 450 Loss 1.4521 Accuracy 0.4087\n",
            "Epoch 5 Batch 500 Loss 1.4499 Accuracy 0.4085\n",
            "Epoch 5 Batch 550 Loss 1.4473 Accuracy 0.4083\n",
            "Epoch 5 Batch 600 Loss 1.4456 Accuracy 0.4082\n",
            "Epoch 5 Batch 650 Loss 1.4430 Accuracy 0.4084\n",
            "Epoch 5 Batch 700 Loss 1.4432 Accuracy 0.4085\n",
            "Epoch 5 Batch 750 Loss 1.4423 Accuracy 0.4084\n",
            "Epoch 5 Batch 800 Loss 1.4412 Accuracy 0.4084\n",
            "Epoch 5 Batch 850 Loss 1.4392 Accuracy 0.4085\n",
            "Epoch 5 Batch 900 Loss 1.4410 Accuracy 0.4086\n",
            "Epoch 5 Batch 950 Loss 1.4409 Accuracy 0.4083\n",
            "Epoch 5 Batch 1000 Loss 1.4409 Accuracy 0.4083\n",
            "Epoch 5 Batch 1050 Loss 1.4420 Accuracy 0.4081\n",
            "Epoch 5 Batch 1100 Loss 1.4404 Accuracy 0.4083\n",
            "Epoch 5 Batch 1150 Loss 1.4391 Accuracy 0.4077\n",
            "Epoch 5 Batch 1200 Loss 1.4388 Accuracy 0.4076\n",
            "Epoch 5 Batch 1250 Loss 1.4386 Accuracy 0.4071\n",
            "Epoch 5 Batch 1300 Loss 1.4381 Accuracy 0.4068\n",
            "Epoch 5 Batch 1350 Loss 1.4383 Accuracy 0.4065\n",
            "Epoch 5 Batch 1400 Loss 1.4391 Accuracy 0.4060\n",
            "Epoch 5 Batch 1450 Loss 1.4398 Accuracy 0.4060\n",
            "Epoch 5 Batch 1500 Loss 1.4394 Accuracy 0.4060\n",
            "Epoch 5 Batch 1550 Loss 1.4400 Accuracy 0.4060\n",
            "Epoch 5 Batch 1600 Loss 1.4396 Accuracy 0.4060\n",
            "Epoch 5 Batch 1650 Loss 1.4395 Accuracy 0.4061\n",
            "Epoch 5 Batch 1700 Loss 1.4387 Accuracy 0.4063\n",
            "Epoch 5 Batch 1750 Loss 1.4378 Accuracy 0.4065\n",
            "Epoch 5 Batch 1800 Loss 1.4364 Accuracy 0.4066\n",
            "Epoch 5 Batch 1850 Loss 1.4348 Accuracy 0.4068\n",
            "Epoch 5 Batch 1900 Loss 1.4337 Accuracy 0.4070\n",
            "Epoch 5 Batch 1950 Loss 1.4320 Accuracy 0.4072\n",
            "Epoch 5 Batch 2000 Loss 1.4303 Accuracy 0.4073\n",
            "Epoch 5 Batch 2050 Loss 1.4280 Accuracy 0.4075\n",
            "Epoch 5 Batch 2100 Loss 1.4259 Accuracy 0.4077\n",
            "Epoch 5 Batch 2150 Loss 1.4242 Accuracy 0.4079\n",
            "Epoch 5 Batch 2200 Loss 1.4228 Accuracy 0.4082\n",
            "Epoch 5 Batch 2250 Loss 1.4205 Accuracy 0.4083\n",
            "Epoch 5 Batch 2300 Loss 1.4185 Accuracy 0.4085\n",
            "Epoch 5 Batch 2350 Loss 1.4159 Accuracy 0.4088\n",
            "Epoch 5 Batch 2400 Loss 1.4135 Accuracy 0.4091\n",
            "Epoch 5 Batch 2450 Loss 1.4107 Accuracy 0.4093\n",
            "Epoch 5 Batch 2500 Loss 1.4078 Accuracy 0.4095\n",
            "Epoch 5 Batch 2550 Loss 1.4047 Accuracy 0.4097\n",
            "Epoch 5 Batch 2600 Loss 1.4017 Accuracy 0.4100\n",
            "Epoch 5 Batch 2650 Loss 1.3982 Accuracy 0.4104\n",
            "Epoch 5 Batch 2700 Loss 1.3950 Accuracy 0.4107\n",
            "Epoch 5 Batch 2750 Loss 1.3920 Accuracy 0.4111\n",
            "Epoch 5 Batch 2800 Loss 1.3882 Accuracy 0.4114\n",
            "Epoch 5 Batch 2850 Loss 1.3843 Accuracy 0.4119\n",
            "Epoch 5 Batch 2900 Loss 1.3808 Accuracy 0.4124\n",
            "Epoch 5 Batch 2950 Loss 1.3771 Accuracy 0.4128\n",
            "Epoch 5 Batch 3000 Loss 1.3737 Accuracy 0.4133\n",
            "Epoch 5 Batch 3050 Loss 1.3693 Accuracy 0.4139\n",
            "Epoch 5 Batch 3100 Loss 1.3659 Accuracy 0.4144\n",
            "Epoch 5 Batch 3150 Loss 1.3622 Accuracy 0.4150\n",
            "Epoch 5 Batch 3200 Loss 1.3586 Accuracy 0.4155\n",
            "Epoch 5 Batch 3250 Loss 1.3556 Accuracy 0.4160\n",
            "Epoch 5 Batch 3300 Loss 1.3524 Accuracy 0.4164\n",
            "Epoch 5 Batch 3350 Loss 1.3494 Accuracy 0.4169\n",
            "Epoch 5 Batch 3400 Loss 1.3470 Accuracy 0.4173\n",
            "Epoch 5 Batch 3450 Loss 1.3446 Accuracy 0.4177\n",
            "Epoch 5 Batch 3500 Loss 1.3421 Accuracy 0.4180\n",
            "Epoch 5 Batch 3550 Loss 1.3398 Accuracy 0.4184\n",
            "Epoch 5 Batch 3600 Loss 1.3374 Accuracy 0.4188\n",
            "Epoch 5 Batch 3650 Loss 1.3350 Accuracy 0.4191\n",
            "Epoch 5 Batch 3700 Loss 1.3325 Accuracy 0.4194\n",
            "Epoch 5 Batch 3750 Loss 1.3300 Accuracy 0.4197\n",
            "Epoch 5 Batch 3800 Loss 1.3273 Accuracy 0.4200\n",
            "Epoch 5 Batch 3850 Loss 1.3251 Accuracy 0.4203\n",
            "Epoch 5 Batch 3900 Loss 1.3231 Accuracy 0.4206\n",
            "Epoch 5 Batch 3950 Loss 1.3208 Accuracy 0.4209\n",
            "Epoch 5 Batch 4000 Loss 1.3185 Accuracy 0.4213\n",
            "Epoch 5 Batch 4050 Loss 1.3164 Accuracy 0.4216\n",
            "Epoch 5 Batch 4100 Loss 1.3139 Accuracy 0.4219\n",
            "Epoch 5 Batch 4150 Loss 1.3118 Accuracy 0.4222\n",
            "Epoch 5 Batch 4200 Loss 1.3097 Accuracy 0.4225\n",
            "Epoch 5 Batch 4250 Loss 1.3081 Accuracy 0.4228\n",
            "Epoch 5 Batch 4300 Loss 1.3065 Accuracy 0.4231\n",
            "Epoch 5 Batch 4350 Loss 1.3050 Accuracy 0.4234\n",
            "Epoch 5 Batch 4400 Loss 1.3031 Accuracy 0.4237\n",
            "Epoch 5 Batch 4450 Loss 1.3018 Accuracy 0.4240\n",
            "Epoch 5 Batch 4500 Loss 1.3002 Accuracy 0.4243\n",
            "Epoch 5 Batch 4550 Loss 1.2987 Accuracy 0.4246\n",
            "Epoch 5 Batch 4600 Loss 1.2975 Accuracy 0.4249\n",
            "Epoch 5 Batch 4650 Loss 1.2959 Accuracy 0.4252\n",
            "Epoch 5 Batch 4700 Loss 1.2947 Accuracy 0.4254\n",
            "Epoch 5 Batch 4750 Loss 1.2938 Accuracy 0.4255\n",
            "Epoch 5 Batch 4800 Loss 1.2936 Accuracy 0.4256\n",
            "Epoch 5 Batch 4850 Loss 1.2933 Accuracy 0.4256\n",
            "Epoch 5 Batch 4900 Loss 1.2932 Accuracy 0.4256\n",
            "Epoch 5 Batch 4950 Loss 1.2935 Accuracy 0.4255\n",
            "Epoch 5 Batch 5000 Loss 1.2940 Accuracy 0.4255\n",
            "Epoch 5 Batch 5050 Loss 1.2944 Accuracy 0.4255\n",
            "Epoch 5 Batch 5100 Loss 1.2949 Accuracy 0.4254\n",
            "Epoch 5 Batch 5150 Loss 1.2953 Accuracy 0.4254\n",
            "Epoch 5 Batch 5200 Loss 1.2956 Accuracy 0.4254\n",
            "Epoch 5 Batch 5250 Loss 1.2960 Accuracy 0.4253\n",
            "Epoch 5 Batch 5300 Loss 1.2964 Accuracy 0.4252\n",
            "Epoch 5 Batch 5350 Loss 1.2966 Accuracy 0.4252\n",
            "Epoch 5 Batch 5400 Loss 1.2971 Accuracy 0.4251\n",
            "Epoch 5 Batch 5450 Loss 1.2974 Accuracy 0.4251\n",
            "Epoch 5 Batch 5500 Loss 1.2980 Accuracy 0.4250\n",
            "Epoch 5 Batch 5550 Loss 1.2986 Accuracy 0.4249\n",
            "Epoch 5 Batch 5600 Loss 1.2991 Accuracy 0.4249\n",
            "Epoch 5 Batch 5650 Loss 1.2993 Accuracy 0.4249\n",
            "Epoch 5 Batch 5700 Loss 1.2997 Accuracy 0.4248\n",
            "Epoch 5 Batch 5750 Loss 1.3003 Accuracy 0.4248\n",
            "Epoch 5 Batch 5800 Loss 1.3010 Accuracy 0.4247\n",
            "Epoch 5 Batch 5850 Loss 1.3020 Accuracy 0.4246\n",
            "Epoch 5 Batch 5900 Loss 1.3029 Accuracy 0.4245\n",
            "Epoch 5 Batch 5950 Loss 1.3035 Accuracy 0.4244\n",
            "Epoch 5 Batch 6000 Loss 1.3044 Accuracy 0.4243\n",
            "Epoch 5 Batch 6050 Loss 1.3054 Accuracy 0.4242\n",
            "Epoch 5 Batch 6100 Loss 1.3062 Accuracy 0.4240\n",
            "Epoch 5 Batch 6150 Loss 1.3071 Accuracy 0.4238\n",
            "Epoch 5 Batch 6200 Loss 1.3080 Accuracy 0.4236\n",
            "Epoch 5 Batch 6250 Loss 1.3087 Accuracy 0.4235\n",
            "Epoch 5 Batch 6300 Loss 1.3093 Accuracy 0.4234\n",
            "Epoch 5 Batch 6350 Loss 1.3100 Accuracy 0.4232\n",
            "Epoch 5 Batch 6400 Loss 1.3107 Accuracy 0.4231\n",
            "Epoch 5 Batch 6450 Loss 1.3115 Accuracy 0.4230\n",
            "Epoch 5 Batch 6500 Loss 1.3119 Accuracy 0.4229\n",
            "Epoch 5 Batch 6550 Loss 1.3126 Accuracy 0.4227\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 2184.4334881305695 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.3326 Accuracy 0.4268\n",
            "Epoch 6 Batch 50 Loss 1.4497 Accuracy 0.4080\n",
            "Epoch 6 Batch 100 Loss 1.4493 Accuracy 0.4089\n",
            "Epoch 6 Batch 150 Loss 1.4334 Accuracy 0.4102\n",
            "Epoch 6 Batch 200 Loss 1.4291 Accuracy 0.4115\n",
            "Epoch 6 Batch 250 Loss 1.4303 Accuracy 0.4122\n",
            "Epoch 6 Batch 300 Loss 1.4210 Accuracy 0.4139\n",
            "Epoch 6 Batch 350 Loss 1.4164 Accuracy 0.4141\n",
            "Epoch 6 Batch 400 Loss 1.4129 Accuracy 0.4139\n",
            "Epoch 6 Batch 450 Loss 1.4094 Accuracy 0.4140\n",
            "Epoch 6 Batch 500 Loss 1.4062 Accuracy 0.4148\n",
            "Epoch 6 Batch 550 Loss 1.4033 Accuracy 0.4141\n",
            "Epoch 6 Batch 600 Loss 1.4022 Accuracy 0.4142\n",
            "Epoch 6 Batch 650 Loss 1.4021 Accuracy 0.4142\n",
            "Epoch 6 Batch 700 Loss 1.4012 Accuracy 0.4142\n",
            "Epoch 6 Batch 750 Loss 1.4014 Accuracy 0.4143\n",
            "Epoch 6 Batch 800 Loss 1.4010 Accuracy 0.4140\n",
            "Epoch 6 Batch 850 Loss 1.4013 Accuracy 0.4141\n",
            "Epoch 6 Batch 900 Loss 1.4016 Accuracy 0.4142\n",
            "Epoch 6 Batch 950 Loss 1.4009 Accuracy 0.4141\n",
            "Epoch 6 Batch 1000 Loss 1.4009 Accuracy 0.4140\n",
            "Epoch 6 Batch 1050 Loss 1.3996 Accuracy 0.4139\n",
            "Epoch 6 Batch 1100 Loss 1.3991 Accuracy 0.4136\n",
            "Epoch 6 Batch 1150 Loss 1.3978 Accuracy 0.4135\n",
            "Epoch 6 Batch 1200 Loss 1.3976 Accuracy 0.4132\n",
            "Epoch 6 Batch 1250 Loss 1.3977 Accuracy 0.4130\n",
            "Epoch 6 Batch 1300 Loss 1.3977 Accuracy 0.4127\n",
            "Epoch 6 Batch 1350 Loss 1.3977 Accuracy 0.4126\n",
            "Epoch 6 Batch 1400 Loss 1.3977 Accuracy 0.4122\n",
            "Epoch 6 Batch 1450 Loss 1.3967 Accuracy 0.4120\n",
            "Epoch 6 Batch 1500 Loss 1.3971 Accuracy 0.4120\n",
            "Epoch 6 Batch 1550 Loss 1.3973 Accuracy 0.4121\n",
            "Epoch 6 Batch 1600 Loss 1.3964 Accuracy 0.4121\n",
            "Epoch 6 Batch 1650 Loss 1.3958 Accuracy 0.4121\n",
            "Epoch 6 Batch 1700 Loss 1.3951 Accuracy 0.4123\n",
            "Epoch 6 Batch 1750 Loss 1.3940 Accuracy 0.4125\n",
            "Epoch 6 Batch 1800 Loss 1.3918 Accuracy 0.4127\n",
            "Epoch 6 Batch 1850 Loss 1.3905 Accuracy 0.4130\n",
            "Epoch 6 Batch 1900 Loss 1.3891 Accuracy 0.4132\n",
            "Epoch 6 Batch 1950 Loss 1.3880 Accuracy 0.4133\n",
            "Epoch 6 Batch 2000 Loss 1.3871 Accuracy 0.4135\n",
            "Epoch 6 Batch 2050 Loss 1.3858 Accuracy 0.4136\n",
            "Epoch 6 Batch 2100 Loss 1.3839 Accuracy 0.4138\n",
            "Epoch 6 Batch 2150 Loss 1.3820 Accuracy 0.4140\n",
            "Epoch 6 Batch 2200 Loss 1.3803 Accuracy 0.4142\n",
            "Epoch 6 Batch 2250 Loss 1.3785 Accuracy 0.4144\n",
            "Epoch 6 Batch 2300 Loss 1.3767 Accuracy 0.4147\n",
            "Epoch 6 Batch 2350 Loss 1.3751 Accuracy 0.4149\n",
            "Epoch 6 Batch 2400 Loss 1.3729 Accuracy 0.4151\n",
            "Epoch 6 Batch 2450 Loss 1.3699 Accuracy 0.4153\n",
            "Epoch 6 Batch 2500 Loss 1.3667 Accuracy 0.4156\n",
            "Epoch 6 Batch 2550 Loss 1.3631 Accuracy 0.4157\n",
            "Epoch 6 Batch 2600 Loss 1.3597 Accuracy 0.4160\n",
            "Epoch 6 Batch 2650 Loss 1.3561 Accuracy 0.4162\n",
            "Epoch 6 Batch 2700 Loss 1.3533 Accuracy 0.4166\n",
            "Epoch 6 Batch 2750 Loss 1.3503 Accuracy 0.4170\n",
            "Epoch 6 Batch 2800 Loss 1.3474 Accuracy 0.4172\n",
            "Epoch 6 Batch 2850 Loss 1.3438 Accuracy 0.4177\n",
            "Epoch 6 Batch 2900 Loss 1.3404 Accuracy 0.4181\n",
            "Epoch 6 Batch 2950 Loss 1.3364 Accuracy 0.4186\n",
            "Epoch 6 Batch 3000 Loss 1.3323 Accuracy 0.4192\n",
            "Epoch 6 Batch 3050 Loss 1.3287 Accuracy 0.4197\n",
            "Epoch 6 Batch 3100 Loss 1.3250 Accuracy 0.4203\n",
            "Epoch 6 Batch 3150 Loss 1.3212 Accuracy 0.4210\n",
            "Epoch 6 Batch 3200 Loss 1.3179 Accuracy 0.4215\n",
            "Epoch 6 Batch 3250 Loss 1.3147 Accuracy 0.4220\n",
            "Epoch 6 Batch 3300 Loss 1.3116 Accuracy 0.4225\n",
            "Epoch 6 Batch 3350 Loss 1.3089 Accuracy 0.4229\n",
            "Epoch 6 Batch 3400 Loss 1.3064 Accuracy 0.4233\n",
            "Epoch 6 Batch 3450 Loss 1.3040 Accuracy 0.4237\n",
            "Epoch 6 Batch 3500 Loss 1.3015 Accuracy 0.4241\n",
            "Epoch 6 Batch 3550 Loss 1.2993 Accuracy 0.4244\n",
            "Epoch 6 Batch 3600 Loss 1.2970 Accuracy 0.4247\n",
            "Epoch 6 Batch 3650 Loss 1.2948 Accuracy 0.4249\n",
            "Epoch 6 Batch 3700 Loss 1.2924 Accuracy 0.4252\n",
            "Epoch 6 Batch 3750 Loss 1.2901 Accuracy 0.4255\n",
            "Epoch 6 Batch 3800 Loss 1.2879 Accuracy 0.4257\n",
            "Epoch 6 Batch 3850 Loss 1.2854 Accuracy 0.4260\n",
            "Epoch 6 Batch 3900 Loss 1.2832 Accuracy 0.4263\n",
            "Epoch 6 Batch 3950 Loss 1.2808 Accuracy 0.4266\n",
            "Epoch 6 Batch 4000 Loss 1.2787 Accuracy 0.4269\n",
            "Epoch 6 Batch 4050 Loss 1.2771 Accuracy 0.4272\n",
            "Epoch 6 Batch 4100 Loss 1.2751 Accuracy 0.4275\n",
            "Epoch 6 Batch 4150 Loss 1.2729 Accuracy 0.4279\n",
            "Epoch 6 Batch 4200 Loss 1.2709 Accuracy 0.4281\n",
            "Epoch 6 Batch 4250 Loss 1.2690 Accuracy 0.4284\n",
            "Epoch 6 Batch 4300 Loss 1.2671 Accuracy 0.4287\n",
            "Epoch 6 Batch 4350 Loss 1.2654 Accuracy 0.4291\n",
            "Epoch 6 Batch 4400 Loss 1.2639 Accuracy 0.4294\n",
            "Epoch 6 Batch 4450 Loss 1.2623 Accuracy 0.4297\n",
            "Epoch 6 Batch 4500 Loss 1.2612 Accuracy 0.4300\n",
            "Epoch 6 Batch 4550 Loss 1.2597 Accuracy 0.4302\n",
            "Epoch 6 Batch 4600 Loss 1.2580 Accuracy 0.4306\n",
            "Epoch 6 Batch 4650 Loss 1.2566 Accuracy 0.4308\n",
            "Epoch 6 Batch 4700 Loss 1.2553 Accuracy 0.4310\n",
            "Epoch 6 Batch 4750 Loss 1.2546 Accuracy 0.4312\n",
            "Epoch 6 Batch 4800 Loss 1.2542 Accuracy 0.4312\n",
            "Epoch 6 Batch 4850 Loss 1.2541 Accuracy 0.4313\n",
            "Epoch 6 Batch 4900 Loss 1.2541 Accuracy 0.4313\n",
            "Epoch 6 Batch 4950 Loss 1.2545 Accuracy 0.4313\n",
            "Epoch 6 Batch 5000 Loss 1.2549 Accuracy 0.4312\n",
            "Epoch 6 Batch 5050 Loss 1.2553 Accuracy 0.4312\n",
            "Epoch 6 Batch 5100 Loss 1.2557 Accuracy 0.4312\n",
            "Epoch 6 Batch 5150 Loss 1.2561 Accuracy 0.4310\n",
            "Epoch 6 Batch 5200 Loss 1.2567 Accuracy 0.4310\n",
            "Epoch 6 Batch 5250 Loss 1.2572 Accuracy 0.4310\n",
            "Epoch 6 Batch 5300 Loss 1.2576 Accuracy 0.4309\n",
            "Epoch 6 Batch 5350 Loss 1.2582 Accuracy 0.4309\n",
            "Epoch 6 Batch 5400 Loss 1.2587 Accuracy 0.4308\n",
            "Epoch 6 Batch 5450 Loss 1.2593 Accuracy 0.4308\n",
            "Epoch 6 Batch 5500 Loss 1.2599 Accuracy 0.4306\n",
            "Epoch 6 Batch 5550 Loss 1.2605 Accuracy 0.4306\n",
            "Epoch 6 Batch 5600 Loss 1.2614 Accuracy 0.4306\n",
            "Epoch 6 Batch 5650 Loss 1.2620 Accuracy 0.4305\n",
            "Epoch 6 Batch 5700 Loss 1.2622 Accuracy 0.4304\n",
            "Epoch 6 Batch 5750 Loss 1.2628 Accuracy 0.4303\n",
            "Epoch 6 Batch 5800 Loss 1.2638 Accuracy 0.4302\n",
            "Epoch 6 Batch 5850 Loss 1.2644 Accuracy 0.4301\n",
            "Epoch 6 Batch 5900 Loss 1.2651 Accuracy 0.4301\n",
            "Epoch 6 Batch 5950 Loss 1.2660 Accuracy 0.4299\n",
            "Epoch 6 Batch 6000 Loss 1.2668 Accuracy 0.4298\n",
            "Epoch 6 Batch 6050 Loss 1.2682 Accuracy 0.4296\n",
            "Epoch 6 Batch 6100 Loss 1.2690 Accuracy 0.4295\n",
            "Epoch 6 Batch 6150 Loss 1.2700 Accuracy 0.4293\n",
            "Epoch 6 Batch 6200 Loss 1.2708 Accuracy 0.4291\n",
            "Epoch 6 Batch 6250 Loss 1.2718 Accuracy 0.4289\n",
            "Epoch 6 Batch 6300 Loss 1.2724 Accuracy 0.4288\n",
            "Epoch 6 Batch 6350 Loss 1.2732 Accuracy 0.4287\n",
            "Epoch 6 Batch 6400 Loss 1.2739 Accuracy 0.4285\n",
            "Epoch 6 Batch 6450 Loss 1.2744 Accuracy 0.4284\n",
            "Epoch 6 Batch 6500 Loss 1.2750 Accuracy 0.4283\n",
            "Epoch 6 Batch 6550 Loss 1.2754 Accuracy 0.4282\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 2160.002251148224 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.3946 Accuracy 0.3988\n",
            "Epoch 7 Batch 50 Loss 1.3970 Accuracy 0.4099\n",
            "Epoch 7 Batch 100 Loss 1.4146 Accuracy 0.4100\n",
            "Epoch 7 Batch 150 Loss 1.4055 Accuracy 0.4124\n",
            "Epoch 7 Batch 200 Loss 1.3959 Accuracy 0.4142\n",
            "Epoch 7 Batch 250 Loss 1.3922 Accuracy 0.4158\n",
            "Epoch 7 Batch 300 Loss 1.3877 Accuracy 0.4163\n",
            "Epoch 7 Batch 350 Loss 1.3862 Accuracy 0.4170\n",
            "Epoch 7 Batch 400 Loss 1.3794 Accuracy 0.4171\n",
            "Epoch 7 Batch 450 Loss 1.3756 Accuracy 0.4179\n",
            "Epoch 7 Batch 500 Loss 1.3703 Accuracy 0.4186\n",
            "Epoch 7 Batch 550 Loss 1.3717 Accuracy 0.4186\n",
            "Epoch 7 Batch 600 Loss 1.3712 Accuracy 0.4183\n",
            "Epoch 7 Batch 650 Loss 1.3719 Accuracy 0.4185\n",
            "Epoch 7 Batch 700 Loss 1.3703 Accuracy 0.4186\n",
            "Epoch 7 Batch 750 Loss 1.3705 Accuracy 0.4185\n",
            "Epoch 7 Batch 800 Loss 1.3690 Accuracy 0.4188\n",
            "Epoch 7 Batch 850 Loss 1.3694 Accuracy 0.4190\n",
            "Epoch 7 Batch 900 Loss 1.3689 Accuracy 0.4189\n",
            "Epoch 7 Batch 950 Loss 1.3681 Accuracy 0.4186\n",
            "Epoch 7 Batch 1000 Loss 1.3674 Accuracy 0.4183\n",
            "Epoch 7 Batch 1050 Loss 1.3690 Accuracy 0.4182\n",
            "Epoch 7 Batch 1100 Loss 1.3682 Accuracy 0.4181\n",
            "Epoch 7 Batch 1150 Loss 1.3668 Accuracy 0.4180\n",
            "Epoch 7 Batch 1200 Loss 1.3670 Accuracy 0.4179\n",
            "Epoch 7 Batch 1250 Loss 1.3665 Accuracy 0.4178\n",
            "Epoch 7 Batch 1300 Loss 1.3660 Accuracy 0.4176\n",
            "Epoch 7 Batch 1350 Loss 1.3661 Accuracy 0.4173\n",
            "Epoch 7 Batch 1400 Loss 1.3669 Accuracy 0.4170\n",
            "Epoch 7 Batch 1450 Loss 1.3676 Accuracy 0.4166\n",
            "Epoch 7 Batch 1500 Loss 1.3672 Accuracy 0.4164\n",
            "Epoch 7 Batch 1550 Loss 1.3664 Accuracy 0.4161\n",
            "Epoch 7 Batch 1600 Loss 1.3654 Accuracy 0.4161\n",
            "Epoch 7 Batch 1650 Loss 1.3644 Accuracy 0.4161\n",
            "Epoch 7 Batch 1700 Loss 1.3631 Accuracy 0.4163\n",
            "Epoch 7 Batch 1750 Loss 1.3616 Accuracy 0.4166\n",
            "Epoch 7 Batch 1800 Loss 1.3603 Accuracy 0.4170\n",
            "Epoch 7 Batch 1850 Loss 1.3590 Accuracy 0.4172\n",
            "Epoch 7 Batch 1900 Loss 1.3583 Accuracy 0.4175\n",
            "Epoch 7 Batch 1950 Loss 1.3578 Accuracy 0.4177\n",
            "Epoch 7 Batch 2000 Loss 1.3567 Accuracy 0.4180\n",
            "Epoch 7 Batch 2050 Loss 1.3550 Accuracy 0.4181\n",
            "Epoch 7 Batch 2100 Loss 1.3540 Accuracy 0.4182\n",
            "Epoch 7 Batch 2150 Loss 1.3525 Accuracy 0.4183\n",
            "Epoch 7 Batch 2200 Loss 1.3508 Accuracy 0.4185\n",
            "Epoch 7 Batch 2250 Loss 1.3491 Accuracy 0.4187\n",
            "Epoch 7 Batch 2300 Loss 1.3470 Accuracy 0.4189\n",
            "Epoch 7 Batch 2350 Loss 1.3445 Accuracy 0.4192\n",
            "Epoch 7 Batch 2400 Loss 1.3423 Accuracy 0.4194\n",
            "Epoch 7 Batch 2450 Loss 1.3391 Accuracy 0.4197\n",
            "Epoch 7 Batch 2500 Loss 1.3359 Accuracy 0.4200\n",
            "Epoch 7 Batch 2550 Loss 1.3325 Accuracy 0.4202\n",
            "Epoch 7 Batch 2600 Loss 1.3292 Accuracy 0.4204\n",
            "Epoch 7 Batch 2650 Loss 1.3260 Accuracy 0.4208\n",
            "Epoch 7 Batch 2700 Loss 1.3223 Accuracy 0.4211\n",
            "Epoch 7 Batch 2750 Loss 1.3188 Accuracy 0.4214\n",
            "Epoch 7 Batch 2800 Loss 1.3155 Accuracy 0.4218\n",
            "Epoch 7 Batch 2850 Loss 1.3125 Accuracy 0.4222\n",
            "Epoch 7 Batch 2900 Loss 1.3092 Accuracy 0.4227\n",
            "Epoch 7 Batch 2950 Loss 1.3060 Accuracy 0.4232\n",
            "Epoch 7 Batch 3000 Loss 1.3023 Accuracy 0.4237\n",
            "Epoch 7 Batch 3050 Loss 1.2985 Accuracy 0.4242\n",
            "Epoch 7 Batch 3100 Loss 1.2946 Accuracy 0.4247\n",
            "Epoch 7 Batch 3150 Loss 1.2913 Accuracy 0.4253\n",
            "Epoch 7 Batch 3200 Loss 1.2880 Accuracy 0.4258\n",
            "Epoch 7 Batch 3250 Loss 1.2847 Accuracy 0.4263\n",
            "Epoch 7 Batch 3300 Loss 1.2818 Accuracy 0.4268\n",
            "Epoch 7 Batch 3350 Loss 1.2788 Accuracy 0.4272\n",
            "Epoch 7 Batch 3400 Loss 1.2759 Accuracy 0.4275\n",
            "Epoch 7 Batch 3450 Loss 1.2735 Accuracy 0.4279\n",
            "Epoch 7 Batch 3500 Loss 1.2711 Accuracy 0.4283\n",
            "Epoch 7 Batch 3550 Loss 1.2687 Accuracy 0.4287\n",
            "Epoch 7 Batch 3600 Loss 1.2662 Accuracy 0.4290\n",
            "Epoch 7 Batch 3650 Loss 1.2641 Accuracy 0.4292\n",
            "Epoch 7 Batch 3700 Loss 1.2619 Accuracy 0.4295\n",
            "Epoch 7 Batch 3750 Loss 1.2596 Accuracy 0.4298\n",
            "Epoch 7 Batch 3800 Loss 1.2574 Accuracy 0.4302\n",
            "Epoch 7 Batch 3850 Loss 1.2554 Accuracy 0.4305\n",
            "Epoch 7 Batch 3900 Loss 1.2532 Accuracy 0.4309\n",
            "Epoch 7 Batch 3950 Loss 1.2512 Accuracy 0.4312\n",
            "Epoch 7 Batch 4000 Loss 1.2493 Accuracy 0.4316\n",
            "Epoch 7 Batch 4050 Loss 1.2471 Accuracy 0.4319\n",
            "Epoch 7 Batch 4100 Loss 1.2449 Accuracy 0.4321\n",
            "Epoch 7 Batch 4150 Loss 1.2430 Accuracy 0.4324\n",
            "Epoch 7 Batch 4200 Loss 1.2413 Accuracy 0.4327\n",
            "Epoch 7 Batch 4250 Loss 1.2395 Accuracy 0.4330\n",
            "Epoch 7 Batch 4300 Loss 1.2381 Accuracy 0.4333\n",
            "Epoch 7 Batch 4350 Loss 1.2364 Accuracy 0.4336\n",
            "Epoch 7 Batch 4400 Loss 1.2349 Accuracy 0.4339\n",
            "Epoch 7 Batch 4450 Loss 1.2333 Accuracy 0.4342\n",
            "Epoch 7 Batch 4500 Loss 1.2316 Accuracy 0.4344\n",
            "Epoch 7 Batch 4550 Loss 1.2302 Accuracy 0.4347\n",
            "Epoch 7 Batch 4600 Loss 1.2288 Accuracy 0.4350\n",
            "Epoch 7 Batch 4650 Loss 1.2273 Accuracy 0.4353\n",
            "Epoch 7 Batch 4700 Loss 1.2261 Accuracy 0.4355\n",
            "Epoch 7 Batch 4750 Loss 1.2255 Accuracy 0.4356\n",
            "Epoch 7 Batch 4800 Loss 1.2250 Accuracy 0.4357\n",
            "Epoch 7 Batch 4850 Loss 1.2249 Accuracy 0.4357\n",
            "Epoch 7 Batch 4900 Loss 1.2250 Accuracy 0.4357\n",
            "Epoch 7 Batch 4950 Loss 1.2251 Accuracy 0.4357\n",
            "Epoch 7 Batch 5000 Loss 1.2256 Accuracy 0.4356\n",
            "Epoch 7 Batch 5050 Loss 1.2260 Accuracy 0.4355\n",
            "Epoch 7 Batch 5100 Loss 1.2263 Accuracy 0.4355\n",
            "Epoch 7 Batch 5150 Loss 1.2269 Accuracy 0.4354\n",
            "Epoch 7 Batch 5200 Loss 1.2271 Accuracy 0.4354\n",
            "Epoch 7 Batch 5250 Loss 1.2275 Accuracy 0.4353\n",
            "Epoch 7 Batch 5300 Loss 1.2282 Accuracy 0.4352\n",
            "Epoch 7 Batch 5350 Loss 1.2288 Accuracy 0.4352\n",
            "Epoch 7 Batch 5400 Loss 1.2292 Accuracy 0.4351\n",
            "Epoch 7 Batch 5450 Loss 1.2299 Accuracy 0.4350\n",
            "Epoch 7 Batch 5500 Loss 1.2308 Accuracy 0.4350\n",
            "Epoch 7 Batch 5550 Loss 1.2315 Accuracy 0.4349\n",
            "Epoch 7 Batch 5600 Loss 1.2320 Accuracy 0.4348\n",
            "Epoch 7 Batch 5650 Loss 1.2325 Accuracy 0.4347\n",
            "Epoch 7 Batch 5700 Loss 1.2332 Accuracy 0.4347\n",
            "Epoch 7 Batch 5750 Loss 1.2338 Accuracy 0.4346\n",
            "Epoch 7 Batch 5800 Loss 1.2346 Accuracy 0.4346\n",
            "Epoch 7 Batch 5850 Loss 1.2354 Accuracy 0.4344\n",
            "Epoch 7 Batch 5900 Loss 1.2364 Accuracy 0.4343\n",
            "Epoch 7 Batch 5950 Loss 1.2374 Accuracy 0.4342\n",
            "Epoch 7 Batch 6000 Loss 1.2383 Accuracy 0.4341\n",
            "Epoch 7 Batch 6050 Loss 1.2391 Accuracy 0.4340\n",
            "Epoch 7 Batch 6100 Loss 1.2400 Accuracy 0.4338\n",
            "Epoch 7 Batch 6150 Loss 1.2411 Accuracy 0.4336\n",
            "Epoch 7 Batch 6200 Loss 1.2419 Accuracy 0.4334\n",
            "Epoch 7 Batch 6250 Loss 1.2427 Accuracy 0.4332\n",
            "Epoch 7 Batch 6300 Loss 1.2436 Accuracy 0.4331\n",
            "Epoch 7 Batch 6350 Loss 1.2446 Accuracy 0.4329\n",
            "Epoch 7 Batch 6400 Loss 1.2452 Accuracy 0.4328\n",
            "Epoch 7 Batch 6450 Loss 1.2460 Accuracy 0.4326\n",
            "Epoch 7 Batch 6500 Loss 1.2465 Accuracy 0.4325\n",
            "Epoch 7 Batch 6550 Loss 1.2471 Accuracy 0.4324\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 2052.779512166977 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.2978 Accuracy 0.3988\n",
            "Epoch 8 Batch 50 Loss 1.3733 Accuracy 0.4166\n",
            "Epoch 8 Batch 100 Loss 1.3809 Accuracy 0.4140\n",
            "Epoch 8 Batch 150 Loss 1.3729 Accuracy 0.4186\n",
            "Epoch 8 Batch 200 Loss 1.3716 Accuracy 0.4187\n",
            "Epoch 8 Batch 250 Loss 1.3664 Accuracy 0.4197\n",
            "Epoch 8 Batch 300 Loss 1.3624 Accuracy 0.4202\n",
            "Epoch 8 Batch 350 Loss 1.3574 Accuracy 0.4205\n",
            "Epoch 8 Batch 400 Loss 1.3552 Accuracy 0.4213\n",
            "Epoch 8 Batch 450 Loss 1.3502 Accuracy 0.4218\n",
            "Epoch 8 Batch 500 Loss 1.3466 Accuracy 0.4217\n",
            "Epoch 8 Batch 550 Loss 1.3438 Accuracy 0.4217\n",
            "Epoch 8 Batch 600 Loss 1.3441 Accuracy 0.4214\n",
            "Epoch 8 Batch 650 Loss 1.3449 Accuracy 0.4214\n",
            "Epoch 8 Batch 700 Loss 1.3449 Accuracy 0.4214\n",
            "Epoch 8 Batch 750 Loss 1.3466 Accuracy 0.4218\n",
            "Epoch 8 Batch 800 Loss 1.3453 Accuracy 0.4219\n",
            "Epoch 8 Batch 850 Loss 1.3455 Accuracy 0.4221\n",
            "Epoch 8 Batch 900 Loss 1.3444 Accuracy 0.4222\n",
            "Epoch 8 Batch 950 Loss 1.3430 Accuracy 0.4223\n",
            "Epoch 8 Batch 1000 Loss 1.3418 Accuracy 0.4223\n",
            "Epoch 8 Batch 1050 Loss 1.3426 Accuracy 0.4220\n",
            "Epoch 8 Batch 1100 Loss 1.3429 Accuracy 0.4218\n",
            "Epoch 8 Batch 1150 Loss 1.3429 Accuracy 0.4217\n",
            "Epoch 8 Batch 1200 Loss 1.3417 Accuracy 0.4215\n",
            "Epoch 8 Batch 1250 Loss 1.3414 Accuracy 0.4213\n",
            "Epoch 8 Batch 1300 Loss 1.3423 Accuracy 0.4211\n",
            "Epoch 8 Batch 1350 Loss 1.3430 Accuracy 0.4209\n",
            "Epoch 8 Batch 1400 Loss 1.3430 Accuracy 0.4206\n",
            "Epoch 8 Batch 1450 Loss 1.3427 Accuracy 0.4204\n",
            "Epoch 8 Batch 1500 Loss 1.3431 Accuracy 0.4203\n",
            "Epoch 8 Batch 1550 Loss 1.3430 Accuracy 0.4200\n",
            "Epoch 8 Batch 1600 Loss 1.3422 Accuracy 0.4200\n",
            "Epoch 8 Batch 1650 Loss 1.3411 Accuracy 0.4201\n",
            "Epoch 8 Batch 1700 Loss 1.3400 Accuracy 0.4202\n",
            "Epoch 8 Batch 1750 Loss 1.3381 Accuracy 0.4204\n",
            "Epoch 8 Batch 1800 Loss 1.3374 Accuracy 0.4205\n",
            "Epoch 8 Batch 1850 Loss 1.3361 Accuracy 0.4206\n",
            "Epoch 8 Batch 1900 Loss 1.3338 Accuracy 0.4209\n",
            "Epoch 8 Batch 1950 Loss 1.3326 Accuracy 0.4212\n",
            "Epoch 8 Batch 2000 Loss 1.3317 Accuracy 0.4214\n",
            "Epoch 8 Batch 2050 Loss 1.3303 Accuracy 0.4215\n",
            "Epoch 8 Batch 2100 Loss 1.3292 Accuracy 0.4216\n",
            "Epoch 8 Batch 2150 Loss 1.3269 Accuracy 0.4219\n",
            "Epoch 8 Batch 2200 Loss 1.3249 Accuracy 0.4221\n",
            "Epoch 8 Batch 2250 Loss 1.3234 Accuracy 0.4223\n",
            "Epoch 8 Batch 2300 Loss 1.3212 Accuracy 0.4226\n",
            "Epoch 8 Batch 2350 Loss 1.3186 Accuracy 0.4228\n",
            "Epoch 8 Batch 2400 Loss 1.3165 Accuracy 0.4230\n",
            "Epoch 8 Batch 2450 Loss 1.3140 Accuracy 0.4234\n",
            "Epoch 8 Batch 2500 Loss 1.3115 Accuracy 0.4236\n",
            "Epoch 8 Batch 2550 Loss 1.3090 Accuracy 0.4238\n",
            "Epoch 8 Batch 2600 Loss 1.3056 Accuracy 0.4240\n",
            "Epoch 8 Batch 2650 Loss 1.3024 Accuracy 0.4243\n",
            "Epoch 8 Batch 2700 Loss 1.2991 Accuracy 0.4247\n",
            "Epoch 8 Batch 2750 Loss 1.2957 Accuracy 0.4251\n",
            "Epoch 8 Batch 2800 Loss 1.2923 Accuracy 0.4255\n",
            "Epoch 8 Batch 2850 Loss 1.2892 Accuracy 0.4259\n",
            "Epoch 8 Batch 2900 Loss 1.2861 Accuracy 0.4264\n",
            "Epoch 8 Batch 2950 Loss 1.2826 Accuracy 0.4269\n",
            "Epoch 8 Batch 3000 Loss 1.2794 Accuracy 0.4274\n",
            "Epoch 8 Batch 3050 Loss 1.2756 Accuracy 0.4280\n",
            "Epoch 8 Batch 3100 Loss 1.2716 Accuracy 0.4285\n",
            "Epoch 8 Batch 3150 Loss 1.2673 Accuracy 0.4290\n",
            "Epoch 8 Batch 3200 Loss 1.2638 Accuracy 0.4295\n",
            "Epoch 8 Batch 3250 Loss 1.2606 Accuracy 0.4300\n",
            "Epoch 8 Batch 3300 Loss 1.2578 Accuracy 0.4305\n",
            "Epoch 8 Batch 3350 Loss 1.2550 Accuracy 0.4309\n",
            "Epoch 8 Batch 3400 Loss 1.2522 Accuracy 0.4313\n",
            "Epoch 8 Batch 3450 Loss 1.2500 Accuracy 0.4316\n",
            "Epoch 8 Batch 3500 Loss 1.2477 Accuracy 0.4320\n",
            "Epoch 8 Batch 3550 Loss 1.2453 Accuracy 0.4324\n",
            "Epoch 8 Batch 3600 Loss 1.2430 Accuracy 0.4327\n",
            "Epoch 8 Batch 3650 Loss 1.2408 Accuracy 0.4329\n",
            "Epoch 8 Batch 3700 Loss 1.2384 Accuracy 0.4332\n",
            "Epoch 8 Batch 3750 Loss 1.2360 Accuracy 0.4334\n",
            "Epoch 8 Batch 3800 Loss 1.2339 Accuracy 0.4337\n",
            "Epoch 8 Batch 3850 Loss 1.2318 Accuracy 0.4340\n",
            "Epoch 8 Batch 3900 Loss 1.2298 Accuracy 0.4343\n",
            "Epoch 8 Batch 3950 Loss 1.2278 Accuracy 0.4346\n",
            "Epoch 8 Batch 4000 Loss 1.2257 Accuracy 0.4349\n",
            "Epoch 8 Batch 4050 Loss 1.2238 Accuracy 0.4353\n",
            "Epoch 8 Batch 4100 Loss 1.2218 Accuracy 0.4356\n",
            "Epoch 8 Batch 4150 Loss 1.2200 Accuracy 0.4358\n",
            "Epoch 8 Batch 4200 Loss 1.2181 Accuracy 0.4362\n",
            "Epoch 8 Batch 4250 Loss 1.2162 Accuracy 0.4365\n",
            "Epoch 8 Batch 4300 Loss 1.2144 Accuracy 0.4368\n",
            "Epoch 8 Batch 4350 Loss 1.2128 Accuracy 0.4371\n",
            "Epoch 8 Batch 4400 Loss 1.2113 Accuracy 0.4375\n",
            "Epoch 8 Batch 4450 Loss 1.2098 Accuracy 0.4377\n",
            "Epoch 8 Batch 4500 Loss 1.2081 Accuracy 0.4380\n",
            "Epoch 8 Batch 4550 Loss 1.2068 Accuracy 0.4383\n",
            "Epoch 8 Batch 4600 Loss 1.2053 Accuracy 0.4385\n",
            "Epoch 8 Batch 4650 Loss 1.2040 Accuracy 0.4387\n",
            "Epoch 8 Batch 4700 Loss 1.2030 Accuracy 0.4389\n",
            "Epoch 8 Batch 4750 Loss 1.2026 Accuracy 0.4390\n",
            "Epoch 8 Batch 4800 Loss 1.2023 Accuracy 0.4391\n",
            "Epoch 8 Batch 4850 Loss 1.2020 Accuracy 0.4391\n",
            "Epoch 8 Batch 4900 Loss 1.2022 Accuracy 0.4391\n",
            "Epoch 8 Batch 4950 Loss 1.2025 Accuracy 0.4391\n",
            "Epoch 8 Batch 5000 Loss 1.2028 Accuracy 0.4390\n",
            "Epoch 8 Batch 5050 Loss 1.2033 Accuracy 0.4389\n",
            "Epoch 8 Batch 5100 Loss 1.2035 Accuracy 0.4389\n",
            "Epoch 8 Batch 5150 Loss 1.2037 Accuracy 0.4389\n",
            "Epoch 8 Batch 5200 Loss 1.2043 Accuracy 0.4388\n",
            "Epoch 8 Batch 5250 Loss 1.2049 Accuracy 0.4387\n",
            "Epoch 8 Batch 5300 Loss 1.2054 Accuracy 0.4386\n",
            "Epoch 8 Batch 5350 Loss 1.2059 Accuracy 0.4386\n",
            "Epoch 8 Batch 5400 Loss 1.2065 Accuracy 0.4385\n",
            "Epoch 8 Batch 5450 Loss 1.2071 Accuracy 0.4384\n",
            "Epoch 8 Batch 5500 Loss 1.2078 Accuracy 0.4384\n",
            "Epoch 8 Batch 5550 Loss 1.2085 Accuracy 0.4383\n",
            "Epoch 8 Batch 5600 Loss 1.2094 Accuracy 0.4383\n",
            "Epoch 8 Batch 5650 Loss 1.2099 Accuracy 0.4382\n",
            "Epoch 8 Batch 5700 Loss 1.2107 Accuracy 0.4381\n",
            "Epoch 8 Batch 5750 Loss 1.2115 Accuracy 0.4380\n",
            "Epoch 8 Batch 5800 Loss 1.2120 Accuracy 0.4379\n",
            "Epoch 8 Batch 5850 Loss 1.2130 Accuracy 0.4378\n",
            "Epoch 8 Batch 5900 Loss 1.2140 Accuracy 0.4377\n",
            "Epoch 8 Batch 5950 Loss 1.2148 Accuracy 0.4376\n",
            "Epoch 8 Batch 6000 Loss 1.2158 Accuracy 0.4374\n",
            "Epoch 8 Batch 6050 Loss 1.2168 Accuracy 0.4373\n",
            "Epoch 8 Batch 6100 Loss 1.2177 Accuracy 0.4371\n",
            "Epoch 8 Batch 6150 Loss 1.2185 Accuracy 0.4369\n",
            "Epoch 8 Batch 6200 Loss 1.2194 Accuracy 0.4368\n",
            "Epoch 8 Batch 6250 Loss 1.2201 Accuracy 0.4367\n",
            "Epoch 8 Batch 6300 Loss 1.2211 Accuracy 0.4365\n",
            "Epoch 8 Batch 6350 Loss 1.2221 Accuracy 0.4363\n",
            "Epoch 8 Batch 6400 Loss 1.2228 Accuracy 0.4362\n",
            "Epoch 8 Batch 6450 Loss 1.2233 Accuracy 0.4360\n",
            "Epoch 8 Batch 6500 Loss 1.2240 Accuracy 0.4359\n",
            "Epoch 8 Batch 6550 Loss 1.2248 Accuracy 0.4358\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 2132.1861701011658 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.4663 Accuracy 0.4424\n",
            "Epoch 9 Batch 50 Loss 1.3259 Accuracy 0.4215\n",
            "Epoch 9 Batch 100 Loss 1.3339 Accuracy 0.4236\n",
            "Epoch 9 Batch 150 Loss 1.3386 Accuracy 0.4243\n",
            "Epoch 9 Batch 200 Loss 1.3432 Accuracy 0.4241\n",
            "Epoch 9 Batch 250 Loss 1.3437 Accuracy 0.4242\n",
            "Epoch 9 Batch 300 Loss 1.3414 Accuracy 0.4244\n",
            "Epoch 9 Batch 350 Loss 1.3355 Accuracy 0.4247\n",
            "Epoch 9 Batch 400 Loss 1.3342 Accuracy 0.4253\n",
            "Epoch 9 Batch 450 Loss 1.3328 Accuracy 0.4255\n",
            "Epoch 9 Batch 500 Loss 1.3308 Accuracy 0.4261\n",
            "Epoch 9 Batch 550 Loss 1.3268 Accuracy 0.4262\n",
            "Epoch 9 Batch 600 Loss 1.3230 Accuracy 0.4262\n",
            "Epoch 9 Batch 650 Loss 1.3254 Accuracy 0.4259\n",
            "Epoch 9 Batch 700 Loss 1.3229 Accuracy 0.4255\n",
            "Epoch 9 Batch 750 Loss 1.3230 Accuracy 0.4259\n",
            "Epoch 9 Batch 800 Loss 1.3227 Accuracy 0.4259\n",
            "Epoch 9 Batch 850 Loss 1.3246 Accuracy 0.4259\n",
            "Epoch 9 Batch 900 Loss 1.3246 Accuracy 0.4259\n",
            "Epoch 9 Batch 950 Loss 1.3252 Accuracy 0.4261\n",
            "Epoch 9 Batch 1000 Loss 1.3254 Accuracy 0.4259\n",
            "Epoch 9 Batch 1050 Loss 1.3251 Accuracy 0.4257\n",
            "Epoch 9 Batch 1100 Loss 1.3236 Accuracy 0.4254\n",
            "Epoch 9 Batch 1150 Loss 1.3239 Accuracy 0.4251\n",
            "Epoch 9 Batch 1200 Loss 1.3237 Accuracy 0.4248\n",
            "Epoch 9 Batch 1250 Loss 1.3237 Accuracy 0.4246\n",
            "Epoch 9 Batch 1300 Loss 1.3231 Accuracy 0.4244\n",
            "Epoch 9 Batch 1350 Loss 1.3232 Accuracy 0.4241\n",
            "Epoch 9 Batch 1400 Loss 1.3245 Accuracy 0.4238\n",
            "Epoch 9 Batch 1450 Loss 1.3238 Accuracy 0.4236\n",
            "Epoch 9 Batch 1500 Loss 1.3230 Accuracy 0.4235\n",
            "Epoch 9 Batch 1550 Loss 1.3229 Accuracy 0.4234\n",
            "Epoch 9 Batch 1600 Loss 1.3229 Accuracy 0.4233\n",
            "Epoch 9 Batch 1650 Loss 1.3208 Accuracy 0.4235\n",
            "Epoch 9 Batch 1700 Loss 1.3193 Accuracy 0.4235\n",
            "Epoch 9 Batch 1750 Loss 1.3181 Accuracy 0.4236\n",
            "Epoch 9 Batch 1800 Loss 1.3174 Accuracy 0.4237\n",
            "Epoch 9 Batch 1850 Loss 1.3162 Accuracy 0.4238\n",
            "Epoch 9 Batch 1900 Loss 1.3146 Accuracy 0.4240\n",
            "Epoch 9 Batch 1950 Loss 1.3138 Accuracy 0.4242\n",
            "Epoch 9 Batch 2000 Loss 1.3122 Accuracy 0.4244\n",
            "Epoch 9 Batch 2050 Loss 1.3112 Accuracy 0.4245\n",
            "Epoch 9 Batch 2100 Loss 1.3096 Accuracy 0.4247\n",
            "Epoch 9 Batch 2150 Loss 1.3078 Accuracy 0.4248\n",
            "Epoch 9 Batch 2200 Loss 1.3065 Accuracy 0.4251\n",
            "Epoch 9 Batch 2250 Loss 1.3050 Accuracy 0.4253\n",
            "Epoch 9 Batch 2300 Loss 1.3033 Accuracy 0.4256\n",
            "Epoch 9 Batch 2350 Loss 1.3006 Accuracy 0.4258\n",
            "Epoch 9 Batch 2400 Loss 1.2986 Accuracy 0.4261\n",
            "Epoch 9 Batch 2450 Loss 1.2955 Accuracy 0.4262\n",
            "Epoch 9 Batch 2500 Loss 1.2925 Accuracy 0.4264\n",
            "Epoch 9 Batch 2550 Loss 1.2901 Accuracy 0.4266\n",
            "Epoch 9 Batch 2600 Loss 1.2867 Accuracy 0.4269\n",
            "Epoch 9 Batch 2650 Loss 1.2838 Accuracy 0.4272\n",
            "Epoch 9 Batch 2700 Loss 1.2803 Accuracy 0.4275\n",
            "Epoch 9 Batch 2750 Loss 1.2769 Accuracy 0.4279\n",
            "Epoch 9 Batch 2800 Loss 1.2736 Accuracy 0.4283\n",
            "Epoch 9 Batch 2850 Loss 1.2704 Accuracy 0.4287\n",
            "Epoch 9 Batch 2900 Loss 1.2671 Accuracy 0.4292\n",
            "Epoch 9 Batch 2950 Loss 1.2636 Accuracy 0.4297\n",
            "Epoch 9 Batch 3000 Loss 1.2594 Accuracy 0.4302\n",
            "Epoch 9 Batch 3050 Loss 1.2559 Accuracy 0.4307\n",
            "Epoch 9 Batch 3100 Loss 1.2522 Accuracy 0.4313\n",
            "Epoch 9 Batch 3150 Loss 1.2484 Accuracy 0.4319\n",
            "Epoch 9 Batch 3200 Loss 1.2448 Accuracy 0.4324\n",
            "Epoch 9 Batch 3250 Loss 1.2416 Accuracy 0.4328\n",
            "Epoch 9 Batch 3300 Loss 1.2389 Accuracy 0.4333\n",
            "Epoch 9 Batch 3350 Loss 1.2365 Accuracy 0.4338\n",
            "Epoch 9 Batch 3400 Loss 1.2338 Accuracy 0.4341\n",
            "Epoch 9 Batch 3450 Loss 1.2313 Accuracy 0.4344\n",
            "Epoch 9 Batch 3500 Loss 1.2289 Accuracy 0.4347\n",
            "Epoch 9 Batch 3550 Loss 1.2266 Accuracy 0.4351\n",
            "Epoch 9 Batch 3600 Loss 1.2247 Accuracy 0.4354\n",
            "Epoch 9 Batch 3650 Loss 1.2226 Accuracy 0.4357\n",
            "Epoch 9 Batch 3700 Loss 1.2205 Accuracy 0.4359\n",
            "Epoch 9 Batch 3750 Loss 1.2184 Accuracy 0.4362\n",
            "Epoch 9 Batch 3800 Loss 1.2163 Accuracy 0.4365\n",
            "Epoch 9 Batch 3850 Loss 1.2141 Accuracy 0.4369\n",
            "Epoch 9 Batch 3900 Loss 1.2119 Accuracy 0.4372\n",
            "Epoch 9 Batch 3950 Loss 1.2097 Accuracy 0.4375\n",
            "Epoch 9 Batch 4000 Loss 1.2078 Accuracy 0.4377\n",
            "Epoch 9 Batch 4050 Loss 1.2058 Accuracy 0.4380\n",
            "Epoch 9 Batch 4100 Loss 1.2037 Accuracy 0.4383\n",
            "Epoch 9 Batch 4150 Loss 1.2017 Accuracy 0.4387\n",
            "Epoch 9 Batch 4200 Loss 1.2000 Accuracy 0.4390\n",
            "Epoch 9 Batch 4250 Loss 1.1985 Accuracy 0.4393\n",
            "Epoch 9 Batch 4300 Loss 1.1967 Accuracy 0.4395\n",
            "Epoch 9 Batch 4350 Loss 1.1952 Accuracy 0.4397\n",
            "Epoch 9 Batch 4400 Loss 1.1940 Accuracy 0.4401\n",
            "Epoch 9 Batch 4450 Loss 1.1925 Accuracy 0.4404\n",
            "Epoch 9 Batch 4500 Loss 1.1910 Accuracy 0.4406\n",
            "Epoch 9 Batch 4550 Loss 1.1895 Accuracy 0.4409\n",
            "Epoch 9 Batch 4600 Loss 1.1879 Accuracy 0.4412\n",
            "Epoch 9 Batch 4650 Loss 1.1864 Accuracy 0.4415\n",
            "Epoch 9 Batch 4700 Loss 1.1853 Accuracy 0.4416\n",
            "Epoch 9 Batch 4750 Loss 1.1846 Accuracy 0.4417\n",
            "Epoch 9 Batch 4800 Loss 1.1843 Accuracy 0.4418\n",
            "Epoch 9 Batch 4850 Loss 1.1843 Accuracy 0.4418\n",
            "Epoch 9 Batch 4900 Loss 1.1843 Accuracy 0.4419\n",
            "Epoch 9 Batch 4950 Loss 1.1844 Accuracy 0.4418\n",
            "Epoch 9 Batch 5000 Loss 1.1848 Accuracy 0.4418\n",
            "Epoch 9 Batch 5050 Loss 1.1851 Accuracy 0.4417\n",
            "Epoch 9 Batch 5100 Loss 1.1858 Accuracy 0.4417\n",
            "Epoch 9 Batch 5150 Loss 1.1862 Accuracy 0.4416\n",
            "Epoch 9 Batch 5200 Loss 1.1868 Accuracy 0.4415\n",
            "Epoch 9 Batch 5250 Loss 1.1874 Accuracy 0.4414\n",
            "Epoch 9 Batch 5300 Loss 1.1879 Accuracy 0.4413\n",
            "Epoch 9 Batch 5350 Loss 1.1884 Accuracy 0.4412\n",
            "Epoch 9 Batch 5400 Loss 1.1889 Accuracy 0.4411\n",
            "Epoch 9 Batch 5450 Loss 1.1895 Accuracy 0.4410\n",
            "Epoch 9 Batch 5500 Loss 1.1901 Accuracy 0.4410\n",
            "Epoch 9 Batch 5550 Loss 1.1907 Accuracy 0.4409\n",
            "Epoch 9 Batch 5600 Loss 1.1915 Accuracy 0.4408\n",
            "Epoch 9 Batch 5650 Loss 1.1923 Accuracy 0.4407\n",
            "Epoch 9 Batch 5700 Loss 1.1930 Accuracy 0.4406\n",
            "Epoch 9 Batch 5750 Loss 1.1939 Accuracy 0.4406\n",
            "Epoch 9 Batch 5800 Loss 1.1948 Accuracy 0.4404\n",
            "Epoch 9 Batch 5850 Loss 1.1954 Accuracy 0.4403\n",
            "Epoch 9 Batch 5900 Loss 1.1964 Accuracy 0.4402\n",
            "Epoch 9 Batch 5950 Loss 1.1973 Accuracy 0.4401\n",
            "Epoch 9 Batch 6000 Loss 1.1983 Accuracy 0.4399\n",
            "Epoch 9 Batch 6050 Loss 1.1992 Accuracy 0.4398\n",
            "Epoch 9 Batch 6100 Loss 1.2001 Accuracy 0.4397\n",
            "Epoch 9 Batch 6150 Loss 1.2011 Accuracy 0.4395\n",
            "Epoch 9 Batch 6200 Loss 1.2019 Accuracy 0.4393\n",
            "Epoch 9 Batch 6250 Loss 1.2027 Accuracy 0.4392\n",
            "Epoch 9 Batch 6300 Loss 1.2037 Accuracy 0.4390\n",
            "Epoch 9 Batch 6350 Loss 1.2046 Accuracy 0.4388\n",
            "Epoch 9 Batch 6400 Loss 1.2054 Accuracy 0.4387\n",
            "Epoch 9 Batch 6450 Loss 1.2061 Accuracy 0.4385\n",
            "Epoch 9 Batch 6500 Loss 1.2068 Accuracy 0.4384\n",
            "Epoch 9 Batch 6550 Loss 1.2073 Accuracy 0.4383\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 2136.083458185196 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.2838 Accuracy 0.4490\n",
            "Epoch 10 Batch 50 Loss 1.3258 Accuracy 0.4218\n",
            "Epoch 10 Batch 100 Loss 1.3453 Accuracy 0.4197\n",
            "Epoch 10 Batch 150 Loss 1.3493 Accuracy 0.4224\n",
            "Epoch 10 Batch 200 Loss 1.3426 Accuracy 0.4222\n",
            "Epoch 10 Batch 250 Loss 1.3408 Accuracy 0.4237\n",
            "Epoch 10 Batch 300 Loss 1.3365 Accuracy 0.4247\n",
            "Epoch 10 Batch 350 Loss 1.3320 Accuracy 0.4258\n",
            "Epoch 10 Batch 400 Loss 1.3282 Accuracy 0.4264\n",
            "Epoch 10 Batch 450 Loss 1.3207 Accuracy 0.4274\n",
            "Epoch 10 Batch 500 Loss 1.3181 Accuracy 0.4276\n",
            "Epoch 10 Batch 550 Loss 1.3163 Accuracy 0.4275\n",
            "Epoch 10 Batch 600 Loss 1.3145 Accuracy 0.4276\n",
            "Epoch 10 Batch 650 Loss 1.3113 Accuracy 0.4279\n",
            "Epoch 10 Batch 700 Loss 1.3105 Accuracy 0.4280\n",
            "Epoch 10 Batch 750 Loss 1.3094 Accuracy 0.4279\n",
            "Epoch 10 Batch 800 Loss 1.3084 Accuracy 0.4280\n",
            "Epoch 10 Batch 850 Loss 1.3069 Accuracy 0.4283\n",
            "Epoch 10 Batch 900 Loss 1.3063 Accuracy 0.4282\n",
            "Epoch 10 Batch 950 Loss 1.3049 Accuracy 0.4280\n",
            "Epoch 10 Batch 1000 Loss 1.3048 Accuracy 0.4279\n",
            "Epoch 10 Batch 1050 Loss 1.3061 Accuracy 0.4277\n",
            "Epoch 10 Batch 1100 Loss 1.3059 Accuracy 0.4275\n",
            "Epoch 10 Batch 1150 Loss 1.3057 Accuracy 0.4273\n",
            "Epoch 10 Batch 1200 Loss 1.3056 Accuracy 0.4269\n",
            "Epoch 10 Batch 1250 Loss 1.3066 Accuracy 0.4269\n",
            "Epoch 10 Batch 1300 Loss 1.3068 Accuracy 0.4267\n",
            "Epoch 10 Batch 1350 Loss 1.3065 Accuracy 0.4263\n",
            "Epoch 10 Batch 1400 Loss 1.3068 Accuracy 0.4260\n",
            "Epoch 10 Batch 1450 Loss 1.3063 Accuracy 0.4256\n",
            "Epoch 10 Batch 1500 Loss 1.3064 Accuracy 0.4253\n",
            "Epoch 10 Batch 1550 Loss 1.3062 Accuracy 0.4252\n",
            "Epoch 10 Batch 1600 Loss 1.3057 Accuracy 0.4252\n",
            "Epoch 10 Batch 1650 Loss 1.3057 Accuracy 0.4252\n",
            "Epoch 10 Batch 1700 Loss 1.3055 Accuracy 0.4254\n",
            "Epoch 10 Batch 1750 Loss 1.3045 Accuracy 0.4256\n",
            "Epoch 10 Batch 1800 Loss 1.3027 Accuracy 0.4259\n",
            "Epoch 10 Batch 1850 Loss 1.3011 Accuracy 0.4260\n",
            "Epoch 10 Batch 1900 Loss 1.2995 Accuracy 0.4265\n",
            "Epoch 10 Batch 1950 Loss 1.2983 Accuracy 0.4267\n",
            "Epoch 10 Batch 2000 Loss 1.2968 Accuracy 0.4268\n",
            "Epoch 10 Batch 2050 Loss 1.2951 Accuracy 0.4271\n",
            "Epoch 10 Batch 2100 Loss 1.2929 Accuracy 0.4272\n",
            "Epoch 10 Batch 2150 Loss 1.2919 Accuracy 0.4274\n",
            "Epoch 10 Batch 2200 Loss 1.2900 Accuracy 0.4275\n",
            "Epoch 10 Batch 2250 Loss 1.2880 Accuracy 0.4277\n",
            "Epoch 10 Batch 2300 Loss 1.2860 Accuracy 0.4279\n",
            "Epoch 10 Batch 2350 Loss 1.2840 Accuracy 0.4282\n",
            "Epoch 10 Batch 2400 Loss 1.2816 Accuracy 0.4283\n",
            "Epoch 10 Batch 2450 Loss 1.2785 Accuracy 0.4286\n",
            "Epoch 10 Batch 2500 Loss 1.2758 Accuracy 0.4288\n",
            "Epoch 10 Batch 2550 Loss 1.2723 Accuracy 0.4291\n",
            "Epoch 10 Batch 2600 Loss 1.2694 Accuracy 0.4293\n",
            "Epoch 10 Batch 2650 Loss 1.2664 Accuracy 0.4295\n",
            "Epoch 10 Batch 2700 Loss 1.2633 Accuracy 0.4298\n",
            "Epoch 10 Batch 2750 Loss 1.2597 Accuracy 0.4302\n",
            "Epoch 10 Batch 2800 Loss 1.2564 Accuracy 0.4306\n",
            "Epoch 10 Batch 2850 Loss 1.2530 Accuracy 0.4310\n",
            "Epoch 10 Batch 2900 Loss 1.2501 Accuracy 0.4314\n",
            "Epoch 10 Batch 2950 Loss 1.2463 Accuracy 0.4319\n",
            "Epoch 10 Batch 3000 Loss 1.2430 Accuracy 0.4324\n",
            "Epoch 10 Batch 3050 Loss 1.2393 Accuracy 0.4330\n",
            "Epoch 10 Batch 3100 Loss 1.2356 Accuracy 0.4336\n",
            "Epoch 10 Batch 3150 Loss 1.2321 Accuracy 0.4342\n",
            "Epoch 10 Batch 3200 Loss 1.2288 Accuracy 0.4347\n",
            "Epoch 10 Batch 3250 Loss 1.2260 Accuracy 0.4352\n",
            "Epoch 10 Batch 3300 Loss 1.2233 Accuracy 0.4356\n",
            "Epoch 10 Batch 3350 Loss 1.2203 Accuracy 0.4360\n",
            "Epoch 10 Batch 3400 Loss 1.2174 Accuracy 0.4363\n",
            "Epoch 10 Batch 3450 Loss 1.2149 Accuracy 0.4367\n",
            "Epoch 10 Batch 3500 Loss 1.2125 Accuracy 0.4370\n",
            "Epoch 10 Batch 3550 Loss 1.2099 Accuracy 0.4373\n",
            "Epoch 10 Batch 3600 Loss 1.2079 Accuracy 0.4377\n",
            "Epoch 10 Batch 3650 Loss 1.2056 Accuracy 0.4380\n",
            "Epoch 10 Batch 3700 Loss 1.2033 Accuracy 0.4383\n",
            "Epoch 10 Batch 3750 Loss 1.2012 Accuracy 0.4385\n",
            "Epoch 10 Batch 3800 Loss 1.1993 Accuracy 0.4388\n",
            "Epoch 10 Batch 3850 Loss 1.1973 Accuracy 0.4392\n",
            "Epoch 10 Batch 3900 Loss 1.1951 Accuracy 0.4394\n",
            "Epoch 10 Batch 3950 Loss 1.1928 Accuracy 0.4397\n",
            "Epoch 10 Batch 4000 Loss 1.1912 Accuracy 0.4400\n",
            "Epoch 10 Batch 4050 Loss 1.1897 Accuracy 0.4402\n",
            "Epoch 10 Batch 4100 Loss 1.1876 Accuracy 0.4405\n",
            "Epoch 10 Batch 4150 Loss 1.1857 Accuracy 0.4409\n",
            "Epoch 10 Batch 4200 Loss 1.1837 Accuracy 0.4412\n",
            "Epoch 10 Batch 4250 Loss 1.1820 Accuracy 0.4415\n",
            "Epoch 10 Batch 4300 Loss 1.1802 Accuracy 0.4418\n",
            "Epoch 10 Batch 4350 Loss 1.1788 Accuracy 0.4421\n",
            "Epoch 10 Batch 4400 Loss 1.1774 Accuracy 0.4424\n",
            "Epoch 10 Batch 4450 Loss 1.1764 Accuracy 0.4426\n",
            "Epoch 10 Batch 4500 Loss 1.1749 Accuracy 0.4429\n",
            "Epoch 10 Batch 4550 Loss 1.1734 Accuracy 0.4432\n",
            "Epoch 10 Batch 4600 Loss 1.1720 Accuracy 0.4435\n",
            "Epoch 10 Batch 4650 Loss 1.1704 Accuracy 0.4438\n",
            "Epoch 10 Batch 4700 Loss 1.1693 Accuracy 0.4439\n",
            "Epoch 10 Batch 4750 Loss 1.1688 Accuracy 0.4441\n",
            "Epoch 10 Batch 4800 Loss 1.1685 Accuracy 0.4441\n",
            "Epoch 10 Batch 4850 Loss 1.1686 Accuracy 0.4441\n",
            "Epoch 10 Batch 4900 Loss 1.1685 Accuracy 0.4441\n",
            "Epoch 10 Batch 4950 Loss 1.1686 Accuracy 0.4441\n",
            "Epoch 10 Batch 5000 Loss 1.1691 Accuracy 0.4440\n",
            "Epoch 10 Batch 5050 Loss 1.1694 Accuracy 0.4440\n",
            "Epoch 10 Batch 5100 Loss 1.1698 Accuracy 0.4438\n",
            "Epoch 10 Batch 5150 Loss 1.1704 Accuracy 0.4438\n",
            "Epoch 10 Batch 5200 Loss 1.1710 Accuracy 0.4438\n",
            "Epoch 10 Batch 5250 Loss 1.1715 Accuracy 0.4437\n",
            "Epoch 10 Batch 5300 Loss 1.1719 Accuracy 0.4436\n",
            "Epoch 10 Batch 5350 Loss 1.1726 Accuracy 0.4435\n",
            "Epoch 10 Batch 5400 Loss 1.1732 Accuracy 0.4434\n",
            "Epoch 10 Batch 5450 Loss 1.1739 Accuracy 0.4434\n",
            "Epoch 10 Batch 5500 Loss 1.1745 Accuracy 0.4433\n",
            "Epoch 10 Batch 5550 Loss 1.1750 Accuracy 0.4433\n",
            "Epoch 10 Batch 5600 Loss 1.1753 Accuracy 0.4432\n",
            "Epoch 10 Batch 5650 Loss 1.1760 Accuracy 0.4432\n",
            "Epoch 10 Batch 5700 Loss 1.1766 Accuracy 0.4431\n",
            "Epoch 10 Batch 5750 Loss 1.1775 Accuracy 0.4430\n",
            "Epoch 10 Batch 5800 Loss 1.1783 Accuracy 0.4429\n",
            "Epoch 10 Batch 5850 Loss 1.1791 Accuracy 0.4428\n",
            "Epoch 10 Batch 5900 Loss 1.1801 Accuracy 0.4427\n",
            "Epoch 10 Batch 5950 Loss 1.1809 Accuracy 0.4425\n",
            "Epoch 10 Batch 6000 Loss 1.1821 Accuracy 0.4424\n",
            "Epoch 10 Batch 6050 Loss 1.1833 Accuracy 0.4423\n",
            "Epoch 10 Batch 6100 Loss 1.1841 Accuracy 0.4422\n",
            "Epoch 10 Batch 6150 Loss 1.1852 Accuracy 0.4419\n",
            "Epoch 10 Batch 6200 Loss 1.1861 Accuracy 0.4417\n",
            "Epoch 10 Batch 6250 Loss 1.1869 Accuracy 0.4415\n",
            "Epoch 10 Batch 6300 Loss 1.1877 Accuracy 0.4414\n",
            "Epoch 10 Batch 6350 Loss 1.1886 Accuracy 0.4412\n",
            "Epoch 10 Batch 6400 Loss 1.1895 Accuracy 0.4411\n",
            "Epoch 10 Batch 6450 Loss 1.1902 Accuracy 0.4409\n",
            "Epoch 10 Batch 6500 Loss 1.1908 Accuracy 0.4408\n",
            "Epoch 10 Batch 6550 Loss 1.1914 Accuracy 0.4406\n",
            "Saving checkpoint for epoch 10 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 2166.564466238022 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voLD4bGtpO93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNpyoQY3ANQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74xJF07q_fIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTKcbgGOpVBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_NL-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_NL-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1YXUC2LO6Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()\n",
        "\n",
        "  predicted_sentence = tokenizer_nl.decode(\n",
        "      [i for i in output if i < VOCAB_SIZE_NL-2]\n",
        "    )\n",
        "  \n",
        "  print(\"Input: {}\".format(sentence))\n",
        "  print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5xy1n6NpdbF",
        "colab_type": "code",
        "outputId": "ddace781-322f-43a2-eb4e-0f26195f5316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "translate(\"Today is a good day.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-64c07512360a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Today is a good day.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'translate' is not defined"
          ]
        }
      ]
    }
  ]
}
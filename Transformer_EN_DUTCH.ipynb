{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_EN_DUTCH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPp4XTh9pw2kCtCdjbWXFgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjevans25/English_to_Dutch_Transformer/blob/master/Transformer_EN_DUTCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lXTs0nDl3b4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ua1NAklnaLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEvwZ_bjnb6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading Files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryMCaiO6nfzE",
        "colab_type": "code",
        "outputId": "4cfaff1f-063b-48c8-c6c3-29ef8331070c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nyQktsZniFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/europarl-v7.nl-en.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/europarl-v7.nl-en.nl\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      europarl_nl = f.read()\n",
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/nonbreaking_prefix.en\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/NLP_Projects/Dutch_Transformer/nonbreaking_prefix2.nl\",\n",
        "          mode='r',\n",
        "          encoding=\"utf-8\") as f:\n",
        "      non_breaking_prefix_nl = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXZVS2Mankws",
        "colab_type": "code",
        "outputId": "b3d11892-78f4-41e4-a373-1cedd0ced765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "non_breaking_prefix_nl[:500]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\nk\\nl\\nm\\nn\\no\\np\\nq\\nr\\ns\\nt\\nu\\nv\\nw\\nx\\ny\\nz\\nbacc\\nbc\\nbgen\\nc.i\\ndhr\\ndr\\ndr.h.c\\ndrs\\ndrs\\nds\\neint\\nfa\\nFa\\nfam\\ngen\\ngenm\\ning\\nir\\njhr\\njkvr\\njr\\nkand\\nkol\\nlgen\\nlkol\\nLt\\nmaj\\nMej\\nmevr\\nMme\\nmr\\nmr\\nMw\\no.b.s\\nplv\\nprof\\nritm\\ntint\\nVz\\nZ.D\\nZ.D.H\\nZ.E\\nZ.Em\\nZ.H\\nZ.K.H\\nZ.K.M\\nZ.M\\nz.v\\na.g.v\\nbijv\\nbijz\\nbv\\nd.w.z\\ne.c\\ne.g\\ne.k\\nev\\ni.p.v\\ni.s.m\\ni.t.t\\ni.v.m\\nm.a.w\\nm.b.t\\nm.b.v\\nm.h.o\\nm.i\\nm.i.v\\nv.w.t\\nNr \\nNrs\\nnrs\\nnr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzqYm3HWnncl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARu_L7B_nptS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_nl = non_breaking_prefix_nl.split(\"\\n\")\n",
        "non_breaking_prefix_nl = [' ' + pref + '.' for pref in non_breaking_prefix_nl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgTsgsOhnr3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_nl = europarl_nl\n",
        "for prefix in non_breaking_prefix_nl:\n",
        "    corpus_nl = corpus_nl.replace(prefix, prefix + '$$$')\n",
        "corpus_nl = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_nl)\n",
        "corpus_nl = re.sub(r\".\\$\\$\\$\", '', corpus_nl)\n",
        "corpus_nl = re.sub(r\"  +\", \" \", corpus_nl)\n",
        "corpus_nl = corpus_nl.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9UQsZuXnuhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c8QaYROnwiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13\n",
        ")\n",
        "tokenizer_nl = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_nl, target_vocab_size=2**13\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YVgwwQvnyjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_NL = tokenizer_nl.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVHRXHkkn09T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "            for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_NL-2] + tokenizer_nl.encode(sentence) + [VOCAB_SIZE_NL-1]\n",
        "            for sentence in corpus_nl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tvPfBJ6n2zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Max Length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5gcd4hin4o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4fxJJaXn7Av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding=\"post\",\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding=\"post\",\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltbzZH6hn9yP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euGgVhqOn_9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Building "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPmx30IqoEJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Positional Encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhbLPvmvoJtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "  def get_angles(self, pos, i, d_model): # pos : (seq_length, 1) i: (1, d_model)\n",
        "    angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "    return pos * angles # (seq_length, d_model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "    angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis, :],\n",
        "                             d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UdHBKIoNQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsdFEoKhoPCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  product = tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XUrUFDqoU50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multihead attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxsRrJoJoZJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "    self.query_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units=self.d_model)\n",
        "    self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "  def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    splitted_inputs = tf.reshape(inputs, shape=shape) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    return tf.transpose(splitted_inputs, perm=[0, 2, 1, 3]) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    queries = self.query_lin(queries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)                          \n",
        "    values = self.split_proj(values, batch_size)             \n",
        "\n",
        "    attention = scaled_dot_product_attention(queries, keys, values, mask)    \n",
        "\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])       \n",
        "\n",
        "    concat_attention = tf.reshape(attention, \n",
        "                                  shape=(batch_size, -1, self.d_model))  \n",
        "    \n",
        "    outputs = self.final_lin(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJFlKN-ochL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlcnmJ5Qoew5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    attention = self.multi_head_attention(inputs,\n",
        "                                          inputs,\n",
        "                                          inputs,\n",
        "                                          mask)\n",
        "    attention = self.dropout_1(attention, training=training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    outputs = self.dense_1(attention)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_2(outputs, training=training)\n",
        "    outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhxU8vBioilf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name=\"encoder\"):\n",
        "    super(Encoder, self).__init__(name=name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "    self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                     nb_proj,\n",
        "                                     dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.enc_layers[i](outputs, mask, training)\n",
        "    \n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVHZYMCon1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSci5iwgoqMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    # self multi head attention\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # Multi head attention combined with encoder output\n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # Feed forward\n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    attention = self.multi_head_attention_1(inputs,\n",
        "                                            inputs,\n",
        "                                            inputs,\n",
        "                                            mask_1)\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                              enc_outputs,\n",
        "                                              enc_outputs,\n",
        "                                              mask_2)\n",
        "    \n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLNd-PQaot8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name=\"decoder\"):\n",
        "    super(Decoder, self).__init__(name=name)\n",
        "    self.d_model = d_model\n",
        "    self.nb_layers = nb_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                    nb_proj,\n",
        "                                    dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs= self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,\n",
        "                                   enc_outputs,\n",
        "                                   mask_1,\n",
        "                                   mask_2,\n",
        "                                   training)\n",
        "    return outputs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9VZqaN_oyaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnoJ1NdQo1nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92_yTG_vo5lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVUJgipoo8C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_NL,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_K-_GGuo-t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = loss_object(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DwdU-u4pBqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D9CMcuRpEjW",
        "colab_type": "code",
        "outputId": "2b03d38f-70dd-4452-f1a5-9e9c47568ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/My_Projects/Transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer\n",
        "                           )\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest checkpoint restored !!\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored !!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfDcu-68pLJC",
        "colab_type": "code",
        "outputId": "f3723696-90bd-4c29-a55e-c8abcb57c36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 1.3587 Accuracy 0.4400\n",
            "Epoch 1 Batch 50 Loss 1.3388 Accuracy 0.4252\n",
            "Epoch 1 Batch 100 Loss 1.3256 Accuracy 0.4262\n",
            "Epoch 1 Batch 150 Loss 1.3233 Accuracy 0.4274\n",
            "Epoch 1 Batch 200 Loss 1.3239 Accuracy 0.4283\n",
            "Epoch 1 Batch 250 Loss 1.3145 Accuracy 0.4289\n",
            "Epoch 1 Batch 300 Loss 1.3127 Accuracy 0.4297\n",
            "Epoch 1 Batch 350 Loss 1.3110 Accuracy 0.4296\n",
            "Epoch 1 Batch 400 Loss 1.3061 Accuracy 0.4298\n",
            "Epoch 1 Batch 450 Loss 1.3040 Accuracy 0.4301\n",
            "Epoch 1 Batch 500 Loss 1.2991 Accuracy 0.4299\n",
            "Epoch 1 Batch 550 Loss 1.2969 Accuracy 0.4299\n",
            "Epoch 1 Batch 600 Loss 1.2933 Accuracy 0.4300\n",
            "Epoch 1 Batch 650 Loss 1.2911 Accuracy 0.4299\n",
            "Epoch 1 Batch 700 Loss 1.2913 Accuracy 0.4294\n",
            "Epoch 1 Batch 750 Loss 1.2913 Accuracy 0.4294\n",
            "Epoch 1 Batch 800 Loss 1.2900 Accuracy 0.4295\n",
            "Epoch 1 Batch 850 Loss 1.2903 Accuracy 0.4296\n",
            "Epoch 1 Batch 900 Loss 1.2918 Accuracy 0.4294\n",
            "Epoch 1 Batch 950 Loss 1.2919 Accuracy 0.4293\n",
            "Epoch 1 Batch 1000 Loss 1.2922 Accuracy 0.4293\n",
            "Epoch 1 Batch 1050 Loss 1.2916 Accuracy 0.4293\n",
            "Epoch 1 Batch 1100 Loss 1.2923 Accuracy 0.4293\n",
            "Epoch 1 Batch 1150 Loss 1.2936 Accuracy 0.4290\n",
            "Epoch 1 Batch 1200 Loss 1.2928 Accuracy 0.4287\n",
            "Epoch 1 Batch 1250 Loss 1.2936 Accuracy 0.4287\n",
            "Epoch 1 Batch 1300 Loss 1.2933 Accuracy 0.4282\n",
            "Epoch 1 Batch 1350 Loss 1.2924 Accuracy 0.4281\n",
            "Epoch 1 Batch 1400 Loss 1.2930 Accuracy 0.4278\n",
            "Epoch 1 Batch 1450 Loss 1.2928 Accuracy 0.4277\n",
            "Epoch 1 Batch 1500 Loss 1.2923 Accuracy 0.4275\n",
            "Epoch 1 Batch 1550 Loss 1.2923 Accuracy 0.4274\n",
            "Epoch 1 Batch 1600 Loss 1.2913 Accuracy 0.4273\n",
            "Epoch 1 Batch 1650 Loss 1.2896 Accuracy 0.4274\n",
            "Epoch 1 Batch 1700 Loss 1.2888 Accuracy 0.4275\n",
            "Epoch 1 Batch 1750 Loss 1.2875 Accuracy 0.4277\n",
            "Epoch 1 Batch 1800 Loss 1.2858 Accuracy 0.4280\n",
            "Epoch 1 Batch 1850 Loss 1.2847 Accuracy 0.4282\n",
            "Epoch 1 Batch 1900 Loss 1.2829 Accuracy 0.4284\n",
            "Epoch 1 Batch 1950 Loss 1.2824 Accuracy 0.4287\n",
            "Epoch 1 Batch 2000 Loss 1.2810 Accuracy 0.4288\n",
            "Epoch 1 Batch 2050 Loss 1.2794 Accuracy 0.4290\n",
            "Epoch 1 Batch 2100 Loss 1.2785 Accuracy 0.4291\n",
            "Epoch 1 Batch 2150 Loss 1.2760 Accuracy 0.4292\n",
            "Epoch 1 Batch 2200 Loss 1.2746 Accuracy 0.4295\n",
            "Epoch 1 Batch 2250 Loss 1.2728 Accuracy 0.4297\n",
            "Epoch 1 Batch 2300 Loss 1.2707 Accuracy 0.4300\n",
            "Epoch 1 Batch 2350 Loss 1.2691 Accuracy 0.4303\n",
            "Epoch 1 Batch 2400 Loss 1.2667 Accuracy 0.4305\n",
            "Epoch 1 Batch 2450 Loss 1.2642 Accuracy 0.4307\n",
            "Epoch 1 Batch 2500 Loss 1.2617 Accuracy 0.4310\n",
            "Epoch 1 Batch 2550 Loss 1.2588 Accuracy 0.4311\n",
            "Epoch 1 Batch 2600 Loss 1.2558 Accuracy 0.4313\n",
            "Epoch 1 Batch 2650 Loss 1.2532 Accuracy 0.4317\n",
            "Epoch 1 Batch 2700 Loss 1.2500 Accuracy 0.4320\n",
            "Epoch 1 Batch 2750 Loss 1.2471 Accuracy 0.4324\n",
            "Epoch 1 Batch 2800 Loss 1.2436 Accuracy 0.4327\n",
            "Epoch 1 Batch 2850 Loss 1.2396 Accuracy 0.4332\n",
            "Epoch 1 Batch 2900 Loss 1.2363 Accuracy 0.4337\n",
            "Epoch 1 Batch 2950 Loss 1.2327 Accuracy 0.4341\n",
            "Epoch 1 Batch 3000 Loss 1.2290 Accuracy 0.4346\n",
            "Epoch 1 Batch 3050 Loss 1.2255 Accuracy 0.4351\n",
            "Epoch 1 Batch 3100 Loss 1.2220 Accuracy 0.4357\n",
            "Epoch 1 Batch 3150 Loss 1.2181 Accuracy 0.4362\n",
            "Epoch 1 Batch 3200 Loss 1.2151 Accuracy 0.4367\n",
            "Epoch 1 Batch 3250 Loss 1.2120 Accuracy 0.4373\n",
            "Epoch 1 Batch 3300 Loss 1.2094 Accuracy 0.4377\n",
            "Epoch 1 Batch 3350 Loss 1.2062 Accuracy 0.4381\n",
            "Epoch 1 Batch 3400 Loss 1.2032 Accuracy 0.4385\n",
            "Epoch 1 Batch 3450 Loss 1.2006 Accuracy 0.4389\n",
            "Epoch 1 Batch 3500 Loss 1.1980 Accuracy 0.4393\n",
            "Epoch 1 Batch 3550 Loss 1.1960 Accuracy 0.4397\n",
            "Epoch 1 Batch 3600 Loss 1.1942 Accuracy 0.4399\n",
            "Epoch 1 Batch 3650 Loss 1.1919 Accuracy 0.4401\n",
            "Epoch 1 Batch 3700 Loss 1.1898 Accuracy 0.4403\n",
            "Epoch 1 Batch 3750 Loss 1.1875 Accuracy 0.4406\n",
            "Epoch 1 Batch 3800 Loss 1.1854 Accuracy 0.4410\n",
            "Epoch 1 Batch 3850 Loss 1.1833 Accuracy 0.4413\n",
            "Epoch 1 Batch 3900 Loss 1.1816 Accuracy 0.4416\n",
            "Epoch 1 Batch 3950 Loss 1.1795 Accuracy 0.4419\n",
            "Epoch 1 Batch 4000 Loss 1.1776 Accuracy 0.4422\n",
            "Epoch 1 Batch 4050 Loss 1.1758 Accuracy 0.4425\n",
            "Epoch 1 Batch 4100 Loss 1.1740 Accuracy 0.4427\n",
            "Epoch 1 Batch 4150 Loss 1.1720 Accuracy 0.4430\n",
            "Epoch 1 Batch 4200 Loss 1.1698 Accuracy 0.4433\n",
            "Epoch 1 Batch 4250 Loss 1.1681 Accuracy 0.4437\n",
            "Epoch 1 Batch 4300 Loss 1.1665 Accuracy 0.4439\n",
            "Epoch 1 Batch 4350 Loss 1.1649 Accuracy 0.4442\n",
            "Epoch 1 Batch 4400 Loss 1.1632 Accuracy 0.4445\n",
            "Epoch 1 Batch 4450 Loss 1.1617 Accuracy 0.4447\n",
            "Epoch 1 Batch 4500 Loss 1.1605 Accuracy 0.4450\n",
            "Epoch 1 Batch 4550 Loss 1.1593 Accuracy 0.4453\n",
            "Epoch 1 Batch 4600 Loss 1.1579 Accuracy 0.4455\n",
            "Epoch 1 Batch 4650 Loss 1.1566 Accuracy 0.4458\n",
            "Epoch 1 Batch 4700 Loss 1.1557 Accuracy 0.4460\n",
            "Epoch 1 Batch 4750 Loss 1.1552 Accuracy 0.4461\n",
            "Epoch 1 Batch 4800 Loss 1.1548 Accuracy 0.4461\n",
            "Epoch 1 Batch 4850 Loss 1.1546 Accuracy 0.4461\n",
            "Epoch 1 Batch 4900 Loss 1.1548 Accuracy 0.4461\n",
            "Epoch 1 Batch 4950 Loss 1.1551 Accuracy 0.4461\n",
            "Epoch 1 Batch 5000 Loss 1.1555 Accuracy 0.4461\n",
            "Epoch 1 Batch 5050 Loss 1.1558 Accuracy 0.4460\n",
            "Epoch 1 Batch 5100 Loss 1.1563 Accuracy 0.4459\n",
            "Epoch 1 Batch 5150 Loss 1.1568 Accuracy 0.4459\n",
            "Epoch 1 Batch 5200 Loss 1.1570 Accuracy 0.4458\n",
            "Epoch 1 Batch 5250 Loss 1.1576 Accuracy 0.4458\n",
            "Epoch 1 Batch 5300 Loss 1.1581 Accuracy 0.4457\n",
            "Epoch 1 Batch 5350 Loss 1.1588 Accuracy 0.4456\n",
            "Epoch 1 Batch 5400 Loss 1.1595 Accuracy 0.4455\n",
            "Epoch 1 Batch 5450 Loss 1.1601 Accuracy 0.4454\n",
            "Epoch 1 Batch 5500 Loss 1.1611 Accuracy 0.4453\n",
            "Epoch 1 Batch 5550 Loss 1.1616 Accuracy 0.4452\n",
            "Epoch 1 Batch 5600 Loss 1.1623 Accuracy 0.4452\n",
            "Epoch 1 Batch 5650 Loss 1.1628 Accuracy 0.4451\n",
            "Epoch 1 Batch 5700 Loss 1.1634 Accuracy 0.4450\n",
            "Epoch 1 Batch 5750 Loss 1.1641 Accuracy 0.4449\n",
            "Epoch 1 Batch 5800 Loss 1.1649 Accuracy 0.4448\n",
            "Epoch 1 Batch 5850 Loss 1.1659 Accuracy 0.4448\n",
            "Epoch 1 Batch 5900 Loss 1.1665 Accuracy 0.4446\n",
            "Epoch 1 Batch 5950 Loss 1.1673 Accuracy 0.4445\n",
            "Epoch 1 Batch 6000 Loss 1.1685 Accuracy 0.4444\n",
            "Epoch 1 Batch 6050 Loss 1.1694 Accuracy 0.4442\n",
            "Epoch 1 Batch 6100 Loss 1.1703 Accuracy 0.4440\n",
            "Epoch 1 Batch 6150 Loss 1.1712 Accuracy 0.4438\n",
            "Epoch 1 Batch 6200 Loss 1.1724 Accuracy 0.4437\n",
            "Epoch 1 Batch 6250 Loss 1.1734 Accuracy 0.4435\n",
            "Epoch 1 Batch 6300 Loss 1.1742 Accuracy 0.4433\n",
            "Epoch 1 Batch 6350 Loss 1.1749 Accuracy 0.4431\n",
            "Epoch 1 Batch 6400 Loss 1.1756 Accuracy 0.4430\n",
            "Epoch 1 Batch 6450 Loss 1.1764 Accuracy 0.4428\n",
            "Epoch 1 Batch 6500 Loss 1.1772 Accuracy 0.4427\n",
            "Epoch 1 Batch 6550 Loss 1.1781 Accuracy 0.4426\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-11\n",
            "Time taken for 1 epoch: 2121.6521365642548 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.5107 Accuracy 0.4062\n",
            "Epoch 2 Batch 50 Loss 1.3226 Accuracy 0.4245\n",
            "Epoch 2 Batch 100 Loss 1.3182 Accuracy 0.4265\n",
            "Epoch 2 Batch 150 Loss 1.3137 Accuracy 0.4270\n",
            "Epoch 2 Batch 200 Loss 1.3192 Accuracy 0.4271\n",
            "Epoch 2 Batch 250 Loss 1.3128 Accuracy 0.4274\n",
            "Epoch 2 Batch 300 Loss 1.3053 Accuracy 0.4288\n",
            "Epoch 2 Batch 350 Loss 1.2975 Accuracy 0.4300\n",
            "Epoch 2 Batch 400 Loss 1.2930 Accuracy 0.4298\n",
            "Epoch 2 Batch 450 Loss 1.2922 Accuracy 0.4302\n",
            "Epoch 2 Batch 500 Loss 1.2892 Accuracy 0.4309\n",
            "Epoch 2 Batch 550 Loss 1.2847 Accuracy 0.4312\n",
            "Epoch 2 Batch 600 Loss 1.2825 Accuracy 0.4313\n",
            "Epoch 2 Batch 650 Loss 1.2818 Accuracy 0.4313\n",
            "Epoch 2 Batch 700 Loss 1.2839 Accuracy 0.4310\n",
            "Epoch 2 Batch 750 Loss 1.2830 Accuracy 0.4308\n",
            "Epoch 2 Batch 800 Loss 1.2837 Accuracy 0.4309\n",
            "Epoch 2 Batch 850 Loss 1.2847 Accuracy 0.4308\n",
            "Epoch 2 Batch 900 Loss 1.2834 Accuracy 0.4310\n",
            "Epoch 2 Batch 950 Loss 1.2833 Accuracy 0.4310\n",
            "Epoch 2 Batch 1000 Loss 1.2823 Accuracy 0.4309\n",
            "Epoch 2 Batch 1050 Loss 1.2822 Accuracy 0.4307\n",
            "Epoch 2 Batch 1100 Loss 1.2821 Accuracy 0.4306\n",
            "Epoch 2 Batch 1150 Loss 1.2823 Accuracy 0.4305\n",
            "Epoch 2 Batch 1200 Loss 1.2810 Accuracy 0.4302\n",
            "Epoch 2 Batch 1250 Loss 1.2812 Accuracy 0.4298\n",
            "Epoch 2 Batch 1300 Loss 1.2805 Accuracy 0.4297\n",
            "Epoch 2 Batch 1350 Loss 1.2798 Accuracy 0.4294\n",
            "Epoch 2 Batch 1400 Loss 1.2806 Accuracy 0.4293\n",
            "Epoch 2 Batch 1450 Loss 1.2815 Accuracy 0.4290\n",
            "Epoch 2 Batch 1500 Loss 1.2808 Accuracy 0.4291\n",
            "Epoch 2 Batch 1550 Loss 1.2812 Accuracy 0.4290\n",
            "Epoch 2 Batch 1600 Loss 1.2803 Accuracy 0.4292\n",
            "Epoch 2 Batch 1650 Loss 1.2796 Accuracy 0.4293\n",
            "Epoch 2 Batch 1700 Loss 1.2785 Accuracy 0.4296\n",
            "Epoch 2 Batch 1750 Loss 1.2769 Accuracy 0.4297\n",
            "Epoch 2 Batch 1800 Loss 1.2758 Accuracy 0.4299\n",
            "Epoch 2 Batch 1850 Loss 1.2746 Accuracy 0.4301\n",
            "Epoch 2 Batch 1900 Loss 1.2730 Accuracy 0.4303\n",
            "Epoch 2 Batch 1950 Loss 1.2720 Accuracy 0.4306\n",
            "Epoch 2 Batch 2000 Loss 1.2703 Accuracy 0.4306\n",
            "Epoch 2 Batch 2050 Loss 1.2688 Accuracy 0.4309\n",
            "Epoch 2 Batch 2100 Loss 1.2676 Accuracy 0.4311\n",
            "Epoch 2 Batch 2150 Loss 1.2656 Accuracy 0.4315\n",
            "Epoch 2 Batch 2200 Loss 1.2635 Accuracy 0.4315\n",
            "Epoch 2 Batch 2250 Loss 1.2614 Accuracy 0.4317\n",
            "Epoch 2 Batch 2300 Loss 1.2593 Accuracy 0.4319\n",
            "Epoch 2 Batch 2350 Loss 1.2570 Accuracy 0.4321\n",
            "Epoch 2 Batch 2400 Loss 1.2552 Accuracy 0.4323\n",
            "Epoch 2 Batch 2450 Loss 1.2532 Accuracy 0.4326\n",
            "Epoch 2 Batch 2500 Loss 1.2504 Accuracy 0.4328\n",
            "Epoch 2 Batch 2550 Loss 1.2471 Accuracy 0.4329\n",
            "Epoch 2 Batch 2600 Loss 1.2442 Accuracy 0.4331\n",
            "Epoch 2 Batch 2650 Loss 1.2410 Accuracy 0.4335\n",
            "Epoch 2 Batch 2700 Loss 1.2374 Accuracy 0.4338\n",
            "Epoch 2 Batch 2750 Loss 1.2342 Accuracy 0.4341\n",
            "Epoch 2 Batch 2800 Loss 1.2310 Accuracy 0.4345\n",
            "Epoch 2 Batch 2850 Loss 1.2280 Accuracy 0.4349\n",
            "Epoch 2 Batch 2900 Loss 1.2246 Accuracy 0.4353\n",
            "Epoch 2 Batch 2950 Loss 1.2209 Accuracy 0.4358\n",
            "Epoch 2 Batch 3000 Loss 1.2171 Accuracy 0.4363\n",
            "Epoch 2 Batch 3050 Loss 1.2131 Accuracy 0.4369\n",
            "Epoch 2 Batch 3100 Loss 1.2098 Accuracy 0.4374\n",
            "Epoch 2 Batch 3150 Loss 1.2066 Accuracy 0.4379\n",
            "Epoch 2 Batch 3200 Loss 1.2030 Accuracy 0.4384\n",
            "Epoch 2 Batch 3250 Loss 1.1997 Accuracy 0.4389\n",
            "Epoch 2 Batch 3300 Loss 1.1966 Accuracy 0.4394\n",
            "Epoch 2 Batch 3350 Loss 1.1940 Accuracy 0.4398\n",
            "Epoch 2 Batch 3400 Loss 1.1916 Accuracy 0.4402\n",
            "Epoch 2 Batch 3450 Loss 1.1892 Accuracy 0.4406\n",
            "Epoch 2 Batch 3500 Loss 1.1871 Accuracy 0.4409\n",
            "Epoch 2 Batch 3550 Loss 1.1849 Accuracy 0.4413\n",
            "Epoch 2 Batch 3600 Loss 1.1828 Accuracy 0.4416\n",
            "Epoch 2 Batch 3650 Loss 1.1804 Accuracy 0.4418\n",
            "Epoch 2 Batch 3700 Loss 1.1782 Accuracy 0.4421\n",
            "Epoch 2 Batch 3750 Loss 1.1761 Accuracy 0.4423\n",
            "Epoch 2 Batch 3800 Loss 1.1740 Accuracy 0.4427\n",
            "Epoch 2 Batch 3850 Loss 1.1718 Accuracy 0.4429\n",
            "Epoch 2 Batch 3900 Loss 1.1697 Accuracy 0.4433\n",
            "Epoch 2 Batch 3950 Loss 1.1681 Accuracy 0.4436\n",
            "Epoch 2 Batch 4000 Loss 1.1662 Accuracy 0.4439\n",
            "Epoch 2 Batch 4050 Loss 1.1643 Accuracy 0.4441\n",
            "Epoch 2 Batch 4100 Loss 1.1625 Accuracy 0.4445\n",
            "Epoch 2 Batch 4150 Loss 1.1601 Accuracy 0.4447\n",
            "Epoch 2 Batch 4200 Loss 1.1582 Accuracy 0.4450\n",
            "Epoch 2 Batch 4250 Loss 1.1564 Accuracy 0.4454\n",
            "Epoch 2 Batch 4300 Loss 1.1547 Accuracy 0.4457\n",
            "Epoch 2 Batch 4350 Loss 1.1534 Accuracy 0.4459\n",
            "Epoch 2 Batch 4400 Loss 1.1521 Accuracy 0.4461\n",
            "Epoch 2 Batch 4450 Loss 1.1506 Accuracy 0.4464\n",
            "Epoch 2 Batch 4500 Loss 1.1493 Accuracy 0.4466\n",
            "Epoch 2 Batch 4550 Loss 1.1479 Accuracy 0.4469\n",
            "Epoch 2 Batch 4600 Loss 1.1463 Accuracy 0.4471\n",
            "Epoch 2 Batch 4650 Loss 1.1450 Accuracy 0.4474\n",
            "Epoch 2 Batch 4700 Loss 1.1438 Accuracy 0.4475\n",
            "Epoch 2 Batch 4750 Loss 1.1429 Accuracy 0.4477\n",
            "Epoch 2 Batch 4800 Loss 1.1430 Accuracy 0.4477\n",
            "Epoch 2 Batch 4850 Loss 1.1430 Accuracy 0.4478\n",
            "Epoch 2 Batch 4900 Loss 1.1432 Accuracy 0.4478\n",
            "Epoch 2 Batch 4950 Loss 1.1433 Accuracy 0.4478\n",
            "Epoch 2 Batch 5000 Loss 1.1440 Accuracy 0.4478\n",
            "Epoch 2 Batch 5050 Loss 1.1446 Accuracy 0.4478\n",
            "Epoch 2 Batch 5100 Loss 1.1447 Accuracy 0.4477\n",
            "Epoch 2 Batch 5150 Loss 1.1451 Accuracy 0.4476\n",
            "Epoch 2 Batch 5200 Loss 1.1456 Accuracy 0.4475\n",
            "Epoch 2 Batch 5250 Loss 1.1461 Accuracy 0.4475\n",
            "Epoch 2 Batch 5300 Loss 1.1468 Accuracy 0.4474\n",
            "Epoch 2 Batch 5350 Loss 1.1475 Accuracy 0.4473\n",
            "Epoch 2 Batch 5400 Loss 1.1481 Accuracy 0.4473\n",
            "Epoch 2 Batch 5450 Loss 1.1486 Accuracy 0.4472\n",
            "Epoch 2 Batch 5500 Loss 1.1494 Accuracy 0.4471\n",
            "Epoch 2 Batch 5550 Loss 1.1500 Accuracy 0.4470\n",
            "Epoch 2 Batch 5600 Loss 1.1508 Accuracy 0.4469\n",
            "Epoch 2 Batch 5650 Loss 1.1513 Accuracy 0.4468\n",
            "Epoch 2 Batch 5700 Loss 1.1519 Accuracy 0.4467\n",
            "Epoch 2 Batch 5750 Loss 1.1526 Accuracy 0.4467\n",
            "Epoch 2 Batch 5800 Loss 1.1535 Accuracy 0.4466\n",
            "Epoch 2 Batch 5850 Loss 1.1541 Accuracy 0.4465\n",
            "Epoch 2 Batch 5900 Loss 1.1552 Accuracy 0.4463\n",
            "Epoch 2 Batch 5950 Loss 1.1560 Accuracy 0.4462\n",
            "Epoch 2 Batch 6000 Loss 1.1571 Accuracy 0.4461\n",
            "Epoch 2 Batch 6050 Loss 1.1581 Accuracy 0.4459\n",
            "Epoch 2 Batch 6100 Loss 1.1589 Accuracy 0.4457\n",
            "Epoch 2 Batch 6150 Loss 1.1598 Accuracy 0.4455\n",
            "Epoch 2 Batch 6200 Loss 1.1608 Accuracy 0.4454\n",
            "Epoch 2 Batch 6250 Loss 1.1617 Accuracy 0.4452\n",
            "Epoch 2 Batch 6300 Loss 1.1625 Accuracy 0.4450\n",
            "Epoch 2 Batch 6350 Loss 1.1634 Accuracy 0.4449\n",
            "Epoch 2 Batch 6400 Loss 1.1642 Accuracy 0.4447\n",
            "Epoch 2 Batch 6450 Loss 1.1651 Accuracy 0.4446\n",
            "Epoch 2 Batch 6500 Loss 1.1660 Accuracy 0.4444\n",
            "Epoch 2 Batch 6550 Loss 1.1666 Accuracy 0.4443\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-12\n",
            "Time taken for 1 epoch: 1715.399004459381 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.3280 Accuracy 0.4071\n",
            "Epoch 3 Batch 50 Loss 1.3202 Accuracy 0.4255\n",
            "Epoch 3 Batch 100 Loss 1.3281 Accuracy 0.4273\n",
            "Epoch 3 Batch 150 Loss 1.3158 Accuracy 0.4284\n",
            "Epoch 3 Batch 200 Loss 1.3021 Accuracy 0.4299\n",
            "Epoch 3 Batch 250 Loss 1.2972 Accuracy 0.4314\n",
            "Epoch 3 Batch 300 Loss 1.2871 Accuracy 0.4327\n",
            "Epoch 3 Batch 350 Loss 1.2882 Accuracy 0.4329\n",
            "Epoch 3 Batch 400 Loss 1.2844 Accuracy 0.4328\n",
            "Epoch 3 Batch 450 Loss 1.2808 Accuracy 0.4332\n",
            "Epoch 3 Batch 500 Loss 1.2802 Accuracy 0.4333\n",
            "Epoch 3 Batch 550 Loss 1.2764 Accuracy 0.4336\n",
            "Epoch 3 Batch 600 Loss 1.2730 Accuracy 0.4333\n",
            "Epoch 3 Batch 650 Loss 1.2721 Accuracy 0.4332\n",
            "Epoch 3 Batch 700 Loss 1.2709 Accuracy 0.4332\n",
            "Epoch 3 Batch 750 Loss 1.2717 Accuracy 0.4329\n",
            "Epoch 3 Batch 800 Loss 1.2710 Accuracy 0.4331\n",
            "Epoch 3 Batch 850 Loss 1.2705 Accuracy 0.4333\n",
            "Epoch 3 Batch 900 Loss 1.2707 Accuracy 0.4336\n",
            "Epoch 3 Batch 950 Loss 1.2708 Accuracy 0.4337\n",
            "Epoch 3 Batch 1000 Loss 1.2698 Accuracy 0.4335\n",
            "Epoch 3 Batch 1050 Loss 1.2688 Accuracy 0.4332\n",
            "Epoch 3 Batch 1100 Loss 1.2679 Accuracy 0.4329\n",
            "Epoch 3 Batch 1150 Loss 1.2677 Accuracy 0.4329\n",
            "Epoch 3 Batch 1200 Loss 1.2683 Accuracy 0.4324\n",
            "Epoch 3 Batch 1250 Loss 1.2680 Accuracy 0.4322\n",
            "Epoch 3 Batch 1300 Loss 1.2677 Accuracy 0.4322\n",
            "Epoch 3 Batch 1350 Loss 1.2682 Accuracy 0.4320\n",
            "Epoch 3 Batch 1400 Loss 1.2686 Accuracy 0.4318\n",
            "Epoch 3 Batch 1450 Loss 1.2677 Accuracy 0.4316\n",
            "Epoch 3 Batch 1500 Loss 1.2675 Accuracy 0.4314\n",
            "Epoch 3 Batch 1550 Loss 1.2674 Accuracy 0.4312\n",
            "Epoch 3 Batch 1600 Loss 1.2667 Accuracy 0.4310\n",
            "Epoch 3 Batch 1650 Loss 1.2658 Accuracy 0.4312\n",
            "Epoch 3 Batch 1700 Loss 1.2648 Accuracy 0.4314\n",
            "Epoch 3 Batch 1750 Loss 1.2634 Accuracy 0.4316\n",
            "Epoch 3 Batch 1800 Loss 1.2630 Accuracy 0.4318\n",
            "Epoch 3 Batch 1850 Loss 1.2622 Accuracy 0.4319\n",
            "Epoch 3 Batch 1900 Loss 1.2609 Accuracy 0.4322\n",
            "Epoch 3 Batch 1950 Loss 1.2594 Accuracy 0.4324\n",
            "Epoch 3 Batch 2000 Loss 1.2586 Accuracy 0.4326\n",
            "Epoch 3 Batch 2050 Loss 1.2573 Accuracy 0.4327\n",
            "Epoch 3 Batch 2100 Loss 1.2558 Accuracy 0.4330\n",
            "Epoch 3 Batch 2150 Loss 1.2542 Accuracy 0.4331\n",
            "Epoch 3 Batch 2200 Loss 1.2526 Accuracy 0.4333\n",
            "Epoch 3 Batch 2250 Loss 1.2512 Accuracy 0.4334\n",
            "Epoch 3 Batch 2300 Loss 1.2495 Accuracy 0.4335\n",
            "Epoch 3 Batch 2350 Loss 1.2471 Accuracy 0.4338\n",
            "Epoch 3 Batch 2400 Loss 1.2442 Accuracy 0.4341\n",
            "Epoch 3 Batch 2450 Loss 1.2410 Accuracy 0.4343\n",
            "Epoch 3 Batch 2500 Loss 1.2382 Accuracy 0.4345\n",
            "Epoch 3 Batch 2550 Loss 1.2352 Accuracy 0.4347\n",
            "Epoch 3 Batch 2600 Loss 1.2324 Accuracy 0.4349\n",
            "Epoch 3 Batch 2650 Loss 1.2295 Accuracy 0.4352\n",
            "Epoch 3 Batch 2700 Loss 1.2267 Accuracy 0.4356\n",
            "Epoch 3 Batch 2750 Loss 1.2230 Accuracy 0.4360\n",
            "Epoch 3 Batch 2800 Loss 1.2197 Accuracy 0.4364\n",
            "Epoch 3 Batch 2850 Loss 1.2168 Accuracy 0.4367\n",
            "Epoch 3 Batch 2900 Loss 1.2135 Accuracy 0.4371\n",
            "Epoch 3 Batch 2950 Loss 1.2101 Accuracy 0.4376\n",
            "Epoch 3 Batch 3000 Loss 1.2066 Accuracy 0.4381\n",
            "Epoch 3 Batch 3050 Loss 1.2032 Accuracy 0.4387\n",
            "Epoch 3 Batch 3100 Loss 1.2000 Accuracy 0.4391\n",
            "Epoch 3 Batch 3150 Loss 1.1966 Accuracy 0.4397\n",
            "Epoch 3 Batch 3200 Loss 1.1931 Accuracy 0.4402\n",
            "Epoch 3 Batch 3250 Loss 1.1896 Accuracy 0.4406\n",
            "Epoch 3 Batch 3300 Loss 1.1870 Accuracy 0.4410\n",
            "Epoch 3 Batch 3350 Loss 1.1840 Accuracy 0.4414\n",
            "Epoch 3 Batch 3400 Loss 1.1812 Accuracy 0.4417\n",
            "Epoch 3 Batch 3450 Loss 1.1786 Accuracy 0.4421\n",
            "Epoch 3 Batch 3500 Loss 1.1765 Accuracy 0.4425\n",
            "Epoch 3 Batch 3550 Loss 1.1739 Accuracy 0.4428\n",
            "Epoch 3 Batch 3600 Loss 1.1717 Accuracy 0.4431\n",
            "Epoch 3 Batch 3650 Loss 1.1696 Accuracy 0.4434\n",
            "Epoch 3 Batch 3700 Loss 1.1676 Accuracy 0.4436\n",
            "Epoch 3 Batch 3750 Loss 1.1653 Accuracy 0.4438\n",
            "Epoch 3 Batch 3800 Loss 1.1631 Accuracy 0.4441\n",
            "Epoch 3 Batch 3850 Loss 1.1609 Accuracy 0.4445\n",
            "Epoch 3 Batch 3900 Loss 1.1587 Accuracy 0.4447\n",
            "Epoch 3 Batch 3950 Loss 1.1570 Accuracy 0.4450\n",
            "Epoch 3 Batch 4000 Loss 1.1551 Accuracy 0.4452\n",
            "Epoch 3 Batch 4050 Loss 1.1536 Accuracy 0.4455\n",
            "Epoch 3 Batch 4100 Loss 1.1517 Accuracy 0.4458\n",
            "Epoch 3 Batch 4150 Loss 1.1500 Accuracy 0.4461\n",
            "Epoch 3 Batch 4200 Loss 1.1481 Accuracy 0.4464\n",
            "Epoch 3 Batch 4250 Loss 1.1465 Accuracy 0.4468\n",
            "Epoch 3 Batch 4300 Loss 1.1450 Accuracy 0.4471\n",
            "Epoch 3 Batch 4350 Loss 1.1434 Accuracy 0.4474\n",
            "Epoch 3 Batch 4400 Loss 1.1418 Accuracy 0.4476\n",
            "Epoch 3 Batch 4450 Loss 1.1405 Accuracy 0.4479\n",
            "Epoch 3 Batch 4500 Loss 1.1391 Accuracy 0.4482\n",
            "Epoch 3 Batch 4550 Loss 1.1374 Accuracy 0.4484\n",
            "Epoch 3 Batch 4600 Loss 1.1360 Accuracy 0.4487\n",
            "Epoch 3 Batch 4650 Loss 1.1346 Accuracy 0.4489\n",
            "Epoch 3 Batch 4700 Loss 1.1337 Accuracy 0.4491\n",
            "Epoch 3 Batch 4750 Loss 1.1329 Accuracy 0.4493\n",
            "Epoch 3 Batch 4800 Loss 1.1327 Accuracy 0.4494\n",
            "Epoch 3 Batch 4850 Loss 1.1325 Accuracy 0.4493\n",
            "Epoch 3 Batch 4900 Loss 1.1326 Accuracy 0.4494\n",
            "Epoch 3 Batch 4950 Loss 1.1326 Accuracy 0.4493\n",
            "Epoch 3 Batch 5000 Loss 1.1328 Accuracy 0.4493\n",
            "Epoch 3 Batch 5050 Loss 1.1331 Accuracy 0.4492\n",
            "Epoch 3 Batch 5100 Loss 1.1337 Accuracy 0.4492\n",
            "Epoch 3 Batch 5150 Loss 1.1345 Accuracy 0.4490\n",
            "Epoch 3 Batch 5200 Loss 1.1352 Accuracy 0.4490\n",
            "Epoch 3 Batch 5250 Loss 1.1355 Accuracy 0.4489\n",
            "Epoch 3 Batch 5300 Loss 1.1359 Accuracy 0.4488\n",
            "Epoch 3 Batch 5350 Loss 1.1367 Accuracy 0.4487\n",
            "Epoch 3 Batch 5400 Loss 1.1375 Accuracy 0.4487\n",
            "Epoch 3 Batch 5450 Loss 1.1382 Accuracy 0.4486\n",
            "Epoch 3 Batch 5500 Loss 1.1387 Accuracy 0.4485\n",
            "Epoch 3 Batch 5550 Loss 1.1393 Accuracy 0.4484\n",
            "Epoch 3 Batch 5600 Loss 1.1400 Accuracy 0.4484\n",
            "Epoch 3 Batch 5650 Loss 1.1408 Accuracy 0.4483\n",
            "Epoch 3 Batch 5700 Loss 1.1416 Accuracy 0.4482\n",
            "Epoch 3 Batch 5750 Loss 1.1422 Accuracy 0.4482\n",
            "Epoch 3 Batch 5800 Loss 1.1431 Accuracy 0.4481\n",
            "Epoch 3 Batch 5850 Loss 1.1439 Accuracy 0.4480\n",
            "Epoch 3 Batch 5900 Loss 1.1451 Accuracy 0.4478\n",
            "Epoch 3 Batch 5950 Loss 1.1462 Accuracy 0.4477\n",
            "Epoch 3 Batch 6000 Loss 1.1471 Accuracy 0.4475\n",
            "Epoch 3 Batch 6050 Loss 1.1481 Accuracy 0.4474\n",
            "Epoch 3 Batch 6100 Loss 1.1492 Accuracy 0.4472\n",
            "Epoch 3 Batch 6150 Loss 1.1504 Accuracy 0.4470\n",
            "Epoch 3 Batch 6200 Loss 1.1512 Accuracy 0.4468\n",
            "Epoch 3 Batch 6250 Loss 1.1522 Accuracy 0.4467\n",
            "Epoch 3 Batch 6300 Loss 1.1528 Accuracy 0.4465\n",
            "Epoch 3 Batch 6350 Loss 1.1535 Accuracy 0.4463\n",
            "Epoch 3 Batch 6400 Loss 1.1544 Accuracy 0.4462\n",
            "Epoch 3 Batch 6450 Loss 1.1554 Accuracy 0.4461\n",
            "Epoch 3 Batch 6500 Loss 1.1561 Accuracy 0.4459\n",
            "Epoch 3 Batch 6550 Loss 1.1568 Accuracy 0.4458\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-13\n",
            "Time taken for 1 epoch: 1705.3150599002838 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.3494 Accuracy 0.4515\n",
            "Epoch 4 Batch 50 Loss 1.2868 Accuracy 0.4315\n",
            "Epoch 4 Batch 100 Loss 1.2897 Accuracy 0.4315\n",
            "Epoch 4 Batch 150 Loss 1.2943 Accuracy 0.4297\n",
            "Epoch 4 Batch 200 Loss 1.2856 Accuracy 0.4311\n",
            "Epoch 4 Batch 250 Loss 1.2813 Accuracy 0.4320\n",
            "Epoch 4 Batch 300 Loss 1.2833 Accuracy 0.4325\n",
            "Epoch 4 Batch 350 Loss 1.2830 Accuracy 0.4337\n",
            "Epoch 4 Batch 400 Loss 1.2789 Accuracy 0.4339\n",
            "Epoch 4 Batch 450 Loss 1.2740 Accuracy 0.4344\n",
            "Epoch 4 Batch 500 Loss 1.2714 Accuracy 0.4345\n",
            "Epoch 4 Batch 550 Loss 1.2692 Accuracy 0.4347\n",
            "Epoch 4 Batch 600 Loss 1.2671 Accuracy 0.4342\n",
            "Epoch 4 Batch 650 Loss 1.2653 Accuracy 0.4341\n",
            "Epoch 4 Batch 700 Loss 1.2658 Accuracy 0.4340\n",
            "Epoch 4 Batch 750 Loss 1.2659 Accuracy 0.4340\n",
            "Epoch 4 Batch 800 Loss 1.2650 Accuracy 0.4338\n",
            "Epoch 4 Batch 850 Loss 1.2619 Accuracy 0.4340\n",
            "Epoch 4 Batch 900 Loss 1.2622 Accuracy 0.4344\n",
            "Epoch 4 Batch 950 Loss 1.2620 Accuracy 0.4345\n",
            "Epoch 4 Batch 1000 Loss 1.2624 Accuracy 0.4343\n",
            "Epoch 4 Batch 1050 Loss 1.2625 Accuracy 0.4339\n",
            "Epoch 4 Batch 1100 Loss 1.2614 Accuracy 0.4338\n",
            "Epoch 4 Batch 1150 Loss 1.2619 Accuracy 0.4335\n",
            "Epoch 4 Batch 1200 Loss 1.2608 Accuracy 0.4332\n",
            "Epoch 4 Batch 1250 Loss 1.2606 Accuracy 0.4332\n",
            "Epoch 4 Batch 1300 Loss 1.2611 Accuracy 0.4330\n",
            "Epoch 4 Batch 1350 Loss 1.2620 Accuracy 0.4326\n",
            "Epoch 4 Batch 1400 Loss 1.2627 Accuracy 0.4324\n",
            "Epoch 4 Batch 1450 Loss 1.2617 Accuracy 0.4323\n",
            "Epoch 4 Batch 1500 Loss 1.2609 Accuracy 0.4320\n",
            "Epoch 4 Batch 1550 Loss 1.2599 Accuracy 0.4321\n",
            "Epoch 4 Batch 1600 Loss 1.2584 Accuracy 0.4320\n",
            "Epoch 4 Batch 1650 Loss 1.2581 Accuracy 0.4321\n",
            "Epoch 4 Batch 1700 Loss 1.2573 Accuracy 0.4323\n",
            "Epoch 4 Batch 1750 Loss 1.2557 Accuracy 0.4324\n",
            "Epoch 4 Batch 1800 Loss 1.2536 Accuracy 0.4327\n",
            "Epoch 4 Batch 1850 Loss 1.2520 Accuracy 0.4330\n",
            "Epoch 4 Batch 1900 Loss 1.2506 Accuracy 0.4332\n",
            "Epoch 4 Batch 1950 Loss 1.2494 Accuracy 0.4335\n",
            "Epoch 4 Batch 2000 Loss 1.2482 Accuracy 0.4336\n",
            "Epoch 4 Batch 2050 Loss 1.2468 Accuracy 0.4337\n",
            "Epoch 4 Batch 2100 Loss 1.2459 Accuracy 0.4339\n",
            "Epoch 4 Batch 2150 Loss 1.2439 Accuracy 0.4342\n",
            "Epoch 4 Batch 2200 Loss 1.2423 Accuracy 0.4344\n",
            "Epoch 4 Batch 2250 Loss 1.2407 Accuracy 0.4347\n",
            "Epoch 4 Batch 2300 Loss 1.2386 Accuracy 0.4349\n",
            "Epoch 4 Batch 2350 Loss 1.2360 Accuracy 0.4351\n",
            "Epoch 4 Batch 2400 Loss 1.2336 Accuracy 0.4353\n",
            "Epoch 4 Batch 2450 Loss 1.2309 Accuracy 0.4356\n",
            "Epoch 4 Batch 2500 Loss 1.2280 Accuracy 0.4358\n",
            "Epoch 4 Batch 2550 Loss 1.2250 Accuracy 0.4360\n",
            "Epoch 4 Batch 2600 Loss 1.2229 Accuracy 0.4362\n",
            "Epoch 4 Batch 2650 Loss 1.2201 Accuracy 0.4365\n",
            "Epoch 4 Batch 2700 Loss 1.2173 Accuracy 0.4368\n",
            "Epoch 4 Batch 2750 Loss 1.2138 Accuracy 0.4371\n",
            "Epoch 4 Batch 2800 Loss 1.2106 Accuracy 0.4375\n",
            "Epoch 4 Batch 2850 Loss 1.2072 Accuracy 0.4379\n",
            "Epoch 4 Batch 2900 Loss 1.2043 Accuracy 0.4384\n",
            "Epoch 4 Batch 2950 Loss 1.2009 Accuracy 0.4388\n",
            "Epoch 4 Batch 3000 Loss 1.1970 Accuracy 0.4394\n",
            "Epoch 4 Batch 3050 Loss 1.1933 Accuracy 0.4399\n",
            "Epoch 4 Batch 3100 Loss 1.1897 Accuracy 0.4404\n",
            "Epoch 4 Batch 3150 Loss 1.1865 Accuracy 0.4410\n",
            "Epoch 4 Batch 3200 Loss 1.1831 Accuracy 0.4415\n",
            "Epoch 4 Batch 3250 Loss 1.1797 Accuracy 0.4420\n",
            "Epoch 4 Batch 3300 Loss 1.1770 Accuracy 0.4425\n",
            "Epoch 4 Batch 3350 Loss 1.1743 Accuracy 0.4429\n",
            "Epoch 4 Batch 3400 Loss 1.1718 Accuracy 0.4433\n",
            "Epoch 4 Batch 3450 Loss 1.1694 Accuracy 0.4437\n",
            "Epoch 4 Batch 3500 Loss 1.1670 Accuracy 0.4440\n",
            "Epoch 4 Batch 3550 Loss 1.1647 Accuracy 0.4443\n",
            "Epoch 4 Batch 3600 Loss 1.1625 Accuracy 0.4447\n",
            "Epoch 4 Batch 3650 Loss 1.1602 Accuracy 0.4450\n",
            "Epoch 4 Batch 3700 Loss 1.1577 Accuracy 0.4452\n",
            "Epoch 4 Batch 3750 Loss 1.1556 Accuracy 0.4454\n",
            "Epoch 4 Batch 3800 Loss 1.1534 Accuracy 0.4457\n",
            "Epoch 4 Batch 3850 Loss 1.1511 Accuracy 0.4461\n",
            "Epoch 4 Batch 3900 Loss 1.1491 Accuracy 0.4464\n",
            "Epoch 4 Batch 3950 Loss 1.1472 Accuracy 0.4467\n",
            "Epoch 4 Batch 4000 Loss 1.1454 Accuracy 0.4469\n",
            "Epoch 4 Batch 4050 Loss 1.1440 Accuracy 0.4472\n",
            "Epoch 4 Batch 4100 Loss 1.1423 Accuracy 0.4474\n",
            "Epoch 4 Batch 4150 Loss 1.1403 Accuracy 0.4477\n",
            "Epoch 4 Batch 4200 Loss 1.1382 Accuracy 0.4480\n",
            "Epoch 4 Batch 4250 Loss 1.1366 Accuracy 0.4483\n",
            "Epoch 4 Batch 4300 Loss 1.1350 Accuracy 0.4486\n",
            "Epoch 4 Batch 4350 Loss 1.1337 Accuracy 0.4489\n",
            "Epoch 4 Batch 4400 Loss 1.1324 Accuracy 0.4492\n",
            "Epoch 4 Batch 4450 Loss 1.1310 Accuracy 0.4495\n",
            "Epoch 4 Batch 4500 Loss 1.1297 Accuracy 0.4497\n",
            "Epoch 4 Batch 4550 Loss 1.1282 Accuracy 0.4499\n",
            "Epoch 4 Batch 4600 Loss 1.1268 Accuracy 0.4502\n",
            "Epoch 4 Batch 4650 Loss 1.1255 Accuracy 0.4504\n",
            "Epoch 4 Batch 4700 Loss 1.1245 Accuracy 0.4506\n",
            "Epoch 4 Batch 4750 Loss 1.1240 Accuracy 0.4507\n",
            "Epoch 4 Batch 4800 Loss 1.1237 Accuracy 0.4508\n",
            "Epoch 4 Batch 4850 Loss 1.1237 Accuracy 0.4508\n",
            "Epoch 4 Batch 4900 Loss 1.1237 Accuracy 0.4508\n",
            "Epoch 4 Batch 4950 Loss 1.1240 Accuracy 0.4508\n",
            "Epoch 4 Batch 5000 Loss 1.1240 Accuracy 0.4508\n",
            "Epoch 4 Batch 5050 Loss 1.1245 Accuracy 0.4508\n",
            "Epoch 4 Batch 5100 Loss 1.1251 Accuracy 0.4507\n",
            "Epoch 4 Batch 5150 Loss 1.1257 Accuracy 0.4507\n",
            "Epoch 4 Batch 5200 Loss 1.1263 Accuracy 0.4506\n",
            "Epoch 4 Batch 5250 Loss 1.1270 Accuracy 0.4506\n",
            "Epoch 4 Batch 5300 Loss 1.1277 Accuracy 0.4504\n",
            "Epoch 4 Batch 5350 Loss 1.1282 Accuracy 0.4503\n",
            "Epoch 4 Batch 5400 Loss 1.1290 Accuracy 0.4502\n",
            "Epoch 4 Batch 5450 Loss 1.1294 Accuracy 0.4502\n",
            "Epoch 4 Batch 5500 Loss 1.1299 Accuracy 0.4501\n",
            "Epoch 4 Batch 5550 Loss 1.1305 Accuracy 0.4500\n",
            "Epoch 4 Batch 5600 Loss 1.1313 Accuracy 0.4499\n",
            "Epoch 4 Batch 5650 Loss 1.1319 Accuracy 0.4498\n",
            "Epoch 4 Batch 5700 Loss 1.1327 Accuracy 0.4497\n",
            "Epoch 4 Batch 5750 Loss 1.1335 Accuracy 0.4497\n",
            "Epoch 4 Batch 5800 Loss 1.1343 Accuracy 0.4496\n",
            "Epoch 4 Batch 5850 Loss 1.1353 Accuracy 0.4495\n",
            "Epoch 4 Batch 5900 Loss 1.1358 Accuracy 0.4493\n",
            "Epoch 4 Batch 5950 Loss 1.1368 Accuracy 0.4492\n",
            "Epoch 4 Batch 6000 Loss 1.1377 Accuracy 0.4491\n",
            "Epoch 4 Batch 6050 Loss 1.1388 Accuracy 0.4489\n",
            "Epoch 4 Batch 6100 Loss 1.1399 Accuracy 0.4487\n",
            "Epoch 4 Batch 6150 Loss 1.1410 Accuracy 0.4485\n",
            "Epoch 4 Batch 6200 Loss 1.1420 Accuracy 0.4483\n",
            "Epoch 4 Batch 6250 Loss 1.1428 Accuracy 0.4481\n",
            "Epoch 4 Batch 6300 Loss 1.1436 Accuracy 0.4480\n",
            "Epoch 4 Batch 6350 Loss 1.1446 Accuracy 0.4478\n",
            "Epoch 4 Batch 6400 Loss 1.1454 Accuracy 0.4476\n",
            "Epoch 4 Batch 6450 Loss 1.1460 Accuracy 0.4475\n",
            "Epoch 4 Batch 6500 Loss 1.1468 Accuracy 0.4473\n",
            "Epoch 4 Batch 6550 Loss 1.1476 Accuracy 0.4472\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-14\n",
            "Time taken for 1 epoch: 1702.2672877311707 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.2048 Accuracy 0.4071\n",
            "Epoch 5 Batch 50 Loss 1.2802 Accuracy 0.4321\n",
            "Epoch 5 Batch 100 Loss 1.2834 Accuracy 0.4307\n",
            "Epoch 5 Batch 150 Loss 1.2875 Accuracy 0.4324\n",
            "Epoch 5 Batch 200 Loss 1.2856 Accuracy 0.4324\n",
            "Epoch 5 Batch 250 Loss 1.2820 Accuracy 0.4328\n",
            "Epoch 5 Batch 300 Loss 1.2743 Accuracy 0.4329\n",
            "Epoch 5 Batch 350 Loss 1.2752 Accuracy 0.4335\n",
            "Epoch 5 Batch 400 Loss 1.2700 Accuracy 0.4337\n",
            "Epoch 5 Batch 450 Loss 1.2645 Accuracy 0.4342\n",
            "Epoch 5 Batch 500 Loss 1.2602 Accuracy 0.4343\n",
            "Epoch 5 Batch 550 Loss 1.2592 Accuracy 0.4345\n",
            "Epoch 5 Batch 600 Loss 1.2563 Accuracy 0.4349\n",
            "Epoch 5 Batch 650 Loss 1.2553 Accuracy 0.4350\n",
            "Epoch 5 Batch 700 Loss 1.2555 Accuracy 0.4348\n",
            "Epoch 5 Batch 750 Loss 1.2548 Accuracy 0.4348\n",
            "Epoch 5 Batch 800 Loss 1.2536 Accuracy 0.4349\n",
            "Epoch 5 Batch 850 Loss 1.2525 Accuracy 0.4351\n",
            "Epoch 5 Batch 900 Loss 1.2513 Accuracy 0.4352\n",
            "Epoch 5 Batch 950 Loss 1.2517 Accuracy 0.4353\n",
            "Epoch 5 Batch 1000 Loss 1.2511 Accuracy 0.4351\n",
            "Epoch 5 Batch 1050 Loss 1.2511 Accuracy 0.4349\n",
            "Epoch 5 Batch 1100 Loss 1.2509 Accuracy 0.4347\n",
            "Epoch 5 Batch 1150 Loss 1.2509 Accuracy 0.4346\n",
            "Epoch 5 Batch 1200 Loss 1.2501 Accuracy 0.4344\n",
            "Epoch 5 Batch 1250 Loss 1.2498 Accuracy 0.4343\n",
            "Epoch 5 Batch 1300 Loss 1.2498 Accuracy 0.4343\n",
            "Epoch 5 Batch 1350 Loss 1.2510 Accuracy 0.4342\n",
            "Epoch 5 Batch 1400 Loss 1.2501 Accuracy 0.4340\n",
            "Epoch 5 Batch 1450 Loss 1.2511 Accuracy 0.4336\n",
            "Epoch 5 Batch 1500 Loss 1.2504 Accuracy 0.4334\n",
            "Epoch 5 Batch 1550 Loss 1.2496 Accuracy 0.4336\n",
            "Epoch 5 Batch 1600 Loss 1.2487 Accuracy 0.4335\n",
            "Epoch 5 Batch 1650 Loss 1.2475 Accuracy 0.4335\n",
            "Epoch 5 Batch 1700 Loss 1.2466 Accuracy 0.4339\n",
            "Epoch 5 Batch 1750 Loss 1.2447 Accuracy 0.4342\n",
            "Epoch 5 Batch 1800 Loss 1.2437 Accuracy 0.4345\n",
            "Epoch 5 Batch 1850 Loss 1.2421 Accuracy 0.4346\n",
            "Epoch 5 Batch 1900 Loss 1.2417 Accuracy 0.4346\n",
            "Epoch 5 Batch 1950 Loss 1.2411 Accuracy 0.4347\n",
            "Epoch 5 Batch 2000 Loss 1.2402 Accuracy 0.4349\n",
            "Epoch 5 Batch 2050 Loss 1.2385 Accuracy 0.4350\n",
            "Epoch 5 Batch 2100 Loss 1.2370 Accuracy 0.4353\n",
            "Epoch 5 Batch 2150 Loss 1.2360 Accuracy 0.4355\n",
            "Epoch 5 Batch 2200 Loss 1.2341 Accuracy 0.4358\n",
            "Epoch 5 Batch 2250 Loss 1.2324 Accuracy 0.4359\n",
            "Epoch 5 Batch 2300 Loss 1.2315 Accuracy 0.4361\n",
            "Epoch 5 Batch 2350 Loss 1.2292 Accuracy 0.4364\n",
            "Epoch 5 Batch 2400 Loss 1.2264 Accuracy 0.4367\n",
            "Epoch 5 Batch 2450 Loss 1.2239 Accuracy 0.4370\n",
            "Epoch 5 Batch 2500 Loss 1.2213 Accuracy 0.4372\n",
            "Epoch 5 Batch 2550 Loss 1.2185 Accuracy 0.4374\n",
            "Epoch 5 Batch 2600 Loss 1.2147 Accuracy 0.4376\n",
            "Epoch 5 Batch 2650 Loss 1.2114 Accuracy 0.4379\n",
            "Epoch 5 Batch 2700 Loss 1.2082 Accuracy 0.4382\n",
            "Epoch 5 Batch 2750 Loss 1.2050 Accuracy 0.4386\n",
            "Epoch 5 Batch 2800 Loss 1.2021 Accuracy 0.4390\n",
            "Epoch 5 Batch 2850 Loss 1.1990 Accuracy 0.4395\n",
            "Epoch 5 Batch 2900 Loss 1.1955 Accuracy 0.4399\n",
            "Epoch 5 Batch 2950 Loss 1.1922 Accuracy 0.4404\n",
            "Epoch 5 Batch 3000 Loss 1.1886 Accuracy 0.4410\n",
            "Epoch 5 Batch 3050 Loss 1.1851 Accuracy 0.4414\n",
            "Epoch 5 Batch 3100 Loss 1.1814 Accuracy 0.4420\n",
            "Epoch 5 Batch 3150 Loss 1.1779 Accuracy 0.4425\n",
            "Epoch 5 Batch 3200 Loss 1.1744 Accuracy 0.4430\n",
            "Epoch 5 Batch 3250 Loss 1.1716 Accuracy 0.4434\n",
            "Epoch 5 Batch 3300 Loss 1.1691 Accuracy 0.4438\n",
            "Epoch 5 Batch 3350 Loss 1.1663 Accuracy 0.4443\n",
            "Epoch 5 Batch 3400 Loss 1.1637 Accuracy 0.4446\n",
            "Epoch 5 Batch 3450 Loss 1.1610 Accuracy 0.4449\n",
            "Epoch 5 Batch 3500 Loss 1.1588 Accuracy 0.4453\n",
            "Epoch 5 Batch 3550 Loss 1.1565 Accuracy 0.4456\n",
            "Epoch 5 Batch 3600 Loss 1.1541 Accuracy 0.4459\n",
            "Epoch 5 Batch 3650 Loss 1.1521 Accuracy 0.4462\n",
            "Epoch 5 Batch 3700 Loss 1.1497 Accuracy 0.4465\n",
            "Epoch 5 Batch 3750 Loss 1.1476 Accuracy 0.4467\n",
            "Epoch 5 Batch 3800 Loss 1.1452 Accuracy 0.4470\n",
            "Epoch 5 Batch 3850 Loss 1.1428 Accuracy 0.4473\n",
            "Epoch 5 Batch 3900 Loss 1.1412 Accuracy 0.4476\n",
            "Epoch 5 Batch 3950 Loss 1.1390 Accuracy 0.4479\n",
            "Epoch 5 Batch 4000 Loss 1.1366 Accuracy 0.4482\n",
            "Epoch 5 Batch 4050 Loss 1.1347 Accuracy 0.4485\n",
            "Epoch 5 Batch 4100 Loss 1.1328 Accuracy 0.4488\n",
            "Epoch 5 Batch 4150 Loss 1.1311 Accuracy 0.4491\n",
            "Epoch 5 Batch 4200 Loss 1.1293 Accuracy 0.4493\n",
            "Epoch 5 Batch 4250 Loss 1.1278 Accuracy 0.4497\n",
            "Epoch 5 Batch 4300 Loss 1.1262 Accuracy 0.4500\n",
            "Epoch 5 Batch 4350 Loss 1.1249 Accuracy 0.4503\n",
            "Epoch 5 Batch 4400 Loss 1.1235 Accuracy 0.4505\n",
            "Epoch 5 Batch 4450 Loss 1.1222 Accuracy 0.4508\n",
            "Epoch 5 Batch 4500 Loss 1.1208 Accuracy 0.4511\n",
            "Epoch 5 Batch 4550 Loss 1.1191 Accuracy 0.4513\n",
            "Epoch 5 Batch 4600 Loss 1.1175 Accuracy 0.4516\n",
            "Epoch 5 Batch 4650 Loss 1.1162 Accuracy 0.4518\n",
            "Epoch 5 Batch 4700 Loss 1.1153 Accuracy 0.4520\n",
            "Epoch 5 Batch 4750 Loss 1.1146 Accuracy 0.4521\n",
            "Epoch 5 Batch 4800 Loss 1.1144 Accuracy 0.4522\n",
            "Epoch 5 Batch 4850 Loss 1.1142 Accuracy 0.4522\n",
            "Epoch 5 Batch 4900 Loss 1.1144 Accuracy 0.4522\n",
            "Epoch 5 Batch 4950 Loss 1.1146 Accuracy 0.4522\n",
            "Epoch 5 Batch 5000 Loss 1.1151 Accuracy 0.4521\n",
            "Epoch 5 Batch 5050 Loss 1.1154 Accuracy 0.4521\n",
            "Epoch 5 Batch 5100 Loss 1.1156 Accuracy 0.4520\n",
            "Epoch 5 Batch 5150 Loss 1.1163 Accuracy 0.4519\n",
            "Epoch 5 Batch 5200 Loss 1.1170 Accuracy 0.4519\n",
            "Epoch 5 Batch 5250 Loss 1.1176 Accuracy 0.4518\n",
            "Epoch 5 Batch 5300 Loss 1.1182 Accuracy 0.4517\n",
            "Epoch 5 Batch 5350 Loss 1.1189 Accuracy 0.4516\n",
            "Epoch 5 Batch 5400 Loss 1.1198 Accuracy 0.4516\n",
            "Epoch 5 Batch 5450 Loss 1.1204 Accuracy 0.4515\n",
            "Epoch 5 Batch 5500 Loss 1.1211 Accuracy 0.4514\n",
            "Epoch 5 Batch 5550 Loss 1.1219 Accuracy 0.4513\n",
            "Epoch 5 Batch 5600 Loss 1.1227 Accuracy 0.4513\n",
            "Epoch 5 Batch 5650 Loss 1.1231 Accuracy 0.4512\n",
            "Epoch 5 Batch 5700 Loss 1.1238 Accuracy 0.4511\n",
            "Epoch 5 Batch 5750 Loss 1.1246 Accuracy 0.4510\n",
            "Epoch 5 Batch 5800 Loss 1.1255 Accuracy 0.4509\n",
            "Epoch 5 Batch 5850 Loss 1.1262 Accuracy 0.4508\n",
            "Epoch 5 Batch 5900 Loss 1.1271 Accuracy 0.4507\n",
            "Epoch 5 Batch 5950 Loss 1.1280 Accuracy 0.4505\n",
            "Epoch 5 Batch 6000 Loss 1.1288 Accuracy 0.4503\n",
            "Epoch 5 Batch 6050 Loss 1.1296 Accuracy 0.4502\n",
            "Epoch 5 Batch 6100 Loss 1.1306 Accuracy 0.4500\n",
            "Epoch 5 Batch 6150 Loss 1.1317 Accuracy 0.4497\n",
            "Epoch 5 Batch 6200 Loss 1.1327 Accuracy 0.4496\n",
            "Epoch 5 Batch 6250 Loss 1.1336 Accuracy 0.4494\n",
            "Epoch 5 Batch 6300 Loss 1.1344 Accuracy 0.4492\n",
            "Epoch 5 Batch 6350 Loss 1.1353 Accuracy 0.4491\n",
            "Epoch 5 Batch 6400 Loss 1.1364 Accuracy 0.4489\n",
            "Epoch 5 Batch 6450 Loss 1.1374 Accuracy 0.4488\n",
            "Epoch 5 Batch 6500 Loss 1.1382 Accuracy 0.4486\n",
            "Epoch 5 Batch 6550 Loss 1.1388 Accuracy 0.4485\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-15\n",
            "Time taken for 1 epoch: 1688.4510469436646 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.2018 Accuracy 0.4359\n",
            "Epoch 6 Batch 50 Loss 1.2951 Accuracy 0.4277\n",
            "Epoch 6 Batch 100 Loss 1.2863 Accuracy 0.4311\n",
            "Epoch 6 Batch 150 Loss 1.2789 Accuracy 0.4335\n",
            "Epoch 6 Batch 200 Loss 1.2734 Accuracy 0.4338\n",
            "Epoch 6 Batch 250 Loss 1.2701 Accuracy 0.4350\n",
            "Epoch 6 Batch 300 Loss 1.2639 Accuracy 0.4354\n",
            "Epoch 6 Batch 350 Loss 1.2560 Accuracy 0.4364\n",
            "Epoch 6 Batch 400 Loss 1.2518 Accuracy 0.4370\n",
            "Epoch 6 Batch 450 Loss 1.2503 Accuracy 0.4374\n",
            "Epoch 6 Batch 500 Loss 1.2488 Accuracy 0.4375\n",
            "Epoch 6 Batch 550 Loss 1.2486 Accuracy 0.4374\n",
            "Epoch 6 Batch 600 Loss 1.2494 Accuracy 0.4373\n",
            "Epoch 6 Batch 650 Loss 1.2493 Accuracy 0.4377\n",
            "Epoch 6 Batch 700 Loss 1.2463 Accuracy 0.4375\n",
            "Epoch 6 Batch 750 Loss 1.2461 Accuracy 0.4374\n",
            "Epoch 6 Batch 800 Loss 1.2428 Accuracy 0.4375\n",
            "Epoch 6 Batch 850 Loss 1.2430 Accuracy 0.4371\n",
            "Epoch 6 Batch 900 Loss 1.2436 Accuracy 0.4370\n",
            "Epoch 6 Batch 950 Loss 1.2422 Accuracy 0.4369\n",
            "Epoch 6 Batch 1000 Loss 1.2418 Accuracy 0.4365\n",
            "Epoch 6 Batch 1050 Loss 1.2422 Accuracy 0.4365\n",
            "Epoch 6 Batch 1100 Loss 1.2428 Accuracy 0.4365\n",
            "Epoch 6 Batch 1150 Loss 1.2427 Accuracy 0.4362\n",
            "Epoch 6 Batch 1200 Loss 1.2419 Accuracy 0.4361\n",
            "Epoch 6 Batch 1250 Loss 1.2422 Accuracy 0.4358\n",
            "Epoch 6 Batch 1300 Loss 1.2423 Accuracy 0.4355\n",
            "Epoch 6 Batch 1350 Loss 1.2428 Accuracy 0.4354\n",
            "Epoch 6 Batch 1400 Loss 1.2423 Accuracy 0.4351\n",
            "Epoch 6 Batch 1450 Loss 1.2425 Accuracy 0.4347\n",
            "Epoch 6 Batch 1500 Loss 1.2429 Accuracy 0.4346\n",
            "Epoch 6 Batch 1550 Loss 1.2430 Accuracy 0.4345\n",
            "Epoch 6 Batch 1600 Loss 1.2427 Accuracy 0.4344\n",
            "Epoch 6 Batch 1650 Loss 1.2418 Accuracy 0.4346\n",
            "Epoch 6 Batch 1700 Loss 1.2409 Accuracy 0.4348\n",
            "Epoch 6 Batch 1750 Loss 1.2397 Accuracy 0.4351\n",
            "Epoch 6 Batch 1800 Loss 1.2382 Accuracy 0.4355\n",
            "Epoch 6 Batch 1850 Loss 1.2373 Accuracy 0.4357\n",
            "Epoch 6 Batch 1900 Loss 1.2353 Accuracy 0.4358\n",
            "Epoch 6 Batch 1950 Loss 1.2337 Accuracy 0.4361\n",
            "Epoch 6 Batch 2000 Loss 1.2327 Accuracy 0.4361\n",
            "Epoch 6 Batch 2050 Loss 1.2317 Accuracy 0.4364\n",
            "Epoch 6 Batch 2100 Loss 1.2307 Accuracy 0.4366\n",
            "Epoch 6 Batch 2150 Loss 1.2294 Accuracy 0.4368\n",
            "Epoch 6 Batch 2200 Loss 1.2280 Accuracy 0.4370\n",
            "Epoch 6 Batch 2250 Loss 1.2263 Accuracy 0.4372\n",
            "Epoch 6 Batch 2300 Loss 1.2234 Accuracy 0.4375\n",
            "Epoch 6 Batch 2350 Loss 1.2217 Accuracy 0.4376\n",
            "Epoch 6 Batch 2400 Loss 1.2196 Accuracy 0.4379\n",
            "Epoch 6 Batch 2450 Loss 1.2165 Accuracy 0.4381\n",
            "Epoch 6 Batch 2500 Loss 1.2138 Accuracy 0.4382\n",
            "Epoch 6 Batch 2550 Loss 1.2109 Accuracy 0.4385\n",
            "Epoch 6 Batch 2600 Loss 1.2076 Accuracy 0.4388\n",
            "Epoch 6 Batch 2650 Loss 1.2044 Accuracy 0.4390\n",
            "Epoch 6 Batch 2700 Loss 1.2008 Accuracy 0.4394\n",
            "Epoch 6 Batch 2750 Loss 1.1973 Accuracy 0.4399\n",
            "Epoch 6 Batch 2800 Loss 1.1942 Accuracy 0.4402\n",
            "Epoch 6 Batch 2850 Loss 1.1911 Accuracy 0.4406\n",
            "Epoch 6 Batch 2900 Loss 1.1881 Accuracy 0.4411\n",
            "Epoch 6 Batch 2950 Loss 1.1847 Accuracy 0.4415\n",
            "Epoch 6 Batch 3000 Loss 1.1811 Accuracy 0.4421\n",
            "Epoch 6 Batch 3050 Loss 1.1775 Accuracy 0.4425\n",
            "Epoch 6 Batch 3100 Loss 1.1735 Accuracy 0.4430\n",
            "Epoch 6 Batch 3150 Loss 1.1700 Accuracy 0.4436\n",
            "Epoch 6 Batch 3200 Loss 1.1669 Accuracy 0.4441\n",
            "Epoch 6 Batch 3250 Loss 1.1637 Accuracy 0.4445\n",
            "Epoch 6 Batch 3300 Loss 1.1611 Accuracy 0.4450\n",
            "Epoch 6 Batch 3350 Loss 1.1585 Accuracy 0.4454\n",
            "Epoch 6 Batch 3400 Loss 1.1561 Accuracy 0.4457\n",
            "Epoch 6 Batch 3450 Loss 1.1533 Accuracy 0.4461\n",
            "Epoch 6 Batch 3500 Loss 1.1510 Accuracy 0.4465\n",
            "Epoch 6 Batch 3550 Loss 1.1485 Accuracy 0.4468\n",
            "Epoch 6 Batch 3600 Loss 1.1467 Accuracy 0.4471\n",
            "Epoch 6 Batch 3650 Loss 1.1444 Accuracy 0.4474\n",
            "Epoch 6 Batch 3700 Loss 1.1426 Accuracy 0.4476\n",
            "Epoch 6 Batch 3750 Loss 1.1405 Accuracy 0.4479\n",
            "Epoch 6 Batch 3800 Loss 1.1383 Accuracy 0.4482\n",
            "Epoch 6 Batch 3850 Loss 1.1360 Accuracy 0.4485\n",
            "Epoch 6 Batch 3900 Loss 1.1339 Accuracy 0.4488\n",
            "Epoch 6 Batch 3950 Loss 1.1318 Accuracy 0.4491\n",
            "Epoch 6 Batch 4000 Loss 1.1297 Accuracy 0.4494\n",
            "Epoch 6 Batch 4050 Loss 1.1280 Accuracy 0.4497\n",
            "Epoch 6 Batch 4100 Loss 1.1262 Accuracy 0.4499\n",
            "Epoch 6 Batch 4150 Loss 1.1242 Accuracy 0.4502\n",
            "Epoch 6 Batch 4200 Loss 1.1224 Accuracy 0.4505\n",
            "Epoch 6 Batch 4250 Loss 1.1209 Accuracy 0.4508\n",
            "Epoch 6 Batch 4300 Loss 1.1195 Accuracy 0.4511\n",
            "Epoch 6 Batch 4350 Loss 1.1181 Accuracy 0.4514\n",
            "Epoch 6 Batch 4400 Loss 1.1165 Accuracy 0.4516\n",
            "Epoch 6 Batch 4450 Loss 1.1148 Accuracy 0.4519\n",
            "Epoch 6 Batch 4500 Loss 1.1133 Accuracy 0.4521\n",
            "Epoch 6 Batch 4550 Loss 1.1122 Accuracy 0.4524\n",
            "Epoch 6 Batch 4600 Loss 1.1109 Accuracy 0.4527\n",
            "Epoch 6 Batch 4650 Loss 1.1098 Accuracy 0.4529\n",
            "Epoch 6 Batch 4700 Loss 1.1088 Accuracy 0.4531\n",
            "Epoch 6 Batch 4750 Loss 1.1082 Accuracy 0.4532\n",
            "Epoch 6 Batch 4800 Loss 1.1082 Accuracy 0.4532\n",
            "Epoch 6 Batch 4850 Loss 1.1082 Accuracy 0.4533\n",
            "Epoch 6 Batch 4900 Loss 1.1081 Accuracy 0.4532\n",
            "Epoch 6 Batch 4950 Loss 1.1083 Accuracy 0.4532\n",
            "Epoch 6 Batch 5000 Loss 1.1086 Accuracy 0.4531\n",
            "Epoch 6 Batch 5050 Loss 1.1091 Accuracy 0.4531\n",
            "Epoch 6 Batch 5100 Loss 1.1093 Accuracy 0.4530\n",
            "Epoch 6 Batch 5150 Loss 1.1096 Accuracy 0.4530\n",
            "Epoch 6 Batch 5200 Loss 1.1103 Accuracy 0.4529\n",
            "Epoch 6 Batch 5250 Loss 1.1107 Accuracy 0.4529\n",
            "Epoch 6 Batch 5300 Loss 1.1113 Accuracy 0.4528\n",
            "Epoch 6 Batch 5350 Loss 1.1118 Accuracy 0.4527\n",
            "Epoch 6 Batch 5400 Loss 1.1125 Accuracy 0.4527\n",
            "Epoch 6 Batch 5450 Loss 1.1134 Accuracy 0.4526\n",
            "Epoch 6 Batch 5500 Loss 1.1139 Accuracy 0.4526\n",
            "Epoch 6 Batch 5550 Loss 1.1144 Accuracy 0.4526\n",
            "Epoch 6 Batch 5600 Loss 1.1151 Accuracy 0.4525\n",
            "Epoch 6 Batch 5650 Loss 1.1159 Accuracy 0.4524\n",
            "Epoch 6 Batch 5700 Loss 1.1166 Accuracy 0.4522\n",
            "Epoch 6 Batch 5750 Loss 1.1171 Accuracy 0.4521\n",
            "Epoch 6 Batch 5800 Loss 1.1179 Accuracy 0.4520\n",
            "Epoch 6 Batch 5850 Loss 1.1186 Accuracy 0.4519\n",
            "Epoch 6 Batch 5900 Loss 1.1196 Accuracy 0.4518\n",
            "Epoch 6 Batch 5950 Loss 1.1207 Accuracy 0.4517\n",
            "Epoch 6 Batch 6000 Loss 1.1217 Accuracy 0.4516\n",
            "Epoch 6 Batch 6050 Loss 1.1227 Accuracy 0.4514\n",
            "Epoch 6 Batch 6100 Loss 1.1239 Accuracy 0.4512\n",
            "Epoch 6 Batch 6150 Loss 1.1250 Accuracy 0.4510\n",
            "Epoch 6 Batch 6200 Loss 1.1259 Accuracy 0.4508\n",
            "Epoch 6 Batch 6250 Loss 1.1267 Accuracy 0.4507\n",
            "Epoch 6 Batch 6300 Loss 1.1278 Accuracy 0.4505\n",
            "Epoch 6 Batch 6350 Loss 1.1285 Accuracy 0.4503\n",
            "Epoch 6 Batch 6400 Loss 1.1293 Accuracy 0.4502\n",
            "Epoch 6 Batch 6450 Loss 1.1302 Accuracy 0.4500\n",
            "Epoch 6 Batch 6500 Loss 1.1310 Accuracy 0.4498\n",
            "Epoch 6 Batch 6550 Loss 1.1315 Accuracy 0.4497\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-16\n",
            "Time taken for 1 epoch: 1685.3779783248901 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.2492 Accuracy 0.4548\n",
            "Epoch 7 Batch 50 Loss 1.2895 Accuracy 0.4357\n",
            "Epoch 7 Batch 100 Loss 1.2764 Accuracy 0.4340\n",
            "Epoch 7 Batch 150 Loss 1.2699 Accuracy 0.4338\n",
            "Epoch 7 Batch 200 Loss 1.2601 Accuracy 0.4358\n",
            "Epoch 7 Batch 250 Loss 1.2594 Accuracy 0.4357\n",
            "Epoch 7 Batch 300 Loss 1.2521 Accuracy 0.4366\n",
            "Epoch 7 Batch 350 Loss 1.2524 Accuracy 0.4363\n",
            "Epoch 7 Batch 400 Loss 1.2475 Accuracy 0.4372\n",
            "Epoch 7 Batch 450 Loss 1.2455 Accuracy 0.4375\n",
            "Epoch 7 Batch 500 Loss 1.2396 Accuracy 0.4380\n",
            "Epoch 7 Batch 550 Loss 1.2369 Accuracy 0.4382\n",
            "Epoch 7 Batch 600 Loss 1.2379 Accuracy 0.4381\n",
            "Epoch 7 Batch 650 Loss 1.2386 Accuracy 0.4382\n",
            "Epoch 7 Batch 700 Loss 1.2376 Accuracy 0.4381\n",
            "Epoch 7 Batch 750 Loss 1.2378 Accuracy 0.4379\n",
            "Epoch 7 Batch 800 Loss 1.2373 Accuracy 0.4380\n",
            "Epoch 7 Batch 850 Loss 1.2376 Accuracy 0.4380\n",
            "Epoch 7 Batch 900 Loss 1.2363 Accuracy 0.4379\n",
            "Epoch 7 Batch 950 Loss 1.2355 Accuracy 0.4379\n",
            "Epoch 7 Batch 1000 Loss 1.2367 Accuracy 0.4378\n",
            "Epoch 7 Batch 1050 Loss 1.2376 Accuracy 0.4376\n",
            "Epoch 7 Batch 1100 Loss 1.2376 Accuracy 0.4374\n",
            "Epoch 7 Batch 1150 Loss 1.2373 Accuracy 0.4372\n",
            "Epoch 7 Batch 1200 Loss 1.2357 Accuracy 0.4372\n",
            "Epoch 7 Batch 1250 Loss 1.2356 Accuracy 0.4368\n",
            "Epoch 7 Batch 1300 Loss 1.2360 Accuracy 0.4367\n",
            "Epoch 7 Batch 1350 Loss 1.2364 Accuracy 0.4364\n",
            "Epoch 7 Batch 1400 Loss 1.2369 Accuracy 0.4362\n",
            "Epoch 7 Batch 1450 Loss 1.2363 Accuracy 0.4360\n",
            "Epoch 7 Batch 1500 Loss 1.2366 Accuracy 0.4358\n",
            "Epoch 7 Batch 1550 Loss 1.2367 Accuracy 0.4357\n",
            "Epoch 7 Batch 1600 Loss 1.2364 Accuracy 0.4358\n",
            "Epoch 7 Batch 1650 Loss 1.2346 Accuracy 0.4359\n",
            "Epoch 7 Batch 1700 Loss 1.2338 Accuracy 0.4360\n",
            "Epoch 7 Batch 1750 Loss 1.2329 Accuracy 0.4362\n",
            "Epoch 7 Batch 1800 Loss 1.2318 Accuracy 0.4364\n",
            "Epoch 7 Batch 1850 Loss 1.2302 Accuracy 0.4366\n",
            "Epoch 7 Batch 1900 Loss 1.2295 Accuracy 0.4369\n",
            "Epoch 7 Batch 1950 Loss 1.2287 Accuracy 0.4371\n",
            "Epoch 7 Batch 2000 Loss 1.2269 Accuracy 0.4373\n",
            "Epoch 7 Batch 2050 Loss 1.2257 Accuracy 0.4374\n",
            "Epoch 7 Batch 2100 Loss 1.2245 Accuracy 0.4375\n",
            "Epoch 7 Batch 2150 Loss 1.2231 Accuracy 0.4377\n",
            "Epoch 7 Batch 2200 Loss 1.2214 Accuracy 0.4380\n",
            "Epoch 7 Batch 2250 Loss 1.2192 Accuracy 0.4383\n",
            "Epoch 7 Batch 2300 Loss 1.2172 Accuracy 0.4385\n",
            "Epoch 7 Batch 2350 Loss 1.2148 Accuracy 0.4387\n",
            "Epoch 7 Batch 2400 Loss 1.2120 Accuracy 0.4389\n",
            "Epoch 7 Batch 2450 Loss 1.2091 Accuracy 0.4392\n",
            "Epoch 7 Batch 2500 Loss 1.2061 Accuracy 0.4395\n",
            "Epoch 7 Batch 2550 Loss 1.2036 Accuracy 0.4396\n",
            "Epoch 7 Batch 2600 Loss 1.2011 Accuracy 0.4398\n",
            "Epoch 7 Batch 2650 Loss 1.1979 Accuracy 0.4401\n",
            "Epoch 7 Batch 2700 Loss 1.1945 Accuracy 0.4405\n",
            "Epoch 7 Batch 2750 Loss 1.1914 Accuracy 0.4409\n",
            "Epoch 7 Batch 2800 Loss 1.1880 Accuracy 0.4413\n",
            "Epoch 7 Batch 2850 Loss 1.1852 Accuracy 0.4415\n",
            "Epoch 7 Batch 2900 Loss 1.1818 Accuracy 0.4419\n",
            "Epoch 7 Batch 2950 Loss 1.1780 Accuracy 0.4425\n",
            "Epoch 7 Batch 3000 Loss 1.1741 Accuracy 0.4430\n",
            "Epoch 7 Batch 3050 Loss 1.1703 Accuracy 0.4435\n",
            "Epoch 7 Batch 3100 Loss 1.1665 Accuracy 0.4440\n",
            "Epoch 7 Batch 3150 Loss 1.1628 Accuracy 0.4446\n",
            "Epoch 7 Batch 3200 Loss 1.1597 Accuracy 0.4451\n",
            "Epoch 7 Batch 3250 Loss 1.1569 Accuracy 0.4457\n",
            "Epoch 7 Batch 3300 Loss 1.1542 Accuracy 0.4461\n",
            "Epoch 7 Batch 3350 Loss 1.1512 Accuracy 0.4465\n",
            "Epoch 7 Batch 3400 Loss 1.1482 Accuracy 0.4468\n",
            "Epoch 7 Batch 3450 Loss 1.1458 Accuracy 0.4471\n",
            "Epoch 7 Batch 3500 Loss 1.1435 Accuracy 0.4476\n",
            "Epoch 7 Batch 3550 Loss 1.1411 Accuracy 0.4479\n",
            "Epoch 7 Batch 3600 Loss 1.1393 Accuracy 0.4483\n",
            "Epoch 7 Batch 3650 Loss 1.1373 Accuracy 0.4485\n",
            "Epoch 7 Batch 3700 Loss 1.1351 Accuracy 0.4487\n",
            "Epoch 7 Batch 3750 Loss 1.1328 Accuracy 0.4490\n",
            "Epoch 7 Batch 3800 Loss 1.1307 Accuracy 0.4493\n",
            "Epoch 7 Batch 3850 Loss 1.1286 Accuracy 0.4496\n",
            "Epoch 7 Batch 3900 Loss 1.1267 Accuracy 0.4499\n",
            "Epoch 7 Batch 3950 Loss 1.1250 Accuracy 0.4501\n",
            "Epoch 7 Batch 4000 Loss 1.1233 Accuracy 0.4504\n",
            "Epoch 7 Batch 4050 Loss 1.1217 Accuracy 0.4507\n",
            "Epoch 7 Batch 4100 Loss 1.1199 Accuracy 0.4509\n",
            "Epoch 7 Batch 4150 Loss 1.1181 Accuracy 0.4513\n",
            "Epoch 7 Batch 4200 Loss 1.1163 Accuracy 0.4516\n",
            "Epoch 7 Batch 4250 Loss 1.1147 Accuracy 0.4519\n",
            "Epoch 7 Batch 4300 Loss 1.1130 Accuracy 0.4521\n",
            "Epoch 7 Batch 4350 Loss 1.1113 Accuracy 0.4524\n",
            "Epoch 7 Batch 4400 Loss 1.1100 Accuracy 0.4527\n",
            "Epoch 7 Batch 4450 Loss 1.1086 Accuracy 0.4529\n",
            "Epoch 7 Batch 4500 Loss 1.1073 Accuracy 0.4532\n",
            "Epoch 7 Batch 4550 Loss 1.1058 Accuracy 0.4535\n",
            "Epoch 7 Batch 4600 Loss 1.1046 Accuracy 0.4538\n",
            "Epoch 7 Batch 4650 Loss 1.1030 Accuracy 0.4540\n",
            "Epoch 7 Batch 4700 Loss 1.1020 Accuracy 0.4542\n",
            "Epoch 7 Batch 4750 Loss 1.1013 Accuracy 0.4543\n",
            "Epoch 7 Batch 4800 Loss 1.1010 Accuracy 0.4543\n",
            "Epoch 7 Batch 4850 Loss 1.1010 Accuracy 0.4543\n",
            "Epoch 7 Batch 4900 Loss 1.1011 Accuracy 0.4543\n",
            "Epoch 7 Batch 4950 Loss 1.1016 Accuracy 0.4543\n",
            "Epoch 7 Batch 5000 Loss 1.1019 Accuracy 0.4543\n",
            "Epoch 7 Batch 5050 Loss 1.1023 Accuracy 0.4542\n",
            "Epoch 7 Batch 5100 Loss 1.1029 Accuracy 0.4542\n",
            "Epoch 7 Batch 5150 Loss 1.1034 Accuracy 0.4541\n",
            "Epoch 7 Batch 5200 Loss 1.1038 Accuracy 0.4540\n",
            "Epoch 7 Batch 5250 Loss 1.1043 Accuracy 0.4539\n",
            "Epoch 7 Batch 5300 Loss 1.1050 Accuracy 0.4539\n",
            "Epoch 7 Batch 5350 Loss 1.1053 Accuracy 0.4538\n",
            "Epoch 7 Batch 5400 Loss 1.1060 Accuracy 0.4538\n",
            "Epoch 7 Batch 5450 Loss 1.1066 Accuracy 0.4537\n",
            "Epoch 7 Batch 5500 Loss 1.1073 Accuracy 0.4536\n",
            "Epoch 7 Batch 5550 Loss 1.1079 Accuracy 0.4535\n",
            "Epoch 7 Batch 5600 Loss 1.1086 Accuracy 0.4535\n",
            "Epoch 7 Batch 5650 Loss 1.1093 Accuracy 0.4534\n",
            "Epoch 7 Batch 5700 Loss 1.1101 Accuracy 0.4533\n",
            "Epoch 7 Batch 5750 Loss 1.1108 Accuracy 0.4532\n",
            "Epoch 7 Batch 5800 Loss 1.1113 Accuracy 0.4531\n",
            "Epoch 7 Batch 5850 Loss 1.1121 Accuracy 0.4530\n",
            "Epoch 7 Batch 5900 Loss 1.1130 Accuracy 0.4529\n",
            "Epoch 7 Batch 5950 Loss 1.1140 Accuracy 0.4528\n",
            "Epoch 7 Batch 6000 Loss 1.1148 Accuracy 0.4526\n",
            "Epoch 7 Batch 6050 Loss 1.1160 Accuracy 0.4525\n",
            "Epoch 7 Batch 6100 Loss 1.1170 Accuracy 0.4523\n",
            "Epoch 7 Batch 6150 Loss 1.1180 Accuracy 0.4521\n",
            "Epoch 7 Batch 6200 Loss 1.1191 Accuracy 0.4519\n",
            "Epoch 7 Batch 6250 Loss 1.1199 Accuracy 0.4517\n",
            "Epoch 7 Batch 6300 Loss 1.1206 Accuracy 0.4516\n",
            "Epoch 7 Batch 6350 Loss 1.1216 Accuracy 0.4514\n",
            "Epoch 7 Batch 6400 Loss 1.1224 Accuracy 0.4513\n",
            "Epoch 7 Batch 6450 Loss 1.1232 Accuracy 0.4511\n",
            "Epoch 7 Batch 6500 Loss 1.1239 Accuracy 0.4510\n",
            "Epoch 7 Batch 6550 Loss 1.1247 Accuracy 0.4508\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-17\n",
            "Time taken for 1 epoch: 1674.3453769683838 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.1221 Accuracy 0.4465\n",
            "Epoch 8 Batch 50 Loss 1.2672 Accuracy 0.4360\n",
            "Epoch 8 Batch 100 Loss 1.2652 Accuracy 0.4350\n",
            "Epoch 8 Batch 150 Loss 1.2646 Accuracy 0.4347\n",
            "Epoch 8 Batch 200 Loss 1.2621 Accuracy 0.4367\n",
            "Epoch 8 Batch 250 Loss 1.2550 Accuracy 0.4365\n",
            "Epoch 8 Batch 300 Loss 1.2529 Accuracy 0.4367\n",
            "Epoch 8 Batch 350 Loss 1.2469 Accuracy 0.4377\n",
            "Epoch 8 Batch 400 Loss 1.2394 Accuracy 0.4382\n",
            "Epoch 8 Batch 450 Loss 1.2388 Accuracy 0.4387\n",
            "Epoch 8 Batch 500 Loss 1.2358 Accuracy 0.4396\n",
            "Epoch 8 Batch 550 Loss 1.2344 Accuracy 0.4392\n",
            "Epoch 8 Batch 600 Loss 1.2341 Accuracy 0.4391\n",
            "Epoch 8 Batch 650 Loss 1.2321 Accuracy 0.4393\n",
            "Epoch 8 Batch 700 Loss 1.2319 Accuracy 0.4394\n",
            "Epoch 8 Batch 750 Loss 1.2313 Accuracy 0.4390\n",
            "Epoch 8 Batch 800 Loss 1.2320 Accuracy 0.4393\n",
            "Epoch 8 Batch 850 Loss 1.2328 Accuracy 0.4397\n",
            "Epoch 8 Batch 900 Loss 1.2310 Accuracy 0.4397\n",
            "Epoch 8 Batch 950 Loss 1.2312 Accuracy 0.4396\n",
            "Epoch 8 Batch 1000 Loss 1.2311 Accuracy 0.4393\n",
            "Epoch 8 Batch 1050 Loss 1.2308 Accuracy 0.4389\n",
            "Epoch 8 Batch 1100 Loss 1.2299 Accuracy 0.4386\n",
            "Epoch 8 Batch 1150 Loss 1.2297 Accuracy 0.4380\n",
            "Epoch 8 Batch 1200 Loss 1.2295 Accuracy 0.4379\n",
            "Epoch 8 Batch 1250 Loss 1.2286 Accuracy 0.4377\n",
            "Epoch 8 Batch 1300 Loss 1.2287 Accuracy 0.4375\n",
            "Epoch 8 Batch 1350 Loss 1.2290 Accuracy 0.4373\n",
            "Epoch 8 Batch 1400 Loss 1.2292 Accuracy 0.4370\n",
            "Epoch 8 Batch 1450 Loss 1.2289 Accuracy 0.4368\n",
            "Epoch 8 Batch 1500 Loss 1.2286 Accuracy 0.4367\n",
            "Epoch 8 Batch 1550 Loss 1.2284 Accuracy 0.4367\n",
            "Epoch 8 Batch 1600 Loss 1.2276 Accuracy 0.4366\n",
            "Epoch 8 Batch 1650 Loss 1.2262 Accuracy 0.4367\n",
            "Epoch 8 Batch 1700 Loss 1.2250 Accuracy 0.4370\n",
            "Epoch 8 Batch 1750 Loss 1.2240 Accuracy 0.4372\n",
            "Epoch 8 Batch 1800 Loss 1.2231 Accuracy 0.4375\n",
            "Epoch 8 Batch 1850 Loss 1.2219 Accuracy 0.4377\n",
            "Epoch 8 Batch 1900 Loss 1.2205 Accuracy 0.4381\n",
            "Epoch 8 Batch 1950 Loss 1.2194 Accuracy 0.4384\n",
            "Epoch 8 Batch 2000 Loss 1.2182 Accuracy 0.4385\n",
            "Epoch 8 Batch 2050 Loss 1.2170 Accuracy 0.4385\n",
            "Epoch 8 Batch 2100 Loss 1.2164 Accuracy 0.4387\n",
            "Epoch 8 Batch 2150 Loss 1.2151 Accuracy 0.4391\n",
            "Epoch 8 Batch 2200 Loss 1.2135 Accuracy 0.4393\n",
            "Epoch 8 Batch 2250 Loss 1.2114 Accuracy 0.4395\n",
            "Epoch 8 Batch 2300 Loss 1.2096 Accuracy 0.4398\n",
            "Epoch 8 Batch 2350 Loss 1.2068 Accuracy 0.4399\n",
            "Epoch 8 Batch 2400 Loss 1.2042 Accuracy 0.4401\n",
            "Epoch 8 Batch 2450 Loss 1.2013 Accuracy 0.4403\n",
            "Epoch 8 Batch 2500 Loss 1.1986 Accuracy 0.4407\n",
            "Epoch 8 Batch 2550 Loss 1.1956 Accuracy 0.4408\n",
            "Epoch 8 Batch 2600 Loss 1.1923 Accuracy 0.4411\n",
            "Epoch 8 Batch 2650 Loss 1.1894 Accuracy 0.4414\n",
            "Epoch 8 Batch 2700 Loss 1.1865 Accuracy 0.4417\n",
            "Epoch 8 Batch 2750 Loss 1.1834 Accuracy 0.4421\n",
            "Epoch 8 Batch 2800 Loss 1.1798 Accuracy 0.4424\n",
            "Epoch 8 Batch 2850 Loss 1.1767 Accuracy 0.4428\n",
            "Epoch 8 Batch 2900 Loss 1.1734 Accuracy 0.4432\n",
            "Epoch 8 Batch 2950 Loss 1.1701 Accuracy 0.4437\n",
            "Epoch 8 Batch 3000 Loss 1.1667 Accuracy 0.4442\n",
            "Epoch 8 Batch 3050 Loss 1.1631 Accuracy 0.4446\n",
            "Epoch 8 Batch 3100 Loss 1.1599 Accuracy 0.4452\n",
            "Epoch 8 Batch 3150 Loss 1.1564 Accuracy 0.4457\n",
            "Epoch 8 Batch 3200 Loss 1.1531 Accuracy 0.4462\n",
            "Epoch 8 Batch 3250 Loss 1.1500 Accuracy 0.4467\n",
            "Epoch 8 Batch 3300 Loss 1.1471 Accuracy 0.4471\n",
            "Epoch 8 Batch 3350 Loss 1.1446 Accuracy 0.4474\n",
            "Epoch 8 Batch 3400 Loss 1.1418 Accuracy 0.4478\n",
            "Epoch 8 Batch 3450 Loss 1.1393 Accuracy 0.4482\n",
            "Epoch 8 Batch 3500 Loss 1.1370 Accuracy 0.4485\n",
            "Epoch 8 Batch 3550 Loss 1.1346 Accuracy 0.4488\n",
            "Epoch 8 Batch 3600 Loss 1.1326 Accuracy 0.4491\n",
            "Epoch 8 Batch 3650 Loss 1.1307 Accuracy 0.4494\n",
            "Epoch 8 Batch 3700 Loss 1.1282 Accuracy 0.4496\n",
            "Epoch 8 Batch 3750 Loss 1.1260 Accuracy 0.4499\n",
            "Epoch 8 Batch 3800 Loss 1.1239 Accuracy 0.4502\n",
            "Epoch 8 Batch 3850 Loss 1.1219 Accuracy 0.4505\n",
            "Epoch 8 Batch 3900 Loss 1.1199 Accuracy 0.4508\n",
            "Epoch 8 Batch 3950 Loss 1.1180 Accuracy 0.4510\n",
            "Epoch 8 Batch 4000 Loss 1.1160 Accuracy 0.4513\n",
            "Epoch 8 Batch 4050 Loss 1.1144 Accuracy 0.4516\n",
            "Epoch 8 Batch 4100 Loss 1.1126 Accuracy 0.4519\n",
            "Epoch 8 Batch 4150 Loss 1.1106 Accuracy 0.4523\n",
            "Epoch 8 Batch 4200 Loss 1.1088 Accuracy 0.4525\n",
            "Epoch 8 Batch 4250 Loss 1.1072 Accuracy 0.4528\n",
            "Epoch 8 Batch 4300 Loss 1.1059 Accuracy 0.4531\n",
            "Epoch 8 Batch 4350 Loss 1.1041 Accuracy 0.4534\n",
            "Epoch 8 Batch 4400 Loss 1.1026 Accuracy 0.4537\n",
            "Epoch 8 Batch 4450 Loss 1.1012 Accuracy 0.4539\n",
            "Epoch 8 Batch 4500 Loss 1.0999 Accuracy 0.4542\n",
            "Epoch 8 Batch 4550 Loss 1.0985 Accuracy 0.4544\n",
            "Epoch 8 Batch 4600 Loss 1.0973 Accuracy 0.4547\n",
            "Epoch 8 Batch 4650 Loss 1.0959 Accuracy 0.4549\n",
            "Epoch 8 Batch 4700 Loss 1.0950 Accuracy 0.4551\n",
            "Epoch 8 Batch 4750 Loss 1.0943 Accuracy 0.4552\n",
            "Epoch 8 Batch 4800 Loss 1.0939 Accuracy 0.4553\n",
            "Epoch 8 Batch 4850 Loss 1.0940 Accuracy 0.4553\n",
            "Epoch 8 Batch 4900 Loss 1.0941 Accuracy 0.4553\n",
            "Epoch 8 Batch 4950 Loss 1.0942 Accuracy 0.4554\n",
            "Epoch 8 Batch 5000 Loss 1.0946 Accuracy 0.4553\n",
            "Epoch 8 Batch 5050 Loss 1.0949 Accuracy 0.4552\n",
            "Epoch 8 Batch 5100 Loss 1.0954 Accuracy 0.4551\n",
            "Epoch 8 Batch 5150 Loss 1.0956 Accuracy 0.4550\n",
            "Epoch 8 Batch 5200 Loss 1.0963 Accuracy 0.4550\n",
            "Epoch 8 Batch 5250 Loss 1.0971 Accuracy 0.4549\n",
            "Epoch 8 Batch 5300 Loss 1.0973 Accuracy 0.4548\n",
            "Epoch 8 Batch 5350 Loss 1.0978 Accuracy 0.4547\n",
            "Epoch 8 Batch 5400 Loss 1.0984 Accuracy 0.4546\n",
            "Epoch 8 Batch 5450 Loss 1.0992 Accuracy 0.4545\n",
            "Epoch 8 Batch 5500 Loss 1.0998 Accuracy 0.4544\n",
            "Epoch 8 Batch 5550 Loss 1.1007 Accuracy 0.4544\n",
            "Epoch 8 Batch 5600 Loss 1.1012 Accuracy 0.4543\n",
            "Epoch 8 Batch 5650 Loss 1.1021 Accuracy 0.4543\n",
            "Epoch 8 Batch 5700 Loss 1.1028 Accuracy 0.4542\n",
            "Epoch 8 Batch 5750 Loss 1.1037 Accuracy 0.4541\n",
            "Epoch 8 Batch 5800 Loss 1.1043 Accuracy 0.4540\n",
            "Epoch 8 Batch 5850 Loss 1.1052 Accuracy 0.4539\n",
            "Epoch 8 Batch 5900 Loss 1.1063 Accuracy 0.4538\n",
            "Epoch 8 Batch 5950 Loss 1.1072 Accuracy 0.4537\n",
            "Epoch 8 Batch 6000 Loss 1.1084 Accuracy 0.4535\n",
            "Epoch 8 Batch 6050 Loss 1.1095 Accuracy 0.4534\n",
            "Epoch 8 Batch 6100 Loss 1.1104 Accuracy 0.4531\n",
            "Epoch 8 Batch 6150 Loss 1.1114 Accuracy 0.4529\n",
            "Epoch 8 Batch 6200 Loss 1.1124 Accuracy 0.4527\n",
            "Epoch 8 Batch 6250 Loss 1.1130 Accuracy 0.4526\n",
            "Epoch 8 Batch 6300 Loss 1.1137 Accuracy 0.4524\n",
            "Epoch 8 Batch 6350 Loss 1.1145 Accuracy 0.4523\n",
            "Epoch 8 Batch 6400 Loss 1.1155 Accuracy 0.4521\n",
            "Epoch 8 Batch 6450 Loss 1.1162 Accuracy 0.4520\n",
            "Epoch 8 Batch 6500 Loss 1.1171 Accuracy 0.4518\n",
            "Epoch 8 Batch 6550 Loss 1.1180 Accuracy 0.4516\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-18\n",
            "Time taken for 1 epoch: 1690.285896062851 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.4711 Accuracy 0.4120\n",
            "Epoch 9 Batch 50 Loss 1.2871 Accuracy 0.4330\n",
            "Epoch 9 Batch 100 Loss 1.2789 Accuracy 0.4350\n",
            "Epoch 9 Batch 150 Loss 1.2770 Accuracy 0.4366\n",
            "Epoch 9 Batch 200 Loss 1.2614 Accuracy 0.4380\n",
            "Epoch 9 Batch 250 Loss 1.2532 Accuracy 0.4394\n",
            "Epoch 9 Batch 300 Loss 1.2462 Accuracy 0.4392\n",
            "Epoch 9 Batch 350 Loss 1.2428 Accuracy 0.4397\n",
            "Epoch 9 Batch 400 Loss 1.2387 Accuracy 0.4392\n",
            "Epoch 9 Batch 450 Loss 1.2375 Accuracy 0.4391\n",
            "Epoch 9 Batch 500 Loss 1.2343 Accuracy 0.4396\n",
            "Epoch 9 Batch 550 Loss 1.2334 Accuracy 0.4395\n",
            "Epoch 9 Batch 600 Loss 1.2337 Accuracy 0.4392\n",
            "Epoch 9 Batch 650 Loss 1.2309 Accuracy 0.4392\n",
            "Epoch 9 Batch 700 Loss 1.2286 Accuracy 0.4393\n",
            "Epoch 9 Batch 750 Loss 1.2272 Accuracy 0.4394\n",
            "Epoch 9 Batch 800 Loss 1.2270 Accuracy 0.4394\n",
            "Epoch 9 Batch 850 Loss 1.2252 Accuracy 0.4398\n",
            "Epoch 9 Batch 900 Loss 1.2257 Accuracy 0.4397\n",
            "Epoch 9 Batch 950 Loss 1.2249 Accuracy 0.4397\n",
            "Epoch 9 Batch 1000 Loss 1.2238 Accuracy 0.4396\n",
            "Epoch 9 Batch 1050 Loss 1.2239 Accuracy 0.4394\n",
            "Epoch 9 Batch 1100 Loss 1.2230 Accuracy 0.4392\n",
            "Epoch 9 Batch 1150 Loss 1.2218 Accuracy 0.4391\n",
            "Epoch 9 Batch 1200 Loss 1.2212 Accuracy 0.4389\n",
            "Epoch 9 Batch 1250 Loss 1.2216 Accuracy 0.4387\n",
            "Epoch 9 Batch 1300 Loss 1.2218 Accuracy 0.4385\n",
            "Epoch 9 Batch 1350 Loss 1.2226 Accuracy 0.4384\n",
            "Epoch 9 Batch 1400 Loss 1.2234 Accuracy 0.4380\n",
            "Epoch 9 Batch 1450 Loss 1.2233 Accuracy 0.4377\n",
            "Epoch 9 Batch 1500 Loss 1.2238 Accuracy 0.4374\n",
            "Epoch 9 Batch 1550 Loss 1.2226 Accuracy 0.4374\n",
            "Epoch 9 Batch 1600 Loss 1.2220 Accuracy 0.4375\n",
            "Epoch 9 Batch 1650 Loss 1.2208 Accuracy 0.4376\n",
            "Epoch 9 Batch 1700 Loss 1.2201 Accuracy 0.4377\n",
            "Epoch 9 Batch 1750 Loss 1.2191 Accuracy 0.4379\n",
            "Epoch 9 Batch 1800 Loss 1.2176 Accuracy 0.4381\n",
            "Epoch 9 Batch 1850 Loss 1.2165 Accuracy 0.4384\n",
            "Epoch 9 Batch 1900 Loss 1.2155 Accuracy 0.4387\n",
            "Epoch 9 Batch 1950 Loss 1.2144 Accuracy 0.4389\n",
            "Epoch 9 Batch 2000 Loss 1.2133 Accuracy 0.4391\n",
            "Epoch 9 Batch 2050 Loss 1.2125 Accuracy 0.4392\n",
            "Epoch 9 Batch 2100 Loss 1.2110 Accuracy 0.4394\n",
            "Epoch 9 Batch 2150 Loss 1.2093 Accuracy 0.4397\n",
            "Epoch 9 Batch 2200 Loss 1.2075 Accuracy 0.4400\n",
            "Epoch 9 Batch 2250 Loss 1.2056 Accuracy 0.4403\n",
            "Epoch 9 Batch 2300 Loss 1.2035 Accuracy 0.4406\n",
            "Epoch 9 Batch 2350 Loss 1.2017 Accuracy 0.4408\n",
            "Epoch 9 Batch 2400 Loss 1.1989 Accuracy 0.4411\n",
            "Epoch 9 Batch 2450 Loss 1.1963 Accuracy 0.4414\n",
            "Epoch 9 Batch 2500 Loss 1.1935 Accuracy 0.4415\n",
            "Epoch 9 Batch 2550 Loss 1.1906 Accuracy 0.4417\n",
            "Epoch 9 Batch 2600 Loss 1.1871 Accuracy 0.4420\n",
            "Epoch 9 Batch 2650 Loss 1.1839 Accuracy 0.4423\n",
            "Epoch 9 Batch 2700 Loss 1.1807 Accuracy 0.4426\n",
            "Epoch 9 Batch 2750 Loss 1.1776 Accuracy 0.4428\n",
            "Epoch 9 Batch 2800 Loss 1.1742 Accuracy 0.4432\n",
            "Epoch 9 Batch 2850 Loss 1.1709 Accuracy 0.4434\n",
            "Epoch 9 Batch 2900 Loss 1.1674 Accuracy 0.4439\n",
            "Epoch 9 Batch 2950 Loss 1.1638 Accuracy 0.4444\n",
            "Epoch 9 Batch 3000 Loss 1.1603 Accuracy 0.4449\n",
            "Epoch 9 Batch 3050 Loss 1.1567 Accuracy 0.4454\n",
            "Epoch 9 Batch 3100 Loss 1.1534 Accuracy 0.4459\n",
            "Epoch 9 Batch 3150 Loss 1.1500 Accuracy 0.4465\n",
            "Epoch 9 Batch 3200 Loss 1.1465 Accuracy 0.4471\n",
            "Epoch 9 Batch 3250 Loss 1.1434 Accuracy 0.4476\n",
            "Epoch 9 Batch 3300 Loss 1.1404 Accuracy 0.4480\n",
            "Epoch 9 Batch 3350 Loss 1.1379 Accuracy 0.4485\n",
            "Epoch 9 Batch 3400 Loss 1.1354 Accuracy 0.4488\n",
            "Epoch 9 Batch 3450 Loss 1.1329 Accuracy 0.4491\n",
            "Epoch 9 Batch 3500 Loss 1.1305 Accuracy 0.4494\n",
            "Epoch 9 Batch 3550 Loss 1.1285 Accuracy 0.4497\n",
            "Epoch 9 Batch 3600 Loss 1.1263 Accuracy 0.4500\n",
            "Epoch 9 Batch 3650 Loss 1.1243 Accuracy 0.4502\n",
            "Epoch 9 Batch 3700 Loss 1.1218 Accuracy 0.4504\n",
            "Epoch 9 Batch 3750 Loss 1.1196 Accuracy 0.4507\n",
            "Epoch 9 Batch 3800 Loss 1.1175 Accuracy 0.4510\n",
            "Epoch 9 Batch 3850 Loss 1.1152 Accuracy 0.4513\n",
            "Epoch 9 Batch 3900 Loss 1.1132 Accuracy 0.4516\n",
            "Epoch 9 Batch 3950 Loss 1.1114 Accuracy 0.4519\n",
            "Epoch 9 Batch 4000 Loss 1.1094 Accuracy 0.4522\n",
            "Epoch 9 Batch 4050 Loss 1.1078 Accuracy 0.4525\n",
            "Epoch 9 Batch 4100 Loss 1.1063 Accuracy 0.4527\n",
            "Epoch 9 Batch 4150 Loss 1.1045 Accuracy 0.4530\n",
            "Epoch 9 Batch 4200 Loss 1.1027 Accuracy 0.4533\n",
            "Epoch 9 Batch 4250 Loss 1.1011 Accuracy 0.4536\n",
            "Epoch 9 Batch 4300 Loss 1.0994 Accuracy 0.4539\n",
            "Epoch 9 Batch 4350 Loss 1.0981 Accuracy 0.4542\n",
            "Epoch 9 Batch 4400 Loss 1.0965 Accuracy 0.4545\n",
            "Epoch 9 Batch 4450 Loss 1.0950 Accuracy 0.4548\n",
            "Epoch 9 Batch 4500 Loss 1.0936 Accuracy 0.4550\n",
            "Epoch 9 Batch 4550 Loss 1.0921 Accuracy 0.4553\n",
            "Epoch 9 Batch 4600 Loss 1.0908 Accuracy 0.4556\n",
            "Epoch 9 Batch 4650 Loss 1.0892 Accuracy 0.4559\n",
            "Epoch 9 Batch 4700 Loss 1.0880 Accuracy 0.4561\n",
            "Epoch 9 Batch 4750 Loss 1.0875 Accuracy 0.4562\n",
            "Epoch 9 Batch 4800 Loss 1.0873 Accuracy 0.4563\n",
            "Epoch 9 Batch 4850 Loss 1.0874 Accuracy 0.4563\n",
            "Epoch 9 Batch 4900 Loss 1.0874 Accuracy 0.4562\n",
            "Epoch 9 Batch 4950 Loss 1.0877 Accuracy 0.4562\n",
            "Epoch 9 Batch 5000 Loss 1.0881 Accuracy 0.4561\n",
            "Epoch 9 Batch 5050 Loss 1.0885 Accuracy 0.4560\n",
            "Epoch 9 Batch 5100 Loss 1.0892 Accuracy 0.4559\n",
            "Epoch 9 Batch 5150 Loss 1.0897 Accuracy 0.4559\n",
            "Epoch 9 Batch 5200 Loss 1.0904 Accuracy 0.4558\n",
            "Epoch 9 Batch 5250 Loss 1.0913 Accuracy 0.4558\n",
            "Epoch 9 Batch 5300 Loss 1.0922 Accuracy 0.4557\n",
            "Epoch 9 Batch 5350 Loss 1.0929 Accuracy 0.4556\n",
            "Epoch 9 Batch 5400 Loss 1.0936 Accuracy 0.4555\n",
            "Epoch 9 Batch 5450 Loss 1.0942 Accuracy 0.4555\n",
            "Epoch 9 Batch 5500 Loss 1.0948 Accuracy 0.4554\n",
            "Epoch 9 Batch 5550 Loss 1.0955 Accuracy 0.4554\n",
            "Epoch 9 Batch 5600 Loss 1.0961 Accuracy 0.4552\n",
            "Epoch 9 Batch 5650 Loss 1.0966 Accuracy 0.4552\n",
            "Epoch 9 Batch 5700 Loss 1.0975 Accuracy 0.4551\n",
            "Epoch 9 Batch 5750 Loss 1.0983 Accuracy 0.4550\n",
            "Epoch 9 Batch 5800 Loss 1.0990 Accuracy 0.4548\n",
            "Epoch 9 Batch 5850 Loss 1.0997 Accuracy 0.4547\n",
            "Epoch 9 Batch 5900 Loss 1.1003 Accuracy 0.4546\n",
            "Epoch 9 Batch 5950 Loss 1.1012 Accuracy 0.4545\n",
            "Epoch 9 Batch 6000 Loss 1.1023 Accuracy 0.4543\n",
            "Epoch 9 Batch 6050 Loss 1.1033 Accuracy 0.4542\n",
            "Epoch 9 Batch 6100 Loss 1.1042 Accuracy 0.4540\n",
            "Epoch 9 Batch 6150 Loss 1.1054 Accuracy 0.4538\n",
            "Epoch 9 Batch 6200 Loss 1.1064 Accuracy 0.4536\n",
            "Epoch 9 Batch 6250 Loss 1.1072 Accuracy 0.4534\n",
            "Epoch 9 Batch 6300 Loss 1.1078 Accuracy 0.4533\n",
            "Epoch 9 Batch 6350 Loss 1.1085 Accuracy 0.4531\n",
            "Epoch 9 Batch 6400 Loss 1.1094 Accuracy 0.4530\n",
            "Epoch 9 Batch 6450 Loss 1.1102 Accuracy 0.4528\n",
            "Epoch 9 Batch 6500 Loss 1.1112 Accuracy 0.4527\n",
            "Epoch 9 Batch 6550 Loss 1.1122 Accuracy 0.4526\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-19\n",
            "Time taken for 1 epoch: 1732.3513412475586 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.1493 Accuracy 0.4202\n",
            "Epoch 10 Batch 50 Loss 1.2617 Accuracy 0.4369\n",
            "Epoch 10 Batch 100 Loss 1.2594 Accuracy 0.4358\n",
            "Epoch 10 Batch 150 Loss 1.2494 Accuracy 0.4382\n",
            "Epoch 10 Batch 200 Loss 1.2417 Accuracy 0.4407\n",
            "Epoch 10 Batch 250 Loss 1.2356 Accuracy 0.4413\n",
            "Epoch 10 Batch 300 Loss 1.2335 Accuracy 0.4405\n",
            "Epoch 10 Batch 350 Loss 1.2293 Accuracy 0.4401\n",
            "Epoch 10 Batch 400 Loss 1.2261 Accuracy 0.4402\n",
            "Epoch 10 Batch 450 Loss 1.2252 Accuracy 0.4408\n",
            "Epoch 10 Batch 500 Loss 1.2198 Accuracy 0.4413\n",
            "Epoch 10 Batch 550 Loss 1.2184 Accuracy 0.4417\n",
            "Epoch 10 Batch 600 Loss 1.2169 Accuracy 0.4420\n",
            "Epoch 10 Batch 650 Loss 1.2167 Accuracy 0.4418\n",
            "Epoch 10 Batch 700 Loss 1.2166 Accuracy 0.4415\n",
            "Epoch 10 Batch 750 Loss 1.2159 Accuracy 0.4413\n",
            "Epoch 10 Batch 800 Loss 1.2162 Accuracy 0.4413\n",
            "Epoch 10 Batch 850 Loss 1.2158 Accuracy 0.4413\n",
            "Epoch 10 Batch 900 Loss 1.2152 Accuracy 0.4413\n",
            "Epoch 10 Batch 950 Loss 1.2170 Accuracy 0.4412\n",
            "Epoch 10 Batch 1000 Loss 1.2167 Accuracy 0.4409\n",
            "Epoch 10 Batch 1050 Loss 1.2172 Accuracy 0.4406\n",
            "Epoch 10 Batch 1100 Loss 1.2170 Accuracy 0.4403\n",
            "Epoch 10 Batch 1150 Loss 1.2169 Accuracy 0.4403\n",
            "Epoch 10 Batch 1200 Loss 1.2158 Accuracy 0.4403\n",
            "Epoch 10 Batch 1250 Loss 1.2156 Accuracy 0.4399\n",
            "Epoch 10 Batch 1300 Loss 1.2163 Accuracy 0.4397\n",
            "Epoch 10 Batch 1350 Loss 1.2175 Accuracy 0.4396\n",
            "Epoch 10 Batch 1400 Loss 1.2175 Accuracy 0.4393\n",
            "Epoch 10 Batch 1450 Loss 1.2180 Accuracy 0.4389\n",
            "Epoch 10 Batch 1500 Loss 1.2185 Accuracy 0.4388\n",
            "Epoch 10 Batch 1550 Loss 1.2180 Accuracy 0.4387\n",
            "Epoch 10 Batch 1600 Loss 1.2167 Accuracy 0.4385\n",
            "Epoch 10 Batch 1650 Loss 1.2159 Accuracy 0.4385\n",
            "Epoch 10 Batch 1700 Loss 1.2153 Accuracy 0.4387\n",
            "Epoch 10 Batch 1750 Loss 1.2137 Accuracy 0.4389\n",
            "Epoch 10 Batch 1800 Loss 1.2119 Accuracy 0.4390\n",
            "Epoch 10 Batch 1850 Loss 1.2109 Accuracy 0.4394\n",
            "Epoch 10 Batch 1900 Loss 1.2097 Accuracy 0.4397\n",
            "Epoch 10 Batch 1950 Loss 1.2082 Accuracy 0.4399\n",
            "Epoch 10 Batch 2000 Loss 1.2073 Accuracy 0.4401\n",
            "Epoch 10 Batch 2050 Loss 1.2061 Accuracy 0.4402\n",
            "Epoch 10 Batch 2100 Loss 1.2044 Accuracy 0.4405\n",
            "Epoch 10 Batch 2150 Loss 1.2026 Accuracy 0.4407\n",
            "Epoch 10 Batch 2200 Loss 1.2000 Accuracy 0.4409\n",
            "Epoch 10 Batch 2250 Loss 1.1989 Accuracy 0.4411\n",
            "Epoch 10 Batch 2300 Loss 1.1968 Accuracy 0.4413\n",
            "Epoch 10 Batch 2350 Loss 1.1943 Accuracy 0.4416\n",
            "Epoch 10 Batch 2400 Loss 1.1916 Accuracy 0.4419\n",
            "Epoch 10 Batch 2450 Loss 1.1889 Accuracy 0.4421\n",
            "Epoch 10 Batch 2500 Loss 1.1864 Accuracy 0.4422\n",
            "Epoch 10 Batch 2550 Loss 1.1831 Accuracy 0.4423\n",
            "Epoch 10 Batch 2600 Loss 1.1802 Accuracy 0.4425\n",
            "Epoch 10 Batch 2650 Loss 1.1770 Accuracy 0.4428\n",
            "Epoch 10 Batch 2700 Loss 1.1743 Accuracy 0.4430\n",
            "Epoch 10 Batch 2750 Loss 1.1707 Accuracy 0.4433\n",
            "Epoch 10 Batch 2800 Loss 1.1673 Accuracy 0.4438\n",
            "Epoch 10 Batch 2850 Loss 1.1641 Accuracy 0.4443\n",
            "Epoch 10 Batch 2900 Loss 1.1610 Accuracy 0.4448\n",
            "Epoch 10 Batch 2950 Loss 1.1577 Accuracy 0.4452\n",
            "Epoch 10 Batch 3000 Loss 1.1537 Accuracy 0.4458\n",
            "Epoch 10 Batch 3050 Loss 1.1505 Accuracy 0.4464\n",
            "Epoch 10 Batch 3100 Loss 1.1469 Accuracy 0.4469\n",
            "Epoch 10 Batch 3150 Loss 1.1436 Accuracy 0.4475\n",
            "Epoch 10 Batch 3200 Loss 1.1403 Accuracy 0.4480\n",
            "Epoch 10 Batch 3250 Loss 1.1373 Accuracy 0.4484\n",
            "Epoch 10 Batch 3300 Loss 1.1349 Accuracy 0.4489\n",
            "Epoch 10 Batch 3350 Loss 1.1325 Accuracy 0.4492\n",
            "Epoch 10 Batch 3400 Loss 1.1298 Accuracy 0.4497\n",
            "Epoch 10 Batch 3450 Loss 1.1274 Accuracy 0.4500\n",
            "Epoch 10 Batch 3500 Loss 1.1252 Accuracy 0.4503\n",
            "Epoch 10 Batch 3550 Loss 1.1228 Accuracy 0.4507\n",
            "Epoch 10 Batch 3600 Loss 1.1205 Accuracy 0.4510\n",
            "Epoch 10 Batch 3650 Loss 1.1182 Accuracy 0.4512\n",
            "Epoch 10 Batch 3700 Loss 1.1163 Accuracy 0.4514\n",
            "Epoch 10 Batch 3750 Loss 1.1147 Accuracy 0.4517\n",
            "Epoch 10 Batch 3800 Loss 1.1126 Accuracy 0.4520\n",
            "Epoch 10 Batch 3850 Loss 1.1105 Accuracy 0.4523\n",
            "Epoch 10 Batch 3900 Loss 1.1087 Accuracy 0.4526\n",
            "Epoch 10 Batch 3950 Loss 1.1064 Accuracy 0.4530\n",
            "Epoch 10 Batch 4000 Loss 1.1048 Accuracy 0.4532\n",
            "Epoch 10 Batch 4050 Loss 1.1028 Accuracy 0.4535\n",
            "Epoch 10 Batch 4100 Loss 1.1009 Accuracy 0.4538\n",
            "Epoch 10 Batch 4150 Loss 1.0992 Accuracy 0.4540\n",
            "Epoch 10 Batch 4200 Loss 1.0973 Accuracy 0.4543\n",
            "Epoch 10 Batch 4250 Loss 1.0956 Accuracy 0.4547\n",
            "Epoch 10 Batch 4300 Loss 1.0940 Accuracy 0.4549\n",
            "Epoch 10 Batch 4350 Loss 1.0925 Accuracy 0.4551\n",
            "Epoch 10 Batch 4400 Loss 1.0910 Accuracy 0.4554\n",
            "Epoch 10 Batch 4450 Loss 1.0899 Accuracy 0.4557\n",
            "Epoch 10 Batch 4500 Loss 1.0885 Accuracy 0.4559\n",
            "Epoch 10 Batch 4550 Loss 1.0870 Accuracy 0.4562\n",
            "Epoch 10 Batch 4600 Loss 1.0855 Accuracy 0.4565\n",
            "Epoch 10 Batch 4650 Loss 1.0840 Accuracy 0.4567\n",
            "Epoch 10 Batch 4700 Loss 1.0832 Accuracy 0.4568\n",
            "Epoch 10 Batch 4750 Loss 1.0828 Accuracy 0.4569\n",
            "Epoch 10 Batch 4800 Loss 1.0825 Accuracy 0.4570\n",
            "Epoch 10 Batch 4850 Loss 1.0823 Accuracy 0.4570\n",
            "Epoch 10 Batch 4900 Loss 1.0824 Accuracy 0.4570\n",
            "Epoch 10 Batch 4950 Loss 1.0824 Accuracy 0.4570\n",
            "Epoch 10 Batch 5000 Loss 1.0829 Accuracy 0.4569\n",
            "Epoch 10 Batch 5050 Loss 1.0832 Accuracy 0.4568\n",
            "Epoch 10 Batch 5100 Loss 1.0839 Accuracy 0.4567\n",
            "Epoch 10 Batch 5150 Loss 1.0843 Accuracy 0.4567\n",
            "Epoch 10 Batch 5200 Loss 1.0849 Accuracy 0.4566\n",
            "Epoch 10 Batch 5250 Loss 1.0856 Accuracy 0.4566\n",
            "Epoch 10 Batch 5300 Loss 1.0864 Accuracy 0.4565\n",
            "Epoch 10 Batch 5350 Loss 1.0872 Accuracy 0.4564\n",
            "Epoch 10 Batch 5400 Loss 1.0877 Accuracy 0.4564\n",
            "Epoch 10 Batch 5450 Loss 1.0884 Accuracy 0.4563\n",
            "Epoch 10 Batch 5500 Loss 1.0890 Accuracy 0.4563\n",
            "Epoch 10 Batch 5550 Loss 1.0897 Accuracy 0.4561\n",
            "Epoch 10 Batch 5600 Loss 1.0905 Accuracy 0.4560\n",
            "Epoch 10 Batch 5650 Loss 1.0913 Accuracy 0.4560\n",
            "Epoch 10 Batch 5700 Loss 1.0920 Accuracy 0.4559\n",
            "Epoch 10 Batch 5750 Loss 1.0925 Accuracy 0.4557\n",
            "Epoch 10 Batch 5800 Loss 1.0934 Accuracy 0.4556\n",
            "Epoch 10 Batch 5850 Loss 1.0941 Accuracy 0.4555\n",
            "Epoch 10 Batch 5900 Loss 1.0949 Accuracy 0.4554\n",
            "Epoch 10 Batch 5950 Loss 1.0959 Accuracy 0.4553\n",
            "Epoch 10 Batch 6000 Loss 1.0971 Accuracy 0.4552\n",
            "Epoch 10 Batch 6050 Loss 1.0980 Accuracy 0.4550\n",
            "Epoch 10 Batch 6100 Loss 1.0991 Accuracy 0.4548\n",
            "Epoch 10 Batch 6150 Loss 1.1000 Accuracy 0.4546\n",
            "Epoch 10 Batch 6200 Loss 1.1010 Accuracy 0.4544\n",
            "Epoch 10 Batch 6250 Loss 1.1019 Accuracy 0.4543\n",
            "Epoch 10 Batch 6300 Loss 1.1028 Accuracy 0.4541\n",
            "Epoch 10 Batch 6350 Loss 1.1036 Accuracy 0.4540\n",
            "Epoch 10 Batch 6400 Loss 1.1043 Accuracy 0.4538\n",
            "Epoch 10 Batch 6450 Loss 1.1052 Accuracy 0.4537\n",
            "Epoch 10 Batch 6500 Loss 1.1060 Accuracy 0.4535\n",
            "Epoch 10 Batch 6550 Loss 1.1068 Accuracy 0.4534\n",
            "Saving checkpoint for epoch 10 at ./drive/My Drive/NLP Projects/Dutch_Transformer/ckpt/ckpt-20\n",
            "Time taken for 1 epoch: 1783.2004616260529 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voLD4bGtpO93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNpyoQY3ANQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74xJF07q_fIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTKcbgGOpVBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_NL-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_NL-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1YXUC2LO6Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()\n",
        "\n",
        "  predicted_sentence = tokenizer_nl.decode(\n",
        "      [i for i in output if i < VOCAB_SIZE_NL-2]\n",
        "    )\n",
        "  \n",
        "  print(\"Input: {}\".format(sentence))\n",
        "  print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5xy1n6NpdbF",
        "colab_type": "code",
        "outputId": "b1035d86-61e2-4206-ea12-f244a556b268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "translate(\"Today is a good day.\") "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: Today is a good day.\n",
            "Predicted translation: Vandaag is het een goede dag.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
